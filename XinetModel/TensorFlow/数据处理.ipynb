{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [TensorFlow 高级接口使用简介（estimator, keras， data, experiment](https://www.cnblogs.com/arkenstone/p/8448208.html)\n",
    "\n",
    "`tf.data`可以方便从多种来源的数据输入到搭建的网络中（利用`tf.feature_column`可以方便的对结构化的数据进行读取和处理，比如存在`csv`中的数据，具体操作可以参考[这篇](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html)文档）；\n",
    "\n",
    "`tf.estimator`提供了`tf.estimator.Estimator.export_savedmodel`函数很方便将训练好的`ckpt`模型文件转成`pb`文件并结合 docker 和 tensorflow serving 进行灵活稳定的模型部署和更新。\n",
    "\n",
    "# 1. `tf.data` 进行数据流操作（TFRecords）\n",
    "\n",
    "在 keras 中有 [`keras.preprocessing.image.ImageDataGenerator`类](https://keras-cn.readthedocs.io/en/latest/preprocessing/image/)和`.flow_from_directory()`函数可以很容易将保存在 文件夹 下面的数据进行读取；也可以用`.flow()`函数将数据直接从`np.array`中读取后输入网络进行训练（具体可以查看官方文档）。在使用图片并以文件夹名作为分类名的训练任务时这个方案是十分简单有效的，但是Tensorflow官方推荐的数据保存格式是 TFRecords，而keras官方不支持直接从tfrecords文件中读取数据（tf.keras也不行，keras作者不太推荐使用），所以这里就可以用`tf.data`类来处理从`TFRecords`中的数据（也可以用之前常用的`tf.train.batch()`或`tf.train.shuffle_batch()`来处理训练数据）。\n",
    "\n",
    "Tensorflow官方提供了详细的文档来介绍`data`的机制和使用方法（看这里），而对于`TFRecords`类型数据，主要利用 `tf.data.Iterator()`来抽取数据，这里简单说下从`TFRecords中`提取数据的方法：\n",
    "\n",
    "以下代码为官方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:14:14.547182Z",
     "start_time": "2018-03-28T12:14:14.494183Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataset_input_fn():\n",
    "    filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\n",
    "    dataset = tf.data.TFRecordDataset(filenames)  # 制定提取数据的tfrecords文件\n",
    "\n",
    "    # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "    # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):   # 对tfrecords中的数据进行解析的操作\n",
    "        keys_to_features = {\n",
    "            \"image_data\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "            \"date_time\": tf.FixedLenFeature((), tf.int64, default_value=\"\"),\n",
    "            \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                        default_value=tf.zeros([], dtype=tf.int64)),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "        # Perform additional preprocessing on the parsed data.\n",
    "        image = tf.image.decode_jpeg(parsed[\"image_data\"])\n",
    "        image = tf.reshape(image, [299, 299, 1])\n",
    "        label = tf.cast(parsed[\"label\"], tf.int32)\n",
    "\n",
    "        return {\"image_data\": image, \"date_time\": parsed[\"date_time\"]}, label\n",
    "\n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    # 一般就用map函数对输入图像进行预处理，而预处理函数可以包含在上面用于解析的parser函数\n",
    "    dataset = dataset.map(parser)\n",
    "    # 在训练的时候一般需要将输入数据进行顺序打乱提高训练的泛化性\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.batch(32)  # 单次读取的batch大小\n",
    "    dataset = dataset.repeat(num_epochs)   # 数据集的重复使用次数，为空的话则无线循环\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels\n",
    "    # return {\"input\": features, labels}  # 对于estimator的输入前者为dict类型，后者为tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注: `dataset.make_one_shot_iterator()`是最简单的`Iterator`类，不需要明确的`initialization`，并且目前这是唯一能够在`estimator`中容易使用的`iterator`。对于需要重新使用的`Dataset`类（比如结构相同的训练和测试数据集），一般是需要用 `reinitializable iterator` ，不过在`estimator`中由于上述问题，现在一般的做法是对训练集和验证集单独写两个`pipeline`用`make_one_shot_iterator`来处理数据流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset + Keras\n",
    "通过`tf.data`处理`TFRecords`数据流，我们就可以使用`keras`来进行训练了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:16:20.768918Z",
     "start_time": "2018-03-28T12:16:20.667903Z"
    }
   },
   "outputs": [],
   "source": [
    "_PATCH_SIZE = 32\n",
    "def architecture(input_shape=(_PATCH_SIZE, _PATCH_SIZE, 3)):\n",
    "    \"\"\"\n",
    "    Model architecture\n",
    "    Args:\n",
    "        input_shape: input image shape (not include batch)\n",
    "\n",
    "    Returns: an keras model instance\n",
    "    \"\"\"\n",
    "    base_model = Xception(include_top=True,\n",
    "                          weights=None,   # no pre-trained weights used\n",
    "                          pooling=\"max\",\n",
    "                          input_shape=input_shape,  # modify first layer\n",
    "                          classes=_NUM_CLASSES)\n",
    "    base_model.summary()\n",
    "    return base_model\n",
    "\n",
    "\n",
    "def train(source_dir, model_save_path):\n",
    "    \"\"\"\n",
    "    Train patch based model\n",
    "    Args:\n",
    "        source_dir: a directory where training tfrecords file stored. All TF records start with train will be used!\n",
    "        model_save_path: weights save path\n",
    "    \"\"\"\n",
    "    if tf.gfile.Exists(source_dir):\n",
    "        train_data_paths = tf.gfile.Glob(source_dir+\"/train*tfrecord\")\n",
    "        val_data_paths = tf.gfile.Glob(source_dir+\"/val*tfrecord\")\n",
    "        if not len(train_data_paths):\n",
    "            raise Exception(\n",
    "                \"[Train Error]: unable to find train*.tfrecord file\")\n",
    "        if not len(val_data_paths):\n",
    "            raise Exception(\"[Eval Error]: unable to find val*.tfrecord file\")\n",
    "    else:\n",
    "        raise Exception(\"[Train Error]: unable to find input directory!\")\n",
    "    (images, labels) = dataset_input_fn(train_data_paths)\n",
    "    # keras model的输入需要为keras.Input类型，但是直接使用tensorflow tensor类型也是可以的\n",
    "    model_input = keras.Input(tensor=images, shape=(\n",
    "        _PATCH_SIZE, _PATCH_SIZE, 3), dtype=tf.float32, name=\"input\")\n",
    "    base_model = architecture()\n",
    "    model_output = base_model(model_input)\n",
    "    model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "    optimizer = keras.optimizers.RMSprop(lr=2e-3, decay=0.9)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=focal_loss,\n",
    "                  metrics=['accuracy'],\n",
    "                  target_tensors=[labels])   # 1\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=model_save_path)\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_save_path+\"/saved_model.h5\")\n",
    "    model.fit(steps_per_epoch=8000, epochs=_EPOCHS,\n",
    "              callbacks=[tensorboard, model_checkpoint])\n",
    "\n",
    "\n",
    "def evaluate(source_dir, weights_path):\n",
    "    \"\"\"\n",
    "    Eval patch based model\n",
    "    Args:\n",
    "        source_dir: directory where val tf records file stored. All TF records start with val will be used!\n",
    "        weights_path: model weights save path\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    base_model = architecture()\n",
    "    base_model.load_weights(weights_path)\n",
    "    # load test dataset\n",
    "    if tf.gfile.Exists(source_dir):\n",
    "        val_data_paths = tf.gfile.Glob(source_dir+\"/val*.tfrecord\")\n",
    "        if not len(val_data_paths):\n",
    "            raise Exception(\"[Eval Error]: unable to find val*.tfrecord file\")\n",
    "    else:\n",
    "        raise Exception(\"[Train Error]: unable to find input directory!\")\n",
    "    (images, labels) = input_fn(source_dir)\n",
    "    probs = base_model(images)\n",
    "    predictions = tf.argmax(probs, axis=-1)\n",
    "    accuracy_score = tf.reduce_mean(tf.equal(probs, predictions))\n",
    "    print(\"Accuracy of testing images: {}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset + estimator\n",
    "可以用`tf.layers`函数来代替`keras`搭建网络，而且可以提供更丰富的 layer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:20:05.904149Z",
     "start_time": "2018-03-28T12:20:05.899150Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T11:59:07.995342Z",
     "start_time": "2018-03-28T11:59:07.978347Z"
    },
    "code_folding": [
     0,
     141,
     208,
     218,
     262
    ]
   },
   "outputs": [],
   "source": [
    "def xception():\n",
    "    def tf_xception(features, classes=2, is_training=True):\n",
    "    \"\"\"\n",
    "    The Xception architecture written in tf.layers\n",
    "    Args:\n",
    "        features: input image tensor\n",
    "        classes: number of classes to classify images into\n",
    "        is_training: is training stage or not\n",
    "\n",
    "    Returns:\n",
    "        2-D logits prediction output after pooling and activation\n",
    "    \"\"\"\n",
    "    x = tf.layers.conv2d(features, 32, (3, 3), strides=(\n",
    "        2, 2), use_bias=False, name='block1_conv1')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block1_conv1_bn')\n",
    "    x = tf.nn.relu(x, name='block1_conv1_act')\n",
    "    x = tf.layers.conv2d(x, 64, (3, 3), use_bias=False, name='block1_conv2')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block1_conv2_bn')\n",
    "    x = tf.nn.relu(x, name='block1_conv2_act')\n",
    "\n",
    "    residual = tf.layers.conv2d(x, 128, (1, 1), strides=(\n",
    "        2, 2), padding='same', use_bias=False)\n",
    "    residual = tf.layers.batch_normalization(residual, training=is_training)\n",
    "\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block2_sepconv1_bn')\n",
    "    x = tf.nn.relu(x, name='block2_sepconv2_act')\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block2_sepconv2_bn')\n",
    "\n",
    "    x = tf.layers.max_pooling2d(x, (3, 3), strides=(\n",
    "        2, 2), padding='same', name='block2_pool')\n",
    "    x = tf.add(x, residual, name='block2_add')\n",
    "\n",
    "    residual = tf.layers.conv2d(x, 256, (1, 1), strides=(\n",
    "        2, 2), padding='same', use_bias=False)\n",
    "    residual = tf.layers.batch_normalization(residual, training=is_training)\n",
    "\n",
    "    x = tf.nn.relu(x, name='block3_sepconv1_act')\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block3_sepconv1_bn')\n",
    "    x = tf.nn.relu(x, name='block3_sepconv2_act')\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block3_sepconv2_bn')\n",
    "\n",
    "    x = tf.layers.max_pooling2d(x, (3, 3), strides=(\n",
    "        2, 2), padding='same', name='block3_pool')\n",
    "    x = tf.add(x, residual, name=\"block3_add\")\n",
    "\n",
    "    residual = tf.layers.conv2d(x, 728, (1, 1), strides=(\n",
    "        2, 2), padding='same', use_bias=False)\n",
    "    residual = tf.layers.batch_normalization(residual, training=is_training)\n",
    "\n",
    "    x = tf.nn.relu(x, name='block4_sepconv1_act')\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block4_sepconv1_bn')\n",
    "    x = tf.nn.relu(x, name='block4_sepconv2_act')\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block4_sepconv2_bn')\n",
    "\n",
    "    x = tf.layers.max_pooling2d(x, (3, 3), strides=(\n",
    "        2, 2), padding='same', name='block4_pool')\n",
    "    x = tf.add(x, residual, name=\"block4_add\")\n",
    "\n",
    "    for i in range(8):\n",
    "        residual = x\n",
    "        prefix = 'block' + str(i + 5)\n",
    "\n",
    "        x = tf.nn.relu(x, name=prefix + '_sepconv1_act')\n",
    "        x = tf.layers.separable_conv2d(\n",
    "            x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')\n",
    "        x = tf.layers.batch_normalization(\n",
    "            x, training=is_training, name=prefix + '_sepconv1_bn')\n",
    "        x = tf.nn.relu(x, name=prefix + '_sepconv2_act')\n",
    "        x = tf.layers.separable_conv2d(\n",
    "            x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')\n",
    "        x = tf.layers.batch_normalization(\n",
    "            x, training=is_training, name=prefix + '_sepconv2_bn')\n",
    "        x = tf.nn.relu(x, name=prefix + '_sepconv3_act')\n",
    "        x = tf.layers.separable_conv2d(\n",
    "            x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')\n",
    "        x = tf.layers.batch_normalization(\n",
    "            x, training=is_training, name=prefix + '_sepconv3_bn')\n",
    "\n",
    "        x = tf.add(x, residual, name=prefix+\"_add\")\n",
    "\n",
    "    residual = tf.layers.conv2d(x, 1024, (1, 1), strides=(\n",
    "        2, 2), padding='same', use_bias=False)\n",
    "    residual = tf.layers.batch_normalization(residual, training=is_training)\n",
    "\n",
    "    x = tf.nn.relu(x, name='block13_sepconv1_act')\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block13_sepconv1_bn')\n",
    "    x = tf.nn.relu(x, name='block13_sepconv2_act')\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block13_sepconv2_bn')\n",
    "\n",
    "    x = tf.layers.max_pooling2d(x, (3, 3), strides=(\n",
    "        2, 2), padding='same', name='block13_pool')\n",
    "    x = tf.add(x, residual, name=\"block13_add\")\n",
    "\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block14_sepconv1_bn')\n",
    "    x = tf.nn.relu(x, name='block14_sepconv1_act')\n",
    "\n",
    "    x = tf.layers.separable_conv2d(\n",
    "        x, 2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')\n",
    "    x = tf.layers.batch_normalization(\n",
    "        x, training=is_training, name='block14_sepconv2_bn')\n",
    "    x = tf.nn.relu(x, name='block14_sepconv2_act')\n",
    "    # replace conv layer with fc\n",
    "    x = tf.layers.average_pooling2d(\n",
    "        x, (3, 3), (2, 2), name=\"global_average_pooling\")\n",
    "    x = tf.layers.conv2d(\n",
    "        x, 2048, [1, 1], activation=None, name=\"block15_conv1\")\n",
    "    x = tf.layers.conv2d(x, classes, [1, 1],\n",
    "                         activation=None, name=\"block15_conv2\")\n",
    "    x = tf.squeeze(x, axis=[1, 2], name=\"logits\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"\n",
    "    Model_fn for estimator model\n",
    "    Args:\n",
    "        features (Tensor): Input features to the model.\n",
    "        labels (Tensor): Labels tensor for training and evaluation.\n",
    "        mode (ModeKeys): Specifies if training, evaluation or prediction.\n",
    "        params (HParams): hyper-parameters for estimator model\n",
    "    Returns:\n",
    "        (EstimatorSpec): Model to be run by Estimator.\n",
    "    \"\"\"\n",
    "    # check if training stage\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        is_training = True\n",
    "    else:\n",
    "        is_training = False\n",
    "    # is_training = False   # 1\n",
    "    input_tensor = features[\"input\"]\n",
    "    logits = xception(input_tensor, classes=_NUM_CLASSES,\n",
    "                      is_training=is_training)\n",
    "    probs = tf.nn.softmax(logits, name=\"output_score\")\n",
    "    predictions = tf.argmax(probs, axis=-1, name=\"output_label\")\n",
    "    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), _NUM_CLASSES)\n",
    "    # provide a tf.estimator spec for PREDICT\n",
    "    predictions_dict = {\"score\": probs,\n",
    "                        \"label\": predictions}\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions_output = tf.estimator.export.PredictOutput(\n",
    "            predictions_dict)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict,\n",
    "                                          export_outputs={\n",
    "                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output\n",
    "                                          })\n",
    "    # calculate loss\n",
    "    # loss = focal_loss(onehot_labels, logits, gamma=1.5)\n",
    "    gamma = 1.5\n",
    "    weights = tf.reduce_sum(tf.multiply(\n",
    "        onehot_labels, tf.pow(1. - probs, gamma)), axis=-1)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels, logits, weights=weights)\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predictions)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        lr = params.learning_rate\n",
    "        # train optimizer\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\n",
    "        update_ops = tf.get_collections(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "          train_op = optimizer.minimize(\n",
    "              loss, global_step=tf.train.get_global_step())\n",
    "        tensors_to_log = {'batch_accuracy': accuracy[1],\n",
    "                          'logits': logits,\n",
    "                          'label': labels}\n",
    "        logging_hook = tf.train.LoggingTensorHook(\n",
    "            tensors=tensors_to_log, every_n_iter=1000)\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=loss,\n",
    "                                          train_op=train_op,\n",
    "                                          training_hooks=[logging_hook])\n",
    "    else:\n",
    "        eval_metric_ops = {\"accuracy\": accuracy}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=loss,\n",
    "                                          eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "def get_estimator_model(config=None, params=None):\n",
    "    \"\"\"\n",
    "    Get estimator model by definition of model_fn\n",
    "    \"\"\"\n",
    "    est_model = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                       config=config,\n",
    "                                       params=params)\n",
    "    return est_model\n",
    "\n",
    "\n",
    "def train(source_dir, model_save_path):\n",
    "    \"\"\"\n",
    "    Train patch based model\n",
    "    Args:\n",
    "        source_dir: a directory where training tfrecords file stored. All TF records start with train will be used!\n",
    "        model_save_path: weights save path\n",
    "    \"\"\"\n",
    "    if tf.gfile.Exists(source_dir):\n",
    "        train_data_paths = tf.gfile.Glob(source_dir+\"/train*tfrecord\")\n",
    "        val_data_paths = tf.gfile.Glob(source_dir+\"/val*tfrecord\")\n",
    "        if not len(train_data_paths):\n",
    "            raise Exception(\n",
    "                \"[Train Error]: unable to find train*.tfrecord file\")\n",
    "        if not len(val_data_paths):\n",
    "            raise Exception(\"[Eval Error]: unable to find val*.tfrecord file\")\n",
    "    else:\n",
    "        raise Exception(\"[Train Error]: unable to find input directory!\")\n",
    "    train_config = tf.estimator.RunConfig()\n",
    "    new_config = train_config.replace(model_dir=model_save_path,\n",
    "                                      save_checkpoints_steps=1000,\n",
    "                                      keep_checkpoint_max=5)\n",
    "    params = tf.contrib.training.HParams(\n",
    "        learning_rate=0.001,\n",
    "        train_steps=5000,\n",
    "        min_eval_frequency=1000\n",
    "    )\n",
    "    est_model = get_estimator_model(config=new_config,\n",
    "                                    params=params)\n",
    "    # define training config\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn(data_path=train_data_paths,\n",
    "                                                                  batch_size=_BATCH_SIZE,\n",
    "                                                                  is_training=True),\n",
    "                                        max_steps=_MAX_STEPS)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=lambda: input_fn(data_path=val_data_paths,\n",
    "                                                                batch_size=_BATCH_SIZE),\n",
    "                                      steps=100,\n",
    "                                      throttle_secs=900)\n",
    "    # train and evaluate model\n",
    "    tf.estimator.train_and_evaluate(estimator=est_model,\n",
    "                                    train_spec=train_spec,\n",
    "                                    eval_spec=eval_spec)\n",
    "\n",
    "\n",
    "def evaluate(source_dir, model_save_path):\n",
    "    \"\"\"\n",
    "    Eval patch based model\n",
    "    Args:\n",
    "        source_dir: directory where val tf records file stored. All TF records start with val will be used!\n",
    "        model_save_path: model save path\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    run_config = tf.contrib.learn.RunConfig(model_dir=model_save_path)\n",
    "    est_model = get_estimator_model(run_config)\n",
    "    # load test dataset\n",
    "    if tf.gfile.Exists(source_dir):\n",
    "        val_data_paths = tf.gfile.Glob(source_dir+\"/val*.tfrecord\")\n",
    "        if not len(val_data_paths):\n",
    "            raise Exception(\"[Eval Error]: unable to find val*.tfrecord file\")\n",
    "    else:\n",
    "        raise Exception(\"[Train Error]: unable to find input directory!\")\n",
    "    accuracy_score = est_model.evaluate(input_fn=lambda: input_fn(val_data_paths,\n",
    "                                                                  batch_size=_BATCH_SIZE,\n",
    "                                                                  is_training=False))\n",
    "    print(\"Accuracy of testing images: {}\".format(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [TensorFlow全新的数据读取方式：Dataset API入门教程](https://www.leiphone.com/news/201711/zV7yM5W1dFrzs8W5.html)\n",
    "\n",
    "## `Dataset` 和 `Iterator`\n",
    "\n",
    "`Dataset`可以看作是相同类型“元素”的有序列表。在实际使用时，单个“元素”可以是向量，也可以是字符串、图片，甚至是`tuple`或者`dict`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:29:29.617906Z",
     "start_time": "2018-03-28T12:29:29.612904Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a = [3, 4, 5]\n",
    "b = 'fg'\n",
    "c = np.array([3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:30:40.956245Z",
     "start_time": "2018-03-28T12:30:40.949271Z"
    }
   },
   "outputs": [],
   "source": [
    "a1 = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:30:58.062276Z",
     "start_time": "2018-03-28T12:30:58.057292Z"
    }
   },
   "outputs": [],
   "source": [
    "a2 = tf.data.Dataset.from_tensor_slices(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:31:30.114831Z",
     "start_time": "2018-03-28T12:31:30.109805Z"
    }
   },
   "source": [
    "如何将这个`dataset`中的元素取出呢？方法是从`Dataset`中示例化一个`Iterator`，然后对`Iterator`进行迭代：\n",
    "- 在非`Eager`模式下，读取上述`dataset`中元素的方法为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:33:28.630004Z",
     "start_time": "2018-03-28T12:33:28.588011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "iterator = a1.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    for i in range(5):\n",
    "        print(sess.run(one_element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_one_shot_iterator()`从`dataset`中实例化了一个`Iterator`，这个`Iterator`是一个“one shot iterator”，即只能从头到尾读取一次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:35:56.052461Z",
     "start_time": "2018-03-28T12:35:56.004478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "end!\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(one_element))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在`Eager`模式中，创建`Iterator`的方式有所不同。是通过`tfe.Iterator(dataset)`的形式直接创建`Iterator`并迭代。迭代时可以直接取出值，不需要使用`sess.run()`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:39:05.153131Z",
     "start_time": "2018-03-28T12:39:03.790090Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas.core.computation' has no attribute 'expressions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-34777f8f9010>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m dataset = tf.data.Dataset.from_tensor_slices(\n\u001b[0;32m      5\u001b[0m     np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Add projects here, they will show up under tf.contrib.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbatching\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbayesflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster_resolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\bayesflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhalton_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhmc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetropolis_hastings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmonte_carlo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\bayesflow\\python\\ops\\layers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# go/tf-wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_conv_variational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_dense_variational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\bayesflow\\python\\ops\\layers_conv_variational.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayesflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mindependent\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mindependent_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\bayesflow\\python\\ops\\layers_util.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdeterministic_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mindependent\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mindependent_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\distributions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftplus_inverse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtridiag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf_normal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\distributions\\python\\ops\\estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_compute_weighted_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_RegressionHead\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgraph_actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlearn_io\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProblemType\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNNClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNNEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDNNRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdnn_linear_combined\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn_linear_combined.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetric_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhead\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhead_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_feeder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaved_model_export_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdask_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_dask_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdask_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_dask_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdask_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHAS_DASK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\learn_io\\dask_io.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m   \u001b[0mallowed_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[0mHAS_DASK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from .core import (DataFrame, Series, Index, _Frame, map_partitions,\n\u001b[0m\u001b[0;32m      4\u001b[0m                    repartition, to_delayed, to_datetime, to_timedelta)\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAggregation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mPANDAS_VERSION\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;34m'0.20.0'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_use_numexpr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas.core.computation' has no attribute 'expressions'"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "tfe.enable_eager_execution()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "\n",
    "for one_element in tfe.Iterator(dataset):\n",
    "    print(one_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从内存中创建更复杂的 Dataset\n",
    "\n",
    "其实，`tf.data.Dataset.from_tensor_slices`的功能不止如此，它的真正作用是切分传入`Tensor`的第一个维度，生成相应的`dataset`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:44:43.101317Z",
     "start_time": "2018-03-28T12:44:43.094278Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.random.uniform(size=(5, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传入的数值是一个矩阵，它的形状为`(5, 2)`，`tf.data.Dataset.from_tensor_slices`就会切分它形状上的第一个维度，最后生成的`dataset`中一个含有`5`个元素，每个元素的形状是`(2, )`，即每个元素是矩阵的一行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:47:43.891813Z",
     "start_time": "2018-03-28T12:47:43.847838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39768109 0.71553004]\n",
      "[0.98190965 0.03232115]\n",
      "[0.8452184  0.17540232]\n",
      "[0.12115773 0.51115376]\n",
      "[0.43379943 0.87275321]\n",
      "end!\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(one_element))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际使用中，我们可能还希望`Dataset`中的每个元素具有更复杂的形式，如每个元素是一个 Python中 的元组，或是 Python 中的词典。例如，在图像识别问题中，一个元素可以是`{\"image\": image_tensor, \"label\": label_tensor}`的形式，这样处理起来更方便。`tf.data.Dataset.from_tensor_slices`同样支持创建这种`dataset`，例如我们可以让每一个元素是一个词典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:50:35.420798Z",
     "start_time": "2018-03-28T12:50:35.414856Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    \"a\":\n",
    "    np.array([1.0, 2.0, 3.0, 4.0, 5.0]),\n",
    "    \"b\":\n",
    "    np.random.uniform(size=(5, 2))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:50:43.940808Z",
     "start_time": "2018-03-28T12:50:43.892809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1.0, 'b': array([0.11961808, 0.48213857])}\n",
      "{'a': 2.0, 'b': array([0.65146467, 0.96469436])}\n",
      "{'a': 3.0, 'b': array([0.63114947, 0.61944099])}\n",
      "{'a': 4.0, 'b': array([0.04219048, 0.02537955])}\n",
      "{'a': 5.0, 'b': array([0.44816809, 0.60973009])}\n",
      "end!\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(one_element))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`tf.data.Dataset.from_tensor_slices`创建每个元素是一个`tuple`的`dataset`也是可以的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T13:00:49.363946Z",
     "start_time": "2018-03-28T13:00:49.098945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, array([0.65244424, 0.29549035]))\n",
      "(2.0, array([0.00764112, 0.7995072 ]))\n",
      "(3.0, array([0.67443196, 0.62777513]))\n",
      "(4.0, array([0.56733892, 0.91038492]))\n",
      "(5.0, array([0.37998656, 0.13630998]))\n",
      "end!\n"
     ]
    }
   ],
   "source": [
    "dataset = tfdataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.random.uniform(size=(5, 2))))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(one_element))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"end!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对 `Dataset`中的元素做变换：`Transformation`\n",
    "`Dataset`支持一类特殊的操作：`Transformation`。一个`Dataset`通过`Transformation`变成一个新的`Dataset`。通常我们可以通过`Transformation`完成数据变换，打乱，组成`batch`，生成`epoch`等一系列操作。\n",
    "\n",
    "常用的`Transformation`有：\n",
    "- `map`\n",
    "- `batch`\n",
    "- `shuffle`\n",
    "- `repeat`\n",
    "\n",
    "### （1）`map`\n",
    "`map`接收一个函数，`Dataset`中的每个元素都会被当作这个函数的输入，并将函数返回值作为新的`Dataset`，如我们可以对`dataset`中每个元素的值加`1`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T12:55:41.917233Z",
     "start_time": "2018-03-28T12:55:41.908223Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
    "dataset = dataset.map(lambda x: x + 1)     # 2.0, 3.0, 4.0, 5.0, 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）`batch`\n",
    "`batch`就是将多个元素组合成`batch`，如下面的程序将`dataset`中的每个元素组成了大小为`32`的`batch`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T13:02:46.486984Z",
     "start_time": "2018-03-28T13:02:46.418011Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.random.sample((1024, 28, 28)))\n",
    "dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T13:02:47.933012Z",
     "start_time": "2018-03-28T13:02:47.266981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28)\n",
      "(32, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "print(one_element.shape)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(one_element).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）`shuffle`\n",
    "`shuffle`的功能为打乱`dataset`中的元素，它有一个参数`buffersize`，表示打乱时使用的`buffer`的大小："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T13:04:45.740263Z",
     "start_time": "2018-03-28T13:04:40.027278Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.random.sample((60000, 28, 28)))\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T13:05:30.301907Z",
     "start_time": "2018-03-28T13:04:57.874900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 28)\n",
      "(32, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "print(one_element.shape)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(one_element).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）`repeat`\n",
    "`repeat`的功能就是将整个序列重复多次，主要用来处理机器学习中的`epoch`，假设原先的数据是一个`epoch`，使用`repeat(5)`就可以将之变成`5`个`epoch`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T13:11:10.671568Z",
     "start_time": "2018-03-28T13:11:10.658552Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(3))\n",
    "dataset = dataset.repeat(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果直接调用`repeat()`的话，生成的序列就会无限重复下去，没有结束，因此也不会抛出`tf.errors.OutOfRangeError`异常：\n",
    "```py\n",
    "dataset=dataset.repeat()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子：读入磁盘图片与对应 label\n",
    "\n",
    "讲到这里，我们可以来考虑一个简单，但同时也非常常用的例子：读入磁盘中的图片和图片相应的 label，并将其打乱，组成 `batch_size=32` 的训练样本。在训练时重复`10`个 epoch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T06:06:22.290856Z",
     "start_time": "2018-03-30T06:06:22.242822Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "root = 'E:/Data/datasets/flower_photos/'\n",
    "\n",
    "\n",
    "def image_string(root):\n",
    "    '''获取根目录下的图片的文件名和标签以及整个数据集的类别'''\n",
    "    file_names = []\n",
    "    label_names = []\n",
    "    for k in os.listdir(root):\n",
    "        if os.path.isdir(root + k):\n",
    "            '''如果根目录下的是目录，则该目录下拥有需要处理的文件'''\n",
    "            for file_name in os.listdir(root + k):\n",
    "                file_names.append(root + k + '/' + file_name)\n",
    "                label_names.append(k)\n",
    "    return file_names, label_names\n",
    "\n",
    "file_names, label_names = image_string(root)\n",
    "\n",
    "class Index(object):\n",
    "    '''对标签进行处理'''\n",
    "\n",
    "    def __init__(self, label_names):\n",
    "        self.label_names = label_names\n",
    "\n",
    "    def one_hot(self):\n",
    "        '''将标签转换为 one-hot 编码格式'''\n",
    "        df = pd.get_dummies(self.label_names)\n",
    "        classes = df.columns\n",
    "        return df.get_values(), classes\n",
    "\n",
    "    def to_categorical_index(self):\n",
    "        '''将标签转换为 Categorical 类，以加快计算速度'''\n",
    "        labels = pd.CategoricalIndex(self.label_names)\n",
    "        labels_to_array = labels.codes\n",
    "        classes = labels.categories\n",
    "        return labels_to_array, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T06:06:27.945094Z",
     "start_time": "2018-03-30T06:06:27.941089Z"
    }
   },
   "outputs": [],
   "source": [
    "def _parse_function(file_name):\n",
    "    '''将 filename 对应的图片文件读进来，并将标签转换为 one-hot 编码'''\n",
    "    image_string = tf.read_file(file_name)\n",
    "    image_decoded = tf.image.decode_image(image_string)\n",
    "    return image_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T06:07:31.606308Z",
     "start_time": "2018-03-30T06:07:31.599308Z"
    }
   },
   "outputs": [],
   "source": [
    "index = Index(label_names)\n",
    "labels, classes = index.to_categorical_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T06:39:56.294913Z",
     "start_time": "2018-03-30T06:39:55.076681Z"
    }
   },
   "outputs": [],
   "source": [
    "# 此时 dataset中的一个元素是(filename, label)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(file_names)\n",
    "# 此时 dataset 中的一个元素是(image_resized, label)\n",
    "dataset = dataset.map(_parse_function)\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "one_element = iterator.get_next()\n",
    "with tf.Session() as sess:\n",
    "    img = sess.run(one_element)\n",
    "    # 缩放到统一的大小\n",
    "    image_resized = tf.image.resize_images(img, [28, 28])\n",
    "# 在每个 epoch 内将图片打乱组成大小为 32 的 batch，并重复 10 次。\n",
    "#dataset = dataset.shuffle(buffer_size=1000).batch(32).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T06:40:03.834066Z",
     "start_time": "2018-03-30T06:40:03.829074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(28), Dimension(28), Dimension(3)])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T06:11:02.338455Z",
     "start_time": "2018-03-30T06:10:59.738443Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    img = sess.run(image_decoded)\n",
    "    # 缩放到统一的大小\n",
    "    image_resized = tf.image.resize_images(img, [28, 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T04:42:06.133131Z",
     "start_time": "2018-03-30T04:42:06.128126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(263, 320, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T04:19:08.194200Z",
     "start_time": "2018-03-29T04:19:08.149219Z"
    }
   },
   "source": [
    "接下来我们就可以用这两个Tensor来建立模型了。\n",
    "\n",
    "除了 `tf.data.Dataset.from_tensor_slices`外，目前 Dataset API 还提供了另外三种创建`Dataset`的方式：\n",
    "- `tf.data.TextLineDataset()`：这个函数的输入是一个文件的列表，输出是一个`dataset`。`dataset`中的每一个元素就对应了文件中的一行。可以使用这个函数来读入`CSV`文件。\n",
    "- `tf.data.FixedLengthRecordDataset()`：这个函数的输入是一个文件的列表和一个`record_bytes`，之后`dataset`的每一个元素就是文件中固定字节数`record_bytes`的内容。通常用来读取以二进制形式保存的文件，如`CIFAR10`数据集就是这种形式。\n",
    "- `tf.data.TFRecordDataset()`：顾名思义，这个函数是用来读`TFRecord`文件的，`dataset`中的每一个元素就是一个`TFExample`。\n",
    "\n",
    "详细内容见：[Module: tf.data](https://www.tensorflow.org/api_docs/python/tf/data)\n",
    "\n",
    "在非 Eager 模式下，最简单的创建 Iterator 的方法就是通过 `dataset.make_one_shot_iterator()`来创建一个one shot iterator。除了这种 one shot iterator 外，还有三个更复杂的 Iterator，即：\n",
    "- `initializable iterator`\n",
    "- `reinitializable iterator`\n",
    "- `feedable iterator`\n",
    "\n",
    "`initializable iterator`必须要在使用前通过`sess.run()`来初始化。使用`initializable iterator`，可以将`placeholder` 代入 Iterator 中，这可以方便我们通过参数快速定义新的 Iterator。一个简单的 initializable iterator 使用示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T04:29:22.984934Z",
     "start_time": "2018-03-29T04:29:22.634628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(start=0, limit=limit))\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, feed_dict={limit: 10})\n",
    "    for i in range(10):\n",
    "        value = sess.run(next_element)\n",
    "        assert i == value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T04:29:23.987529Z",
     "start_time": "2018-03-29T04:29:23.981531Z"
    }
   },
   "source": [
    "此时的`limit`相当于一个“参数”，它规定了 Dataset 中数的“上限”。\n",
    "\n",
    "`initializable iterator`还有一个功能：读入较大的数组。\n",
    "\n",
    "在使用`tf.data.Dataset.from_tensor_slices(array)`时，实际上发生的事情是将`array`作为一个`tf.constants`保存到了计算图中。当`array`很大时，会导致计算图变得很大，给传输、保存带来不便。这时，我们可以用一个`placeholder`取代这里的`array`，并使用`initializable iterator`，只在需要时将`array`传进去，这样就可以避免把大数组保存在图里，示例代码为（来自官方例程）：\n",
    "\n",
    "```py\n",
    "# 从硬盘中读入两个 Numpy 数组\n",
    "with np.load(\"/var/data/training_data.npy\") as data:\n",
    "    features = data[\"features\"]\n",
    "    labels = data[\"labels\"]\n",
    "\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (features_placeholder, labels_placeholder))\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                          labels_placeholder: labels})\n",
    "```\n",
    "\n",
    "`reinitializable iterator`和`feedable iterator`相比`initializable iterator`更复杂，也更加少用，如果想要了解它们的功能，可以参阅[官方介绍](https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator)，这里就不再赘述了。\n",
    "\n",
    "`tf.data`\n",
    "- [官方Guide](https://www.tensorflow.org/programmers_guide/datasets)\n",
    "- [API文档](https://www.tensorflow.org/api_docs/python/tf/data)\n",
    "- [如何联合使用Dataset和Estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T04:36:20.531827Z",
     "start_time": "2018-03-29T04:36:20.500827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'float32'>\n",
      "(10,)\n",
      "(tf.float32, tf.int32)\n",
      "(TensorShape([]), TensorShape([Dimension(100)]))\n",
      "(tf.float32, (tf.float32, tf.int32))\n",
      "(TensorShape([Dimension(10)]), (TensorShape([]), TensorShape([Dimension(100)])))\n"
     ]
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))\n",
    "print(dataset1.output_types)  # ==> \"tf.float32\"\n",
    "print(dataset1.output_shapes)  # ==> \"(10,)\"\n",
    "\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "   (tf.random_uniform([4]),\n",
    "    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))\n",
    "print(dataset2.output_types)  # ==> \"(tf.float32, tf.int32)\"\n",
    "print(dataset2.output_shapes)  # ==> \"((), (100,))\"\n",
    "\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "print(dataset3.output_types)  # ==> (tf.float32, (tf.float32, tf.int32))\n",
    "print(dataset3.output_shapes)  # ==> \"(10, ((), (100,)))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T14:47:40.424754Z",
     "start_time": "2018-03-29T14:47:40.403755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': tf.float32, 'b': tf.int32}\n",
      "{'a': TensorShape([]), 'b': TensorShape([Dimension(100)])}\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "   {\"a\": tf.random_uniform([4]),\n",
    "    \"b\": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})\n",
    "print(dataset.output_types)  # ==> \"{'a': tf.float32, 'b': tf.int32}\"\n",
    "print(dataset.output_shapes)  # ==> \"{'a': (), 'b': (100,)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "dataset1 = dataset1.map(lambda x: ...)\n",
    "\n",
    "dataset2 = dataset2.flat_map(lambda x, y: ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
