{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:04:32.547926Z",
     "start_time": "2018-03-29T05:04:32.544923Z"
    }
   },
   "source": [
    "# 动态图介绍\n",
    "\n",
    "Eager Execution 是命令式的、由运行定义的的交互界面，运算在从 Python 中调出时便同时进行。这使得启动 TensorFlow 更加简单，也使得研究与开发更加直观。\n",
    "\n",
    "## 运用 Eager Execution\n",
    "当你启用 Eager Execution，运算会立刻执行并把值返回 Python，不必调用一次 `Session.run()`。举个例子，要将两个矩阵相乘，我们就写成这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:29:02.544527Z",
     "start_time": "2018-03-29T05:28:51.947210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:12:41.757396Z",
     "start_time": "2018-03-29T05:12:40.193071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "# 使用 print 或者 Python 调试器检查中间结果非常直接。\n",
    "\n",
    "print(m)\n",
    "# The 1x1 matrix [[4.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用 Python 流控制建立动态模型。这里有个使用 TensorFlow 的算术运算做 Collatz conjecture 的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:17:57.435097Z",
     "start_time": "2018-03-29T05:17:57.420065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float64)\n",
      "tf.Tensor(3.0, shape=(), dtype=float64)\n",
      "tf.Tensor(10.0, shape=(), dtype=float64)\n",
      "tf.Tensor(5.0, shape=(), dtype=float64)\n",
      "tf.Tensor(16.0, shape=(), dtype=float64)\n",
      "tf.Tensor(8.0, shape=(), dtype=float64)\n",
      "tf.Tensor(4.0, shape=(), dtype=float64)\n",
      "tf.Tensor(2.0, shape=(), dtype=float64)\n",
      "tf.Tensor(1.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(12)\n",
    "counter = 0\n",
    "while not tf.equal(a, 1):\n",
    "    if tf.equal(a % 2, 0):\n",
    "        a = a / 2\n",
    "    else:\n",
    "        a = 3 * a + 1\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度 Gradients\n",
    "大部分 TensorFlow 使用者都对自动微分（automatic differentiation）有兴趣。因为在每次调用过程中都会产生不同的操作，因此我们将所有的顺序操作记录到“磁带”上，然后在计算梯度时逆序播放。在计算出梯度之后，我们就丢弃磁带。\n",
    "\n",
    "如果你熟悉`autograd`包，那么这个 API 就非常简单。举个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:29:56.321298Z",
     "start_time": "2018-03-29T05:29:56.312287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square(3.) =  tf.Tensor(9.0, shape=(), dtype=float32)\n",
      "grad(3.) =  [<tf.Tensor: id=13, shape=(), dtype=float32, numpy=6.0>]\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return tf.multiply(x, x)\n",
    "\n",
    "grad = tfe.gradients_function(square)\n",
    "\n",
    "print('square(3.) = ', square(3.))    # [9.]\n",
    "print('grad(3.) = ', grad(3.))      # [6.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:25:38.614244Z",
     "start_time": "2018-03-29T05:25:32.718192Z"
    }
   },
   "source": [
    "这个`gradients_function`调用需要一个 Python 函数 `square()`作为一个主目(argument)然后返回一个可调用的Python 来根据所输入的值计算`square()`的偏导数。所以要获得`square()` 在`3.0`的导数, 调用 `grad(3.0)`即可，结果是`6``。\n",
    "\n",
    "相同的`gradients_function`调用可被用于获得平方的二阶导数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:31:07.284036Z",
     "start_time": "2018-03-29T05:31:07.276048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradgrad(3.) =  [<tf.Tensor: id=25, shape=(), dtype=float32, numpy=2.0>]\n"
     ]
    }
   ],
   "source": [
    "gradgrad = tfe.gradients_function(lambda x: grad(x)[0])\n",
    "print('gradgrad(3.) = ', gradgrad(3.))  # [2.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:31:34.031277Z",
     "start_time": "2018-03-29T05:31:34.023272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(2.0) =  [<tf.Tensor: id=10, shape=(), dtype=float32, numpy=1.0>]\n",
      "grad(-2.0) =  [<tf.Tensor: id=40, shape=(), dtype=float32, numpy=-1.0>]\n"
     ]
    }
   ],
   "source": [
    "def abs(x):\n",
    "    return x if x > 0. else -x\n",
    "\n",
    "grad = tfe.gradients_function(abs)\n",
    "print('grad(2.0) = ', grad(2.0))  # [1.]\n",
    "print('grad(-2.0) = ', grad(-2.0)) # [-1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义梯度\n",
    "\n",
    "使用者或许会想给一个运算或一个函数定制梯度。该操作有用的原因有很多，比如提高计算效率和数值稳定性。\n",
    "\n",
    "这里有个关于自定义梯度使用的例子：首先我们来看看这个函数`log(1 + ex)`，此函数通常出现在交叉熵和对数似然的计算中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:32:32.885825Z",
     "start_time": "2018-03-29T05:32:32.873819Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=49, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=58, shape=(), dtype=float32, numpy=nan>]\n"
     ]
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.log(1 + tf.exp(x))\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "\n",
    "# However it returns a `nan` at x = 100 due to numerical instability.\n",
    "print(grad_log1pexp(100.))\n",
    "# [nan]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将自定义梯度应用于上述函数，简化梯度表达式。注意下面的梯度函数实现重用了前向传导中计算的 (`tf.exp(x)`)，避免冗余计算，从而提高梯度计算的效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:33:59.166277Z",
     "start_time": "2018-03-29T05:33:59.146258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=68, shape=(), dtype=float32, numpy=0.5>]\n",
      "[<tf.Tensor: id=78, shape=(), dtype=float32, numpy=1.0>]\n"
     ]
    }
   ],
   "source": [
    "@tfe.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "\n",
    "    return tf.log(1 + e), grad\n",
    "\n",
    "\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "\n",
    "# Gradient at x = 0 works as before.\n",
    "print(grad_log1pexp(0.))\n",
    "# [0.5]\n",
    "\n",
    "# And now gradient computation at x=100 works as well.\n",
    "print(grad_log1pexp(100.))\n",
    "# [1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模\n",
    "\n",
    "模型可以按类别划分。这里有个模型，创建了一个 (简单) 双层网络，可以对标准 MNIST 手写数字进行分类：\n",
    "\n",
    "- 我们推荐在`tf.layers`使用类，因为他们可创立并包含模型参数（变量）。可变寿命与层对象的寿命相关联，所以一定要对其进行跟踪。.\n",
    "\n",
    "- 为何要使用`tfe.Network`呢？网络是层的容器，而且就是`tf.layer.Layer`本身，允许 `Network` 对象 嵌入其他`Network`对象中。它也包含辅助检查，保存和恢复的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:36:57.551730Z",
     "start_time": "2018-03-29T05:36:57.509697Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 784)\n",
      "tf.Tensor([[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class MNISTModel(tfe.Network):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = self.track_layer(tf.layers.Dense(units=10))\n",
    "        self.layer2 = self.track_layer(tf.layers.Dense(units=10))\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"Actually runs the model.\"\"\"\n",
    "        result = self.layer1(input)\n",
    "        result = self.layer2(result)\n",
    "        return result\n",
    "\n",
    "# 即使不对模型进行训练，我们也会调用它并检查输出：\n",
    "# Let's make up a blank input image\n",
    "\n",
    "\n",
    "model = MNISTModel()\n",
    "batch = tf.zeros([1, 1, 784])\n",
    "\n",
    "print(batch.shape)\n",
    "# (1, 1, 784)\n",
    "\n",
    "result = model(batch)\n",
    "print(result)\n",
    "# tf.Tensor([[[ 0.  0., ...., 0.]]], shape=(1, 1, 10), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:49:50.672995Z",
     "start_time": "2018-03-29T05:49:50.421552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 67.118286\n",
      "Loss at step 0: 64.562920\n",
      "Loss at step 20: 29.936848\n",
      "Loss at step 40: 14.214672\n",
      "Loss at step 60: 7.061634\n",
      "Loss at step 80: 3.800792\n",
      "Loss at step 100: 2.311368\n",
      "Loss at step 120: 1.629747\n",
      "Loss at step 140: 1.317222\n",
      "Loss at step 160: 1.173664\n",
      "Loss at step 180: 1.107602\n",
      "Loss at step 200: 1.077150\n",
      "Final loss: 1.077150\n",
      "W, B = 3.069365, 2.1580782\n"
     ]
    }
   ],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B\n",
    "\n",
    "\n",
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# Define:\n",
    "# 1. A model\n",
    "# 2. Derivatives of a loss function with respect to model parameters\n",
    "# 3. A strategy for updating the variables based on the derivatives\n",
    "model = Model()\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# The training loop\n",
    "print(\"Initial loss: %f\" %\n",
    "      loss(model, training_inputs, training_outputs).numpy())\n",
    "for i in range(201):\n",
    "    optimizer.apply_gradients(grad(model, training_inputs, training_outputs))\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" %\n",
    "              (i, loss(model, training_inputs, training_outputs).numpy()))\n",
    "print(\"Final loss: %f\" % loss(model, training_inputs, training_outputs).numpy())\n",
    "print(\"W, B = %s, %s\" % (model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，我们不需要任何占位符或会话（`session`）。在数据第一次输入时，就设置了层的参数的尺寸。\n",
    "\n",
    "训练任何模型我们都得定义一个损失函数来优化、计算梯度，然后使用优化器来更新变量。首先，这里是一个损失函数：\n",
    "`implicit_gradients()` 根据全部计算时所用的 TensorFlow 变量计算 loss_function 的导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)\n",
    "\n",
    "\n",
    "# 之后是我们的训练过程：\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "\n",
    "for (x, y) in tfe.Iterator(dataset):\n",
    "    grads = tfe.implicit_gradients(loss_function)(model, x, y)\n",
    "    optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Eager with Graphs\n",
    "\n",
    "Eager Execution 使开发和调试更加互动，但 TensorFlow 图拥有诸多关于分布式训练，性能优化和产品部署的优点。\n",
    "当启用 Eager Execution 时，执行运算的相同代码会构建一个图，描述其非计算时的情况。把你的模型转化为图，只要在未启用 Eager Execution 的新的 Python 模块中运行相同的代码即可，如你所见，在 MNIST example 这个例子中一样。模型变量的值可在还原点储存并恢复，使得我们可以在 Eager Execution（命令式）和图（陈述式）编程间切换自如。有了这个，使用 Eager Execution 开发的模型可以轻易导出用于产品部署。\n",
    "\n",
    "在不远的将来，我们将提供选择性地将模型的一部分转换为图的功能。用这个方式，你可以把你的计算（比如自定义RNN细胞的内部）同高性能相融合，但也保持灵活性和Eager Execution的可读性。\n",
    "\n",
    "## 如何修改我的代码？\n",
    "使用Eager Execution应对当前TensorFlow 使用者直观。目前只有少数针对Eager Execution的API；大多数现有API和计算工作都启用了Eager Execution。一些要点要记住： \n",
    "- 总得按 TensorFlow 来讲，如果你还没在输入进程中从队列换成使用 `tf.data` ，我们建议你该换了。这样更好用而且一般也更快。\n",
    "- 使用面向对象的层，比如 `tf.layer.Conv2D()` 或 `Keras` 层；这些都有变量的显式储存。\n",
    "\n",
    "对大多数模型而言，你可以写下代码，然后它就可以在 Eager Execution 和图构建情况下一样运行。也有一些例外，比如动态模型会使用 Python 控制流来改变基于输入的计算。\n",
    "\n",
    "一旦你调用了`tfe.enable_eager_execution()`，就没法关闭了。要获取图行为，就要新开一个 Python 模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU 运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:46:11.353446Z",
     "start_time": "2018-03-29T05:46:10.642446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Took 0.1629934310913086 seconds to multiply a (1000, 1000) matrix by itself 10 times\n",
      "GPU: Took 0.0 seconds to multiply a (1000, 1000) matrix by itself 10 times\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def measure(x):\n",
    "    # The very first time a GPU is used by TensorFlow, it is initialized.\n",
    "    # So exclude the first run from timing.\n",
    "    tf.matmul(x, x)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(10):\n",
    "        tf.matmul(x, x)\n",
    "    end = time.time()\n",
    "\n",
    "    return \"Took %s seconds to multiply a %s matrix by itself 10 times\" % (\n",
    "        end - start, x.shape)\n",
    "\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    print(\"CPU: %s\" % measure(tf.random_normal([1000, 1000])))\n",
    "\n",
    "# If a GPU is available, run on GPU:\n",
    "if tfe.num_gpus() > 0:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        print(\"GPU: %s\" % measure(tf.random_normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:47:45.613900Z",
     "start_time": "2018-03-29T05:47:45.406416Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.random_normal([10, 10])\n",
    "\n",
    "x_gpu = x.gpu()\n",
    "x_cpu = x.cpu()\n",
    "\n",
    "_ = tf.matmul(x_cpu, x_cpu)  # Runs on CPU\n",
    "_ = tf.matmul(x_gpu, x_gpu)  # Runs on GPU:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:48:17.070020Z",
     "start_time": "2018-03-29T05:48:17.053034Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return tf.multiply(x, x)  # Or x * x\n",
    "\n",
    "\n",
    "assert 9 == f(3.).numpy()\n",
    "\n",
    "df = tfe.gradients_function(f)\n",
    "assert 6 == df(3.)[0].numpy()\n",
    "\n",
    "# Second order deriviative.\n",
    "d2f = tfe.gradients_function(lambda x: df(x)[0])\n",
    "assert 2 == d2f(3.)[0].numpy()\n",
    "\n",
    "# Third order derivative: Will be None\n",
    "d3f = tfe.gradients_function(lambda x: d2f(x)[0])\n",
    "assert None == d3f(3.)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:48:41.750270Z",
     "start_time": "2018-03-29T05:48:41.531286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 67.932968\n",
      "Loss at step 0: 65.326385\n",
      "Loss at step 20: 30.088427\n",
      "Loss at step 40: 14.177371\n",
      "Loss at step 60: 6.984028\n",
      "Loss at step 80: 3.727696\n",
      "Loss at step 100: 2.251603\n",
      "Loss at step 120: 1.581550\n",
      "Loss at step 140: 1.276950\n",
      "Loss at step 160: 1.138275\n",
      "Loss at step 180: 1.075044\n",
      "Final loss: 1.047138\n",
      "W, B = 3.023534, 2.147858\n"
     ]
    }
   ],
   "source": [
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# A loss function: Mean-squared error\n",
    "\n",
    "\n",
    "def loss(weight, bias):\n",
    "    error = prediction(training_inputs, weight, bias) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "\n",
    "# Function that returns the derivative of loss with respect to\n",
    "# weight and bias\n",
    "grad = tfe.gradients_function(loss)\n",
    "\n",
    "# Train for 200 steps (starting from some random choice for W and B, on the same\n",
    "# batch of data).\n",
    "W = 5.\n",
    "B = 10.\n",
    "learning_rate = 0.01\n",
    "print(\"Initial loss: %f\" % loss(W, B).numpy())\n",
    "for i in range(200):\n",
    "    (dW, dB) = grad(W, B)\n",
    "    W -= dW * learning_rate\n",
    "    B -= dB * learning_rate\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (i, loss(W, B).numpy()))\n",
    "print(\"Final loss: %f\" % loss(W, B).numpy())\n",
    "print(\"W, B = %f, %f\" % (W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras and the Layers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:52:08.460838Z",
     "start_time": "2018-03-29T05:52:08.454834Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.layer = tf.layers.Dense(1)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:54:19.536739Z",
     "start_time": "2018-03-29T05:52:16.079932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-380c96d1de9c>:41: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:219: retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-19-380c96d1de9c>:36: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "No registered 'LogSoftmax' OpKernel for GPU devices compatible with node LogSoftmax = LogSoftmax[T=DT_FLOAT](dummy_input)\n\t.  Registered:  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n [Op:LogSoftmax]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-380c96d1de9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             print(\"Step %d: Loss on training set : %f\" %\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    282\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;34m\"\"\"Computes the gradient of the wrapped function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimplicit_val_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    232\u001b[0m                                            \u001b[0mthis_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                                            \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_node\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                                            sources)\n\u001b[0m\u001b[0;32m    235\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mend_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[1;32m---> 65\u001b[1;33m         tape._tape, vspace, target, sources, output_gradients, status)  # pylint: disable=protected-access\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgrad_fn\u001b[1;34m(*orig_outputs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0morig_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     result = _magic_gradient_function(op_name, attrs, num_inputs,\n\u001b[1;32m--> 142\u001b[1;33m                                       op_inputs, op_outputs, orig_outputs)\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_tracing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m       print(\"Gradient for\", op_name, \"inputs\", op_inputs, \"output_grads\",\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_magic_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_SoftmaxCrossEntropyWithLogitsGrad\u001b[1;34m(op, grad_loss, grad_grad)\u001b[0m\n\u001b[0;32m    473\u001b[0m              softmax)\n\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_BroadcastMul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m                 instructions)\n\u001b[1;32m--> 432\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    434\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(logits, axis, name, dim)\u001b[0m\n\u001b[0;32m   1766\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_softmax\u001b[1;34m(logits, compute_op, dim, name)\u001b[0m\n\u001b[0;32m   1671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1672\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_last_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1673\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompute_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m   \u001b[1;31m# If dim is the last dimension, simply reshape the logits to a matrix and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(logits, name)\u001b[0m\n\u001b[0;32m   4960\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4961\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4962\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: No registered 'LogSoftmax' OpKernel for GPU devices compatible with node LogSoftmax = LogSoftmax[T=DT_FLOAT](dummy_input)\n\t.  Registered:  device='CPU'; T in [DT_HALF]\n  device='CPU'; T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n [Op:LogSoftmax]"
     ]
    }
   ],
   "source": [
    "class MNISTModel(object):\n",
    "    def __init__(self, data_format):\n",
    "        # 'channels_first' is typically faster on GPUs\n",
    "        # while 'channels_last' is typically faster on CPUs.\n",
    "        # See: https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "        if data_format == 'channels_first':\n",
    "            self._input_shape = [-1, 1, 28, 28]\n",
    "        else:\n",
    "            self._input_shape = [-1, 28, 28, 1]\n",
    "        self.conv1 = tf.layers.Conv2D(32, 5,\n",
    "                                      padding='same',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      data_format=data_format)\n",
    "        self.max_pool2d = tf.layers.MaxPooling2D(\n",
    "            (2, 2), (2, 2), padding='same', data_format=data_format)\n",
    "        self.conv2 = tf.layers.Conv2D(64, 5,\n",
    "                                      padding='same',\n",
    "                                      activation=tf.nn.relu,\n",
    "                                      data_format=data_format)\n",
    "        self.dense1 = tf.layers.Dense(1024, activation=tf.nn.relu)\n",
    "        self.dropout = tf.layers.Dropout(0.5)\n",
    "        self.dense2 = tf.layers.Dense(10)\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        x = tf.reshape(inputs, self._input_shape)\n",
    "        x = self.max_pool2d(self.conv1(x))\n",
    "        x = self.max_pool2d(self.conv2(x))\n",
    "        x = tf.layers.flatten(x)\n",
    "        x = self.dropout(self.dense1(x))\n",
    "        return self.dense2(x)\n",
    "\n",
    "\n",
    "def loss(model, inputs, targets):\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=model.predict(inputs), labels=targets))\n",
    "\n",
    "\n",
    "# Load the training and validation data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"./mnist_data\", one_hot=True)\n",
    "\n",
    "# Train\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\n",
    "model = MNISTModel('channels_first' if tfe.num_gpus() else 'channels_last')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "for i in range(20001):\n",
    "    with tf.device(device):\n",
    "        (inputs, targets) = data.train.next_batch(50)\n",
    "        optimizer.apply_gradients(grad(model, inputs, targets))\n",
    "        if i % 100 == 0:\n",
    "            print(\"Step %d: Loss on training set : %f\" %\n",
    "                  (i, loss(model, inputs, targets).numpy()))\n",
    "print(\"Loss on test set: %f\" %\n",
    "      loss(model, data.test.images, data.test.labels).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing trained variables\n",
    "\n",
    "TensorFlow Variables (`tfe.Variable`) provide a way to represent shared, persistent state of your model. The `tfe.Checkpoint` class provides a means to save and restore variables to and from checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-29T05:54:46.269233Z",
     "start_time": "2018-03-29T05:54:46.033189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create variables.\n",
    "x = tfe.Variable(10.)\n",
    "y = tfe.Variable(5.)\n",
    "\n",
    "# Indicate that the variables should be saved as \"x\" and \"y\".\n",
    "checkpoint = tfe.Checkpoint(x=x, y=y)\n",
    "\n",
    "# Assign new values to the variables and save.\n",
    "x.assign(2.)\n",
    "save_path = checkpoint.save('/tmp/ckpt')\n",
    "\n",
    "# Change the variable after saving.\n",
    "x.assign(11.)\n",
    "assert 16. == (x + y).numpy()  # 11 + 5\n",
    "\n",
    "# Restore the values in the checkpoint.\n",
    "checkpoint.restore(save_path)  # save_path='/tmp/ckpt-1'\n",
    "\n",
    "assert 7. == (x + y).numpy()  # 2 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "177px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
