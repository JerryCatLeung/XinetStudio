{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn一些小技巧的记录（pipeline...）\n",
    "\n",
    "## `LabelEncoder`\n",
    "简单来说 LabelEncoder 是对不连续的数字或者文本进行编号："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:10:35.617534Z",
     "start_time": "2018-03-31T10:10:34.681539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit([1,5,67,100])\n",
    "le.transform([1,1,100,67,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `OneHotEncoder`\n",
    "OneHotEncoder 用于将表示分类的数据扩维："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:12:07.695719Z",
     "start_time": "2018-03-31T10:12:07.681715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit([[1], [2], [3], [4]])\n",
    "ohe.transform([[2], [3], [1], [4]]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如 keras 中的 `keras.utils.to_categorical(y_train, num_classes)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sklearn.model_selection.train_test_split` 随机划分训练集和测试集\n",
    "一般形式： \n",
    "`train_test_split`是交叉验证中常用的函数，功能是从样本中随机的按比例选取`train data`和`test data`，形式为：\n",
    "\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    train_data, train_target, test_size=0.4, random_state=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数解释： \n",
    "- `train_data`：所要划分的样本特征集 \n",
    "- `train_target`：所要划分的样本结果 \n",
    "- `test_size`：样本占比，如果是整数的话就是样本的数量 \n",
    "- `random_state`：是随机数的种子。 \n",
    "- 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填`1`，其他参数一样的情况下你得到的随机数组是一样的。但填`0`或不填，每次都会不一样。\n",
    "\n",
    "随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则： \n",
    "- 种子不同，产生不同的随机数；种子相同，即使实例不同也产生相同的随机数。\n",
    "\n",
    "```py\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train = loan_data.iloc[0: 55596, :]\n",
    "test = loan_data.iloc[55596:, :]\n",
    "# 避免过拟合，采用交叉验证，验证集占训练集20%，固定随机种子（random_state)\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    train, target, test_size=0.2, random_state=0)\n",
    "train_y = train_y['label']\n",
    "test_y = test_y['label']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pipeline`\n",
    "`pipeline` 实现了对全部步骤的流式化封装和管理，可以很方便地使参数集在新数据集上被重复使用。\n",
    "\n",
    "`pipeline` 可以用于下面几处：\n",
    "- 模块化 `Feature Transform`，只需写很少的代码就能将新的 `Feature` 更新到训练集中。\n",
    "- 自动化 `Grid Search`，只要预先设定好使用的 `Model` 和参数的候选，就能自动搜索并记录最佳的 `Model`。\n",
    "- 自动化 `Ensemble Generation`，每隔一段时间将现有最好的 `K `个 `Model` 拿来做 `Ensemble`。\n",
    "\n",
    "管道机制在机器学习算法中得以应用的根源在于，参数集在新数据集（比如测试集）上的重复使用。\n",
    "\n",
    "管道机制实现了对全部步骤的流式化封装和管理（streaming workflows with pipelines）。\n",
    "\n",
    "注意：管道机制更像是编程技巧的创新，而非算法的创新。\n",
    "\n",
    "接下来我们以一个具体的例子来演示sklearn库中强大的Pipeline用法：\n",
    "问题是要对数据集 Breast Cancer Wisconsin 进行分类， \n",
    "它包含 $569$ 个样本，第一列 `ID`，第二列类别(`M=恶性肿瘤，B=良性肿瘤`)， \n",
    "第 `3-32` 列是实数值的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:20:57.495284Z",
     "start_time": "2018-03-31T10:20:52.196191Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Breast Cancer Wisconsin dataset\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "    'breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "\n",
    "\n",
    "X, y = df.values[:, 2:], df.values[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:21:38.461204Z",
     "start_time": "2018-03-31T10:21:38.449204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)\n",
    "encoder.transform(['M', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:21:41.378289Z",
     "start_time": "2018-03-31T10:21:41.370296Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要用 Pipeline 对训练集和测试集进行如下操作：\n",
    "- 先用 StandardScaler 对数据集每一列做标准化处理，（是 transformer）\n",
    "- 再用 PCA 将原始的 `30` 维度特征压缩的 `2` 维度，（是 transformer）\n",
    "- 最后再用模型 LogisticRegression。（是 Estimator）\n",
    "- 调用 Pipeline 时，输入由元组构成的列表，每个元组第一个值为变量名，元组第二个元素是 sklearn 中的 transformer 或 Estimator。\n",
    "\n",
    "注意中间每一步是 transformer，即它们必须包含 `fit` 和 `transform` 方法，或者 `fit_transform`。 \n",
    "最后一步是一个 Estimator，即最后一步模型要有 `fit` 方法，可以没有 `transform`方法。\n",
    "然后用 Pipeline.fit对训练集进行训练，`pipe_lr.fit(X_train, y_train)` \n",
    "再直接用 Pipeline.score 对测试集进行预测并评分 `pipe_lr.score(X_test, y_test)`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:23:36.923594Z",
     "start_time": "2018-03-31T10:23:34.960584Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_lr = Pipeline([('sc', StandardScaler()),\n",
    "                    ('pca', PCA(n_components=2)),\n",
    "                    ('clf', LogisticRegression(random_state=1))\n",
    "                    ])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test accuracy: %.3f' % pipe_lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline对象接受二元tuple构成的list，每一个二元 tuple 中的第一个元素为 arbitrary identifier string，我们用以获取（access）Pipeline object 中的 individual elements，二元 tuple 中的第二个元素是 scikit-learn与之相适配的transformer 或者 estimator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
