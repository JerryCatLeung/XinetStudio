{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "- 《[Machine Learning](http://www.cs.cmu.edu/~tom/mlbook.html)》：卡内基梅隆大学 Tom M.Mitchell\n",
    "    - http://www.cs.cmu.edu/~tom/\n",
    "    - [Online Machine Learning software and datasets](http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/ml-examples.html)\n",
    "    - [Lecture slides for instructor](http://www.cs.cmu.edu/~tom/mlbook-chapter-slides.html)\n",
    "    - http://www.cs.cmu.edu/~tom/publications.html\n",
    "    \n",
    "    - 作业与课程：http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-3/www/ml.html\n",
    "\n",
    "## 相关术语\n",
    "\n",
    "### 人工智能\n",
    "\n",
    "学习概念的符号表示。作为搜索问题的机器学习。作为提高问题求解能力的学习。利用先验知识和训练数据一起引导学习\n",
    "\n",
    "### 贝叶斯方法\n",
    "\n",
    "作为计算假设概率基础的贝叶斯法则。朴素贝叶斯分类器。未观测到变量估计值的算法\n",
    "\n",
    "### 计算复杂性理论\n",
    "\n",
    "不同学习任务中固有的复杂性的理论边界，以计算量、训练样例数量、出错数量等衡量\n",
    "\n",
    "### 控制论\n",
    "\n",
    "为了优化预定目标，学习对各种处理过程进行控制，学习预测被控制的过程的下一个状态\n",
    "\n",
    "### 信息论\n",
    "\n",
    "熵和信息内容的度量。学习最小描述长度的方法。编码假设时，对最佳训练序列的最佳编码及其关系\n",
    "\n",
    "### 哲学\n",
    "\n",
    "“奥卡姆的剃刀（Occam's razor）”：最简单的假设是最好的。将观察到的数据泛化的理由分析\n",
    "\n",
    "### 心理学和神经生物学\n",
    "\n",
    "实践的**幂定律**（power law of practice）：该定律指出对于很大范围内的学习问题，人们的反应速度随着实践次数的幂级提高。激发人工神经网络学习模式的神经生物学研究\n",
    "\n",
    "### 统计学\n",
    "\n",
    "根据有限数据样本，对估计假设精度时出现的误差（例如，偏差和方差）的刻画。置信区间。统计检验\n",
    "\n",
    "## 定义\n",
    "\n",
    "如果一个计算机程序针对某类任务 $T$ 的用 $P$ 衡量的性能根据经验 $E$ 来自我完善，那么我们称这个计算机程序从经验 $E$ 中**学习**，针对某类任务 $T$，它的性能用 $P$ 来衡量。\n",
    "\n",
    "## Three niches for machine learning:\n",
    "- **Data mining** : using historical data to improve decisions\n",
    "     - medical records $\\rightarrow$ medical knowledge\n",
    "- Software applications we can't program by hand\n",
    "     - autonomous driving\n",
    "     - speech recognition\n",
    "- Self customizing programs\n",
    "    - Newsreader that learns user interests\n",
    "\n",
    "## Some Issues in Machine Learning\n",
    "\n",
    "- What algorithms can approximate functions well(and when)?\n",
    "- How does number of training examples influence accuracy?\n",
    "- How does complexity of hypothesis representation impact it?\n",
    "- How does noisy data influence accuracy?\n",
    "- What are the theoretical limits of learnability?\n",
    "- How can prior knowledge of learner help?\n",
    "- What clues can we get from biological learning systems?\n",
    "- How can systems alter their own representations?\n",
    "\n",
    "在机器学习方面，一个有效的观点是**机器学习问题经常归结于搜索问题，即对一个非常大的假设空间进行搜索，以确定一个最佳拟合观察到的数据和学习器已有知识的假设**。\n",
    "\n",
    "### 机器学习问题\n",
    "\n",
    "- 存在什么样的算法能从特定的训练数据学习一般的目标函数？如果提供了充足的训练数据，什么样的条件下会使特定的算法收敛到期望的函数？哪个算法对哪些问题和表示的性能最好？\n",
    "\n",
    "- 多少训练数据是充足？怎样找到学习到假设的置信度与训练数据的数量及提供给学习器的假设空间特性之间的一般关系？\n",
    "\n",
    "- 学习器拥有的先验知识是怎样引导从样例进行泛化的过程的？当先验知识仅仅是近似正确时，它们会有帮助吗？\n",
    "\n",
    "- 关于选择有效的后续训练经验，什么样的策略最好？这个策略的选择会如何影响学习问题的复杂性？\n",
    "\n",
    "- 怎样把学习任务简化为一个或多个函数逼近问题？换一种方式，系统该试图学习那些函数？这个过程本身能自动化吗？\n",
    "\n",
    "- 学习器怎样自动化地改变表示法来提高表示和学习目标函数的能力？\n",
    "\n",
    "机器学习算法的设计过程中包含许多选择，包括选择训练经验的类型、要学习的目标函数、该目标函数的表示形式以及从训练样例中学习目标函数的算法。\n",
    "\n",
    "学习的过程即搜索的过程，搜索包含可能假设的空间，使得到的假设最符合已有的训练样例和其他预先的约束或知识。\n",
    "\n",
    "机器学习的**中心问题**：从特殊的训练样例中归纳出一般函数。\n",
    "\n",
    "# 概念学习和一般到特殊序\n",
    "\n",
    "定义：**概念学习**是指从有关某个布尔函数的输入输出训练样例中推断出该布尔函数。\n",
    "\n",
    "多数情况下，为了高效的搜索，可以利用假设空间中的一种自然形成的结构——即一般到特殊偏序结构。\n",
    "\n",
    "每个**概念**可以看作一个对象或事件集合，它是从更大的集合中选取的子集，或者是在较大的集合中定义的布尔函数。\n",
    "\n",
    "**归纳学习假设**：任一假设如果在足够大的训练样例集中很好地逼近目标函数，它也能在未见实例中很好地逼近目标函数。\n",
    "\n",
    "## 作为搜索的概念学习\n",
    "\n",
    "概念学习可以看作是一个搜索的过程，范围是假设的表示所隐含定义的整个空间。搜索的目标是为了寻找能最好地拟合训练样例的假设。当然，当假设的表示形式选定后，那么也就隐含地为学习算法确定了所有假定的空间。\n",
    "## 术语定义\n",
    "- 概念定义在一个**实例**（instance）集合上，这个集合表示为 $X$。\n",
    "- 待学习的概念或函数称为**目标概念**（target concept），记作 $c: X \\rightarrow \\{0, 1\\}$。\n",
    "    - $c(x) = 1$的实例被称为**正例（positive example）**，或称为目标概念的成员。\n",
    "    - $c(x) = 0$的实例被称为**反例（negative example）**，或称为非目标概念的成员。\n",
    "- 训练样例的集合 $D: <x, c(x)>$。\n",
    "- $H$ 表示所有可能假设（hypothesis）的集合。$h: X \\rightarrow \\{0, 1\\} \\\\ h\\in H$\n",
    "\n",
    "对于 $X$ 中的任意实例 $x$ 和 $H$ 中的任意假设 $h$，我们说当且仅当 $h(x) = 1$ 时 $x$ 满足 $h$。\n",
    "\n",
    "定义：令 $h_j$ 和 $h_k$ 为在 $X$ 上定义的布尔函数。称 $h_j$ **more_general_than_or_equal_to** $h_k$（记作 $h_j \\geqslant _g h_k$），当且仅当\n",
    "$$(\\forall x\\in X)[h_k(x) = 1 \\rightarrow h_j(x) = 1]$$\n",
    "\n",
    "易知 $\\geqslant _g$ 是一种偏序关系！\n",
    "\n",
    "## Find-S Algorithm（寻找极大特殊假设）\n",
    "1. Initialize `h` to the most specific hypothesis in `H`\n",
    "2. For each positive training instance $x$\n",
    "    - For each attribute constraint $a_i \\in h$\n",
    "    - If the constraint  $a_i \\in h$ is satisfied by $x$\n",
    "    - Then do nothing\n",
    "    - Else replace $a_i \\in h$ by the next more general constraint that is satisfied by $x$\n",
    "3. Output hypothesis $h$\n",
    "\n",
    "## 变型空间和候选消除算法\n",
    "定义：一个假设 $h$ 与训练样例集合 $D$ **一致**，当且仅当对 $D$ 中的每一个样例 $<x, c(x)>$ 都有 $h(x) = c(x)$。\n",
    "$$Consistent(h, D) \\equiv (\\forall <x, c(x)> \\in D) \\,h(x) = c(x)$$\n",
    "\n",
    "候选消除算法能够表示与训练样例 $D$ 一致的所有假设。在假设空间中这一子集被称为关于假设空间 $H$ 和训练样例 $D$ 的*变型空间*（version space）,因为它包含了目标概念的所有合理的变型。\n",
    "\n",
    "定义：关于假设空间 $H$ 和训练样例 $D$ 的**变型空间**，标记为 $VS_{H, D}$，是 $H$ 中与训练样例 $D$ 一致的所有假设构成的子集。\n",
    "$$VS_{H, D} \\equiv \\{h \\in H|Consistent(h, D)\\}$$\n",
    "\n",
    "### The List-Then-Eliminate Algorithm（列表后消除算法）\n",
    "\n",
    "1. 变型空间 Version Space $\\leftarrow$ a list containing every hypothesis in H\n",
    "2. For each training example,  $<x, c(x)>$\n",
    "    - remove from Version Space any hypothesis $h$ for which $h(x) \\neq c(x)$\n",
    "3. Output the list of hypotheses in Version Space\n",
    "\n",
    "### Representing Version Spaces\n",
    "\n",
    "定义：关于假设空间 $H$ 和训练数据 $D$ 的**一般边界**（General boundary）$G$，是在 $H$ 中与 $D$ 相一致的极大一般（maximally general）成员的集合。\n",
    "\n",
    "定义：关于假设空间 $H$ 和训练数据 $D$ 的**特殊边界**（specific boundary）$S$，是在 $H$ 中与 $D$ 相一致的极大特殊（maximally specific）成员的集合。\n",
    "\n",
    "定理： 变型空间表示定理：\n",
    "$$VS_{H, D} = \\{h \\in H| (\\exists s \\in S)(\\exists g \\in G)(g \\geqslant _g h \\geqslant _g s)\\}$$\n",
    "\n",
    "# Decision Tree Learning（决策树学习）\n",
    "\n",
    "参考资料:\n",
    "- [机器学习--决策树](http://www.cnblogs.com/banshaohuan/p/7262329.html)\n",
    "- [决策树](http://blog.leanote.com/post/elag/决策树)\n",
    "- [码农场：决策树](http://www.hankcs.com/ml/decision-tree.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T03:53:58.918776Z",
     "start_time": "2018-02-27T03:53:53.174505Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过`export_graphviz`将决策树可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T04:06:33.119112Z",
     "start_time": "2018-02-27T04:06:33.112093Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以利用 `graphviz` 将输出的 `.dot` 文件进行转化。\n",
    "\n",
    "转化 `dot` 文件至 `pdf` 可视化决策树，在 `cmd`中执行：`dot -Tpdf iris.dot（要转换的文件） -o outpu.pdf（转换成的文件）`\n",
    "\n",
    "决策树是一种逼近离散值目标函数的方法，在这种方法中学习到的函数被表示为一棵决策树。学习得到的决策树也能表示为多个 `if-then` 的规则，以提高可读性。\n",
    "\n",
    "\n",
    "Decision tree representation:\n",
    "- Each internal node tests an attribute\n",
    "- Each branch corresponds to attribute value\n",
    "- Each leaf node assigns a classification\n",
    "\n",
    "实现方法：合取（conjunction）& 析取（disjunction）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "287px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
