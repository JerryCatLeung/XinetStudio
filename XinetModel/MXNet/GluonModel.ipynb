{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T13:44:17.488674Z",
     "start_time": "2018-02-03T13:44:14.861694Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "c:\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n",
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:82: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:86: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon, nd, autograd\n",
    "root= 'E:/Data/MXNet/fashion-mnist'\n",
    "\n",
    "def transform(data, label):\n",
    "        '''转换为 `float32` 数据类型'''\n",
    "        return nd.transpose(data.astype('float32'), (2, 0, 1)) / 255, label.astype('float32')\n",
    "    \n",
    "mnist_train = gluon.data.vision.FashionMNIST(root, train= True, transform= transform)\n",
    "mnist_test = gluon.data.vision.FashionMNIST(root, train= False, transform= transform)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle= True)\n",
    "test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T13:44:20.695378Z",
     "start_time": "2018-02-03T13:44:20.579065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (256, 1, 28, 28) \n",
      "label.shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_data:\n",
    "    # change data from batch x height x weight x channel to batch x channel x height x weight\n",
    "    print('data.shape: {} \\nlabel.shape: {}'.format(data.shape, label.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型\n",
    "\n",
    "因为卷积网络计算比全连接要复杂，这里我们默认使用 GPU 来计算。如果 GPU 不能用，默认使用CPU。（下面这段代码会保存在 `utils.py` 里可以下次重复使用）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T13:45:46.722199Z",
     "start_time": "2018-02-03T13:45:46.713059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "try:\n",
    "    ctx = mx.gpu()\n",
    "    _ = nd.zeros((1,), ctx= ctx)\n",
    "except:\n",
    "    ctx = mx.cpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T13:45:47.845681Z",
     "start_time": "2018-02-03T13:45:47.828637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2D(None -> 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (1): Dropout(p = 0.2)\n",
       "  (2): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n",
       "  (3): Conv2D(None -> 50, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (4): Dropout(p = 0.5)\n",
       "  (5): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n",
       "  (6): Conv2D(None -> 50, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (7): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)\n",
       "  (8): Flatten\n",
       "  (9): Dense(None -> 128, Activation(relu))\n",
       "  (10): Dense(None -> 10, linear)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "drop_prob1 = 0.2\n",
    "drop_prob2 = 0.5\n",
    "\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Conv2D(channels= 20, kernel_size= 5, activation= 'relu'),\n",
    "        nn.Dropout(drop_prob1),\n",
    "        nn.MaxPool2D(pool_size= 2, strides= 2),\n",
    "        nn.Conv2D(channels= 50, kernel_size= 3, activation= 'relu'),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.MaxPool2D(pool_size= 2, strides= 2),\n",
    "        nn.Conv2D(channels= 50, kernel_size= 1, activation= 'relu'),\n",
    "        nn.MaxPool2D(pool_size= 2, strides= 2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(128, activation= 'relu'),\n",
    "        nn.Dense(10)\n",
    "    )\n",
    "    \n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T11:20:55.116326Z",
     "start_time": "2018-02-03T11:20:55.084209Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(output):\n",
    "    exp = nd.exp(output)\n",
    "    return exp/exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(yhat, y):\n",
    "    '''效果与 `y` 做了 `one-hot` 相同'''\n",
    "    return - nd.pick(nd.log(yhat), y)\n",
    "\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] -= lr * param.grad \n",
    "        \n",
    "def accuracy(output, label):\n",
    "    return nd.mean(output.argmax(axis= 1)==label).asscalar()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx):\n",
    "    acc = nd.array([0.], ctx= ctx)\n",
    "    n = 0.\n",
    "    if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "        data_iterator.reset()\n",
    "    for data, label in data_iterator:\n",
    "        label = label.as_in_context(ctx)\n",
    "        data = data.as_in_context(ctx)\n",
    "        acc += nd.sum(net(data).argmax(axis=1)==label)\n",
    "        n += len(label)\n",
    "        acc.wait_to_read() # don't push too many operators into backend\n",
    "    return acc.asscalar() / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T12:33:47.340599Z",
     "start_time": "2018-02-03T11:58:10.630718Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.77069, Train acc 0.320296, Test acc 0.5274, Time 21.5499 sec\n",
      "Epoch 1. Loss: 0.833003, Train acc 0.671094, Test acc 0.7191, Time 24.0705 sec\n",
      "Epoch 2. Loss: 0.642285, Train acc 0.757064, Test acc 0.7938, Time 24.0359 sec\n",
      "Epoch 3. Loss: 0.568683, Train acc 0.78212, Test acc 0.8037, Time 24.067 sec\n",
      "Epoch 4. Loss: 0.533042, Train acc 0.798327, Test acc 0.8082, Time 24.2194 sec\n",
      "Epoch 5. Loss: 0.50936, Train acc 0.806521, Test acc 0.823, Time 24.076 sec\n",
      "Epoch 6. Loss: 0.48836, Train acc 0.81486, Test acc 0.8331, Time 24.2705 sec\n",
      "Epoch 7. Loss: 0.473143, Train acc 0.82223, Test acc 0.8188, Time 24.8423 sec\n",
      "Epoch 8. Loss: 0.454439, Train acc 0.828618, Test acc 0.8415, Time 23.9647 sec\n",
      "Epoch 9. Loss: 0.447066, Train acc 0.83176, Test acc 0.8309, Time 24.3798 sec\n",
      "Epoch 10. Loss: 0.434089, Train acc 0.836032, Test acc 0.835, Time 24.4589 sec\n",
      "Epoch 11. Loss: 0.42686, Train acc 0.838752, Test acc 0.8349, Time 24.3999 sec\n",
      "Epoch 12. Loss: 0.419692, Train acc 0.843323, Test acc 0.8522, Time 24.3563 sec\n",
      "Epoch 13. Loss: 0.41095, Train acc 0.847529, Test acc 0.8457, Time 24.0219 sec\n",
      "Epoch 14. Loss: 0.402994, Train acc 0.848122, Test acc 0.8504, Time 24.2264 sec\n",
      "Epoch 15. Loss: 0.396424, Train acc 0.851701, Test acc 0.8638, Time 24.1542 sec\n",
      "Epoch 16. Loss: 0.395626, Train acc 0.851169, Test acc 0.8564, Time 24.4511 sec\n",
      "Epoch 17. Loss: 0.389168, Train acc 0.855175, Test acc 0.8642, Time 24.4379 sec\n",
      "Epoch 18. Loss: 0.383598, Train acc 0.857131, Test acc 0.8289, Time 22.6452 sec\n",
      "Epoch 19. Loss: 0.376256, Train acc 0.859092, Test acc 0.8496, Time 21.0351 sec\n",
      "Epoch 20. Loss: 0.37693, Train acc 0.858223, Test acc 0.8578, Time 21.0581 sec\n",
      "Epoch 21. Loss: 0.37209, Train acc 0.859015, Test acc 0.8459, Time 21.4039 sec\n",
      "Epoch 22. Loss: 0.37093, Train acc 0.859929, Test acc 0.8649, Time 21.0489 sec\n",
      "Epoch 23. Loss: 0.36612, Train acc 0.862118, Test acc 0.8488, Time 20.8725 sec\n",
      "Epoch 24. Loss: 0.364317, Train acc 0.86527, Test acc 0.8648, Time 21.3048 sec\n",
      "Epoch 25. Loss: 0.358204, Train acc 0.866949, Test acc 0.8398, Time 20.9608 sec\n",
      "Epoch 26. Loss: 0.357072, Train acc 0.866584, Test acc 0.8674, Time 21.4661 sec\n",
      "Epoch 27. Loss: 0.35242, Train acc 0.868567, Test acc 0.8632, Time 20.8903 sec\n",
      "Epoch 28. Loss: 0.349034, Train acc 0.869553, Test acc 0.8709, Time 20.5951 sec\n",
      "Epoch 29. Loss: 0.345748, Train acc 0.871398, Test acc 0.8587, Time 20.3942 sec\n",
      "Epoch 30. Loss: 0.344985, Train acc 0.870279, Test acc 0.8782, Time 20.5688 sec\n",
      "Epoch 31. Loss: 0.343974, Train acc 0.871471, Test acc 0.8668, Time 21.164 sec\n",
      "Epoch 32. Loss: 0.340604, Train acc 0.871814, Test acc 0.8665, Time 20.5637 sec\n",
      "Epoch 33. Loss: 0.340306, Train acc 0.873338, Test acc 0.8792, Time 20.6329 sec\n",
      "Epoch 34. Loss: 0.339007, Train acc 0.872955, Test acc 0.879, Time 20.9191 sec\n",
      "Epoch 35. Loss: 0.338311, Train acc 0.872878, Test acc 0.8743, Time 21.7496 sec\n",
      "Epoch 36. Loss: 0.33547, Train acc 0.875682, Test acc 0.8683, Time 21.5175 sec\n",
      "Epoch 37. Loss: 0.332813, Train acc 0.875947, Test acc 0.8737, Time 21.4719 sec\n",
      "Epoch 38. Loss: 0.333617, Train acc 0.876369, Test acc 0.8763, Time 21.4964 sec\n",
      "Epoch 39. Loss: 0.329478, Train acc 0.876108, Test acc 0.8886, Time 20.6113 sec\n",
      "Epoch 40. Loss: 0.328838, Train acc 0.876817, Test acc 0.8786, Time 20.7524 sec\n",
      "Epoch 41. Loss: 0.327447, Train acc 0.878114, Test acc 0.8751, Time 20.4404 sec\n",
      "Epoch 42. Loss: 0.324969, Train acc 0.879543, Test acc 0.8751, Time 20.8213 sec\n",
      "Epoch 43. Loss: 0.325054, Train acc 0.877804, Test acc 0.8596, Time 21.9505 sec\n",
      "Epoch 44. Loss: 0.322671, Train acc 0.879438, Test acc 0.8773, Time 20.9548 sec\n",
      "Epoch 45. Loss: 0.321913, Train acc 0.8792, Test acc 0.893, Time 21.0249 sec\n",
      "Epoch 46. Loss: 0.320485, Train acc 0.880524, Test acc 0.8815, Time 20.4514 sec\n",
      "Epoch 47. Loss: 0.319372, Train acc 0.880995, Test acc 0.8851, Time 20.3812 sec\n",
      "Epoch 48. Loss: 0.316172, Train acc 0.882369, Test acc 0.8841, Time 20.5978 sec\n",
      "Epoch 49. Loss: 0.319279, Train acc 0.880807, Test acc 0.8876, Time 21.2615 sec\n",
      "Epoch 50. Loss: 0.316321, Train acc 0.882818, Test acc 0.8738, Time 20.4424 sec\n",
      "Epoch 51. Loss: 0.316544, Train acc 0.880934, Test acc 0.8895, Time 20.4975 sec\n",
      "Epoch 52. Loss: 0.315086, Train acc 0.883029, Test acc 0.893, Time 20.3852 sec\n",
      "Epoch 53. Loss: 0.312408, Train acc 0.883754, Test acc 0.8916, Time 20.5191 sec\n",
      "Epoch 54. Loss: 0.310072, Train acc 0.883993, Test acc 0.8739, Time 20.3321 sec\n",
      "Epoch 55. Loss: 0.310226, Train acc 0.884924, Test acc 0.8907, Time 20.5578 sec\n",
      "Epoch 56. Loss: 0.308567, Train acc 0.885079, Test acc 0.8874, Time 20.5647 sec\n",
      "Epoch 57. Loss: 0.309655, Train acc 0.884441, Test acc 0.8709, Time 20.67 sec\n",
      "Epoch 58. Loss: 0.310019, Train acc 0.884785, Test acc 0.8908, Time 20.705 sec\n",
      "Epoch 59. Loss: 0.30834, Train acc 0.884757, Test acc 0.8872, Time 20.7863 sec\n",
      "Epoch 60. Loss: 0.312522, Train acc 0.883217, Test acc 0.8762, Time 20.8033 sec\n",
      "Epoch 61. Loss: 0.310071, Train acc 0.885417, Test acc 0.886, Time 20.7341 sec\n",
      "Epoch 62. Loss: 0.304424, Train acc 0.885594, Test acc 0.8778, Time 20.7662 sec\n",
      "Epoch 63. Loss: 0.306981, Train acc 0.88402, Test acc 0.8883, Time 20.3441 sec\n",
      "Epoch 64. Loss: 0.304841, Train acc 0.886968, Test acc 0.8875, Time 20.6674 sec\n",
      "Epoch 65. Loss: 0.304182, Train acc 0.887622, Test acc 0.882, Time 20.3752 sec\n",
      "Epoch 66. Loss: 0.303414, Train acc 0.886813, Test acc 0.8913, Time 20.5677 sec\n",
      "Epoch 67. Loss: 0.30382, Train acc 0.88596, Test acc 0.8895, Time 21.1171 sec\n",
      "Epoch 68. Loss: 0.302771, Train acc 0.888514, Test acc 0.8874, Time 20.5416 sec\n",
      "Epoch 69. Loss: 0.301604, Train acc 0.887666, Test acc 0.8757, Time 20.4273 sec\n",
      "Epoch 70. Loss: 0.301876, Train acc 0.8876, Test acc 0.884, Time 20.3852 sec\n",
      "Epoch 71. Loss: 0.300443, Train acc 0.887882, Test acc 0.8805, Time 20.5928 sec\n",
      "Epoch 72. Loss: 0.301419, Train acc 0.888237, Test acc 0.8948, Time 20.697 sec\n",
      "Epoch 73. Loss: 0.297902, Train acc 0.888098, Test acc 0.8938, Time 20.8284 sec\n",
      "Epoch 74. Loss: 0.298835, Train acc 0.889301, Test acc 0.8927, Time 20.4995 sec\n",
      "Epoch 75. Loss: 0.300231, Train acc 0.888935, Test acc 0.8867, Time 20.6399 sec\n",
      "Epoch 76. Loss: 0.29902, Train acc 0.887278, Test acc 0.894, Time 20.3581 sec\n",
      "Epoch 77. Loss: 0.298792, Train acc 0.888736, Test acc 0.8856, Time 20.673 sec\n",
      "Epoch 78. Loss: 0.297091, Train acc 0.888736, Test acc 0.8826, Time 20.702 sec\n",
      "Epoch 79. Loss: 0.295823, Train acc 0.890509, Test acc 0.8948, Time 20.5567 sec\n",
      "Epoch 80. Loss: 0.295712, Train acc 0.889262, Test acc 0.8968, Time 20.7159 sec\n",
      "Epoch 81. Loss: 0.296014, Train acc 0.89098, Test acc 0.8851, Time 20.7933 sec\n",
      "Epoch 82. Loss: 0.295716, Train acc 0.889755, Test acc 0.884, Time 20.7131 sec\n",
      "Epoch 83. Loss: 0.295701, Train acc 0.889611, Test acc 0.8948, Time 20.2549 sec\n",
      "Epoch 84. Loss: 0.296639, Train acc 0.889013, Test acc 0.8915, Time 20.3682 sec\n",
      "Epoch 85. Loss: 0.294551, Train acc 0.889977, Test acc 0.886, Time 20.3952 sec\n",
      "Epoch 86. Loss: 0.292122, Train acc 0.890686, Test acc 0.8942, Time 20.5236 sec\n",
      "Epoch 87. Loss: 0.292934, Train acc 0.890824, Test acc 0.8802, Time 20.3802 sec\n",
      "Epoch 88. Loss: 0.293516, Train acc 0.889694, Test acc 0.8912, Time 20.4474 sec\n",
      "Epoch 89. Loss: 0.293862, Train acc 0.889822, Test acc 0.8824, Time 20.4865 sec\n",
      "Epoch 90. Loss: 0.293352, Train acc 0.891345, Test acc 0.8903, Time 20.4815 sec\n",
      "Epoch 91. Loss: 0.292384, Train acc 0.891379, Test acc 0.8912, Time 20.4454 sec\n",
      "Epoch 92. Loss: 0.29222, Train acc 0.890786, Test acc 0.8846, Time 20.5677 sec\n",
      "Epoch 93. Loss: 0.292492, Train acc 0.890614, Test acc 0.8923, Time 20.8333 sec\n",
      "Epoch 94. Loss: 0.289938, Train acc 0.89273, Test acc 0.8667, Time 21.066 sec\n",
      "Epoch 95. Loss: 0.290499, Train acc 0.892265, Test acc 0.8889, Time 20.6258 sec\n",
      "Epoch 96. Loss: 0.28905, Train acc 0.89257, Test acc 0.8841, Time 20.5667 sec\n",
      "Epoch 97. Loss: 0.289611, Train acc 0.892459, Test acc 0.8934, Time 20.6589 sec\n",
      "Epoch 98. Loss: 0.291012, Train acc 0.892514, Test acc 0.8873, Time 20.9768 sec\n",
      "Epoch 99. Loss: 0.287125, Train acc 0.89293, Test acc 0.8886, Time 20.6208 sec\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "weight_decay = 0.001    # 正则化项系数\n",
    "lr = 0.2              # 学习率\n",
    "\n",
    "net.initialize(ctx= ctx)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'SGD', {'learning_rate': lr, 'wd': weight_decay})\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    m = len(train_data)\n",
    "    \n",
    "    start = time()\n",
    "    for data, label in train_data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        data = data.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        # 将梯度做平均，这样学习率会对 batch size 不那么敏感\n",
    "        trainer.step(batch_size)\n",
    "        \n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, net, ctx)\n",
    "    print((\"Epoch %d. Loss: %g, Train acc %g, Test acc %g, Time %g sec\" % (\n",
    "            epoch, train_loss/m, train_acc/m, test_acc, time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存训练期间最高准确率的模型\n",
    "\n",
    "```py\n",
    "best_acc = 0\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    # validation\n",
    "    val_acc = acc_function\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        # save model\n",
    "        model.save_params()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T14:18:02.369675Z",
     "start_time": "2018-02-03T14:18:02.330571Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None):\n",
    "    \"\"\"Train a network\"\"\"\n",
    "    print((\"Start training on \", ctx))\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        if isinstance(train_data, mx.io.MXDataIter):\n",
    "            train_data.reset()\n",
    "        start = time()\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        m = 0.\n",
    "        n = len(train_data)\n",
    "\n",
    "        start = time()\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            data = data.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            # 将梯度做平均，这样学习率会对 batch size 不那么敏感\n",
    "            trainer.step(batch_size)\n",
    "            m += len(label)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            train_acc += accuracy(output, label)\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_data, net, ctx)\n",
    "        print((\"Epoch %d. Loss: %g, Train acc %g, Test acc %g, Time %g sec\" % (\n",
    "                epoch, train_loss/m, train_acc/n, test_acc, time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
