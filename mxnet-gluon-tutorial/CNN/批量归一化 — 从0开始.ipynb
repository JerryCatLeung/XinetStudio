{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化 --- 从0开始\n",
    "\n",
    "在实际应用中，我们通常将输入数据的每个样本或者每个特征进行归一化，就是将均值变为 $0$ 方差变为 $1$，来使得数值更稳定。\n",
    "\n",
    "这个对\n",
    "我们在之前的课程里学过了[线性回归](../chapter_supervised-learning/linear-regression-\n",
    "scratch.md)和[逻辑回归](../chapter_supervised-learning/softmax-regression-\n",
    "scratch.md)很有效。因为输入层的输入值的大小变化不剧烈，那么输入也不会。但是，对于一个可能有很多层的深度学习模型来说，情况可能会比较复杂。\n",
    "\n",
    "举个例子，随着第一层和第二层的参数在训练时不断变化，第三层所使用的激活函数的输入值可能由于乘法效应而变得极大或极小，例如和第一层所使用的激活函数的输入值不在一个数量级上。这种在训练时可能出现的情况会造成模型训练的不稳定性。例如，给定一个学习率，某次参数迭代后，目标函数值会剧烈变化或甚至升高。数学的解释是，如果把目标函数\n",
    "$f$ 根据参数 $\\mathbf{w}$ 迭代（如 $f(\\mathbf{w} - \\eta \\nabla f(\\mathbf{w}))$\n",
    "）进行泰勒展开，有关学习率 $\\eta$ 的高阶项的系数可能由于数量级的原因（通常由于层数多）而不容忽略。然而常用的低阶优化算法（如梯度下降）对于不断降低目标函\n",
    "数的有效性通常基于一个基本假设：在以上泰勒展开中把有关学习率的高阶项通通忽略不计。\n",
    "\n",
    "为了应对上述这种情况，**Sergey Ioffe** 和 **Christian Szegedy** 在 2015 年提出了批量归一化的方法。简而言之，在训练时给定一个批量输入，批量归一化试图对深度学习模型的某一层所使用的激活函数的输入进行归一化：使批量呈标准正态分布（均值为 $0$，标准差为 $1$）。\n",
    "\n",
    "批量归一化通常应用于输入层或任意中间层。\n",
    "\n",
    "## 简化的批量归一化层\n",
    "\n",
    "给定一个批量 $B = \\{x_1 \\cdots x_m \\}$, 我们需要学习拉升参数 $\\gamma$ 和偏移参数 $\\beta$。\n",
    "\n",
    "我们定义：\n",
    "\n",
    "$$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m}x_i$$\n",
    "$$\\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(x_i - \\mu_B)^2$$\n",
    "$$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y_i \\leftarrow \\gamma \\hat{x_i} + \\beta \\equiv \\mbox{BN}_{\\gamma,\\beta}(x_i)$$\n",
    "\n",
    "批量归一化层的输出是 $\\{y_i = BN_{\\gamma, \\beta}(x_i)\\}$。\n",
    "\n",
    "我们现在来动手实现一个简化的批量归一化层。实现时对全连接层和二维卷积层两种情况做了区分。对于全连接层，很明显我们要对每个批量进行归一化。然而这里需要注意的是，对\n",
    "于二维卷积，我们要对每个通道进行归一化，并需要保持四维形状使得可以正确地广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:04:06.206612Z",
     "start_time": "2018-02-03T15:04:04.310570Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "c:\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "\n",
    "def pure_batch_norm(X, gamma, beta, eps=1e-5):\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    \n",
    "    # 全连接: batch_size x feature\n",
    "    if len(X.shape) == 2:\n",
    "        # 每个输入维度在样本上的平均 和 方差\n",
    "        mean = X.mean(axis= 0)\n",
    "        variance = ((X - mean)**2).mean(axis=0)\n",
    "        \n",
    "    # 2D卷积: batch_size x channel x height x width\n",
    "    else:\n",
    "        # 对每个通道算均值和方差，需要保持 4D 形状使得可以正确地广播\n",
    "        mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "        variance = ((X - mean)**2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "\n",
    "    # 均一化\n",
    "    X_hat = (X - mean) / nd.sqrt(variance + eps)\n",
    "    \n",
    "    # 伸缩和偏移\n",
    "    return gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们检查一下。我们先定义全连接层的输入是这样的。每一行是批量中的一个实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:04:49.395471Z",
     "start_time": "2018-02-03T15:04:49.389457Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 1.]\n",
       " [2. 3.]\n",
       " [4. 5.]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.arange(6).reshape((3,2))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们希望批量中的每一列都被归一化。结果符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T12:58:41.087826Z",
     "start_time": "2018-02-03T12:58:41.059742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-1.2247427 -1.2247427]\n",
       " [ 0.         0.       ]\n",
       " [ 1.2247427  1.2247427]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(A, gamma=nd.array([1,1]), beta=nd.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们定义二维卷积网络层的输入是这样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:17:12.845966Z",
     "start_time": "2018-02-03T15:17:12.838947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 0.  1.  2.]\n",
       "   [ 3.  4.  5.]\n",
       "   [ 6.  7.  8.]]\n",
       "\n",
       "  [[ 9. 10. 11.]\n",
       "   [12. 13. 14.]\n",
       "   [15. 16. 17.]]]\n",
       "\n",
       "\n",
       " [[[18. 19. 20.]\n",
       "   [21. 22. 23.]\n",
       "   [24. 25. 26.]]\n",
       "\n",
       "  [[27. 28. 29.]\n",
       "   [30. 31. 32.]\n",
       "   [33. 34. 35.]]]]\n",
       "<NDArray 2x2x3x3 @cpu(0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = nd.arange(36).reshape((2,2,3,3))\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果也如预期那样，我们对每个通道做了归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:17:24.238339Z",
     "start_time": "2018-02-03T15:17:24.230317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[-1.3884367  -1.2816339  -1.174831  ]\n",
       "   [-1.0680282  -0.9612254  -0.85442257]\n",
       "   [-0.74761975 -0.6408169  -0.5340141 ]]\n",
       "\n",
       "  [[-1.3884367  -1.2816339  -1.174831  ]\n",
       "   [-1.0680282  -0.9612254  -0.85442257]\n",
       "   [-0.74761975 -0.6408169  -0.5340141 ]]]\n",
       "\n",
       "\n",
       " [[[ 0.5340141   0.6408169   0.74761975]\n",
       "   [ 0.85442257  0.9612254   1.0680282 ]\n",
       "   [ 1.174831    1.2816339   1.3884367 ]]\n",
       "\n",
       "  [[ 0.5340141   0.6408169   0.74761975]\n",
       "   [ 0.85442257  0.9612254   1.0680282 ]\n",
       "   [ 1.174831    1.2816339   1.3884367 ]]]]\n",
       "<NDArray 2x2x3x3 @cpu(0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(B, gamma=nd.array([1,1]), beta=nd.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量归一化层\n",
    "\n",
    "你可能会想，既然训练时用了批量归一化，那么测试时也该用批量归一化吗？其实这个问题乍一想不是很好回答，因为：\n",
    "\n",
    "* 不用的话，训练出的模型参数很可能在测试时就不准确了；\n",
    "* 用的话，万一测试的数据就只有一个数据实例就不好办了。\n",
    "\n",
    "事实上，在测试时我们还是需要继续使用批量归一化的，只是需要做些改动。在测试时，我们需要把原先训练时用到的批量均值和方差替换成**整个**训练数据的均值和方差。但\n",
    "是当训练数据极大时，这个计算开销很大。因此，我们用移动平均的方法来近似计算（参见实现中的`moving_mean`和`moving_variance`）。\n",
    "\n",
    "为了方便讨论批量归一化层的实现，我们先看下面这段代码来理解``Python``变量可以如何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:21:08.026833Z",
     "start_time": "2018-02-03T15:21:07.981712Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, is_training, moving_mean, moving_variance,\n",
    "               eps = 1e-5, moving_momentum = 0.9):\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    # 全连接: batch_size x feature\n",
    "    if len(X.shape) == 2:\n",
    "        # 每个输入维度在样本上的平均和方差\n",
    "        mean = X.mean(axis=0)\n",
    "        variance = ((X - mean)**2).mean(axis=0)\n",
    "    # 2D卷积: batch_size x channel x height x width\n",
    "    else:\n",
    "        # 对每个通道算均值和方差，需要保持4D形状使得可以正确的广播\n",
    "        mean = X.mean(axis=(0,2,3), keepdims=True)\n",
    "        variance = ((X - mean)**2).mean(axis=(0,2,3), keepdims=True)\n",
    "        # 变形使得可以正确的广播\n",
    "        moving_mean = moving_mean.reshape(mean.shape)\n",
    "        moving_variance = moving_variance.reshape(mean.shape)\n",
    "\n",
    "    # 均一化\n",
    "    if is_training:\n",
    "        X_hat = (X - mean) / nd.sqrt(variance + eps)\n",
    "        #!!! 更新全局的均值和方差\n",
    "        moving_mean[:] = moving_momentum * moving_mean + (\n",
    "            1.0 - moving_momentum) * mean\n",
    "        moving_variance[:] = moving_momentum * moving_variance + (\n",
    "            1.0 - moving_momentum) * variance\n",
    "    else:\n",
    "        #!!! 测试阶段使用全局的均值和方差\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_variance + eps)\n",
    "\n",
    "    # 伸缩和偏移\n",
    "    return gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "我们尝试使用GPU运行本教程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:21:37.400101Z",
     "start_time": "2018-02-03T15:21:37.010078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Xinet as T\n",
    "ctx = T.try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:22:08.084348Z",
     "start_time": "2018-02-03T15:22:06.355327Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_scale = .01\n",
    "\n",
    "# 输出通道 = 20, 卷积核 = (5,5)\n",
    "c1 = 20\n",
    "W1 = nd.random.normal(shape=(c1,1,5,5), scale=weight_scale, ctx=ctx)\n",
    "b1 = nd.zeros(c1, ctx=ctx)\n",
    "\n",
    "# 第1层批量归一化\n",
    "gamma1 = nd.random.normal(shape=c1, scale=weight_scale, ctx=ctx)\n",
    "beta1 = nd.random.normal(shape=c1, scale=weight_scale, ctx=ctx)\n",
    "moving_mean1 = nd.zeros(c1, ctx=ctx)\n",
    "moving_variance1 = nd.zeros(c1, ctx=ctx)\n",
    "\n",
    "# 输出通道 = 50, 卷积核 = (3,3)\n",
    "c2 = 50\n",
    "W2 = nd.random_normal(shape=(c2,c1,3,3), scale=weight_scale, ctx=ctx)\n",
    "b2 = nd.zeros(c2, ctx=ctx)\n",
    "\n",
    "# 第2层批量归一化\n",
    "gamma2 = nd.random.normal(shape=c2, scale=weight_scale, ctx=ctx)\n",
    "beta2 = nd.random.normal(shape=c2, scale=weight_scale, ctx=ctx)\n",
    "moving_mean2 = nd.zeros(c2, ctx=ctx)\n",
    "moving_variance2 = nd.zeros(c2, ctx=ctx)\n",
    "\n",
    "# 输出维度 = 128\n",
    "o3 = 128\n",
    "W3 = nd.random.normal(shape=(1250, o3), scale=weight_scale, ctx=ctx)\n",
    "b3 = nd.zeros(o3, ctx=ctx)\n",
    "\n",
    "# 输出维度 = 10\n",
    "W4 = nd.random_normal(shape=(W3.shape[1], 10), scale=weight_scale, ctx=ctx)\n",
    "b4 = nd.zeros(W4.shape[1], ctx=ctx)\n",
    "\n",
    "# 注意这里 moving_*是不需要更新的\n",
    "params = [W1, b1, gamma1, beta1, W2, b2, gamma2, beta2, W3, b3, W4, b4]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义模型。我们添加了批量归一化层。特别要注意我们添加的位置：在卷积层后，在激活函数前。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:22:15.524501Z",
     "start_time": "2018-02-03T15:22:15.476373Z"
    }
   },
   "outputs": [],
   "source": [
    "def net(X, is_training=False, verbose=False):\n",
    "    X = X.as_in_context(W1.context)\n",
    "    # 第一层卷积\n",
    "    h1_conv = nd.Convolution(\n",
    "        data=X, weight=W1, bias=b1, kernel=W1.shape[2:], num_filter=c1)\n",
    "    ### 添加了批量归一化层\n",
    "    h1_bn = batch_norm(h1_conv, gamma1, beta1, is_training,\n",
    "                       moving_mean1, moving_variance1)\n",
    "    h1_activation = nd.relu(h1_bn)\n",
    "    h1 = nd.Pooling(\n",
    "        data=h1_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    # 第二层卷积\n",
    "    h2_conv = nd.Convolution(\n",
    "        data=h1, weight=W2, bias=b2, kernel=W2.shape[2:], num_filter=c2)\n",
    "    ### 添加了批量归一化层\n",
    "    h2_bn = batch_norm(h2_conv, gamma2, beta2, is_training,\n",
    "                       moving_mean2, moving_variance2)\n",
    "    h2_activation = nd.relu(h2_bn)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    # 第一层全连接\n",
    "    h3_linear = nd.dot(h2, W3) + b3\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    # 第二层全连接\n",
    "    h4_linear = nd.dot(h3, W4) + b4\n",
    "    if verbose:\n",
    "        print('1st conv block:', h1.shape)\n",
    "        print('2nd conv block:', h2.shape)\n",
    "        print('1st dense:', h3.shape)\n",
    "        print('2nd dense:', h4_linear.shape)\n",
    "        print('output:', h4_linear)\n",
    "    return h4_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们训练并测试模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:22:50.225475Z",
     "start_time": "2018-02-03T15:22:49.525615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:82: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:86: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    },
    {
     "ename": "MXNetError",
     "evalue": "[23:22:50] d:\\program files (x86)\\jenkins\\workspace\\mxnet\\mxnet\\src\\operator\\tensor\\./matrix_op-inl.h:310: Check failed: shp.ndim() == param.axes.ndim() (4 vs. 3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-1873e89950e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_in_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\XinetStudio\\mxnet-gluon-tutorial\\CNN\\utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\XinetStudio\\mxnet-gluon-tutorial\\CNN\\utils.py\u001b[0m in \u001b[0;36mtransform_mnist\u001b[1;34m(data, label)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# change data from batch x height x weight x channel to batch x channel x height x weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mmnist_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFashionMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_mnist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mmnist_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFashionMNIST\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform_mnist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\mxnet\\ndarray\\register.py\u001b[0m in \u001b[0;36mtranspose\u001b[1;34m(data, axes, out, name, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\mxnet\\_ctypes\\ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[1;34m(handle, ndargs, keys, vals, out)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moriginal_output\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \"\"\"\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMXNetError\u001b[0m: [23:22:50] d:\\program files (x86)\\jenkins\\workspace\\mxnet\\mxnet\\src\\operator\\tensor\\./matrix_op-inl.h:310: Check failed: shp.ndim() == param.axes.ndim() (4 vs. 3) "
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "batch_size = 256\n",
    "train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data, is_training=True)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        T.SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += T.accuracy(output, label)\n",
    "\n",
    "    test_acc = T.evaluate_accuracy(test_data, net, ctx)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (\n",
    "            epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "相比[卷积神经网络 --- 从0开始](cnn-scratch.md)来说，通过加入批量归一化层，即使是同样的参数，测试精度也有明显提升，尤其是最开始几轮。\n",
    "\n",
    "## 练习\n",
    "\n",
    "尝试调大学习率，看看跟前面比，是不是可以使用更大的学习率。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T05:06:33.179707Z",
     "start_time": "2018-02-04T05:06:33.172689Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[ 0.  1.  2.]\n",
       "  [ 3.  4.  5.]\n",
       "  [ 6.  7.  8.]]\n",
       "\n",
       " [[ 9. 10. 11.]\n",
       "  [12. 13. 14.]\n",
       "  [15. 16. 17.]]]\n",
       "<NDArray 2x3x3 @cpu(0)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nd.arange(18).reshape((2, 3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T05:09:30.116922Z",
     "start_time": "2018-02-04T05:09:30.110873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[ 0.  9.]\n",
       "  [ 3. 12.]\n",
       "  [ 6. 15.]]\n",
       "\n",
       " [[ 1. 10.]\n",
       "  [ 4. 13.]\n",
       "  [ 7. 16.]]\n",
       "\n",
       " [[ 2. 11.]\n",
       "  [ 5. 14.]\n",
       "  [ 8. 17.]]]\n",
       "<NDArray 3x3x2 @cpu(0)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T05:05:09.060364Z",
     "start_time": "2018-02-04T05:05:09.052369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  5.  14.  23.  32.]\n",
       " [ 14.  50.  86. 122.]\n",
       " [ 23.  86. 149. 212.]\n",
       " [ 32. 122. 212. 302.]]\n",
       "<NDArray 4x4 @cpu(0)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.dot(a, a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
