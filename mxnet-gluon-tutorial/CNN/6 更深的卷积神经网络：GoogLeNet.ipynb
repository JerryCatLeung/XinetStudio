{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 更深的卷积神经网络：GoogLeNet\n",
    "\n",
    "在2014年的Imagenet竞赛里，Google的研究人员利用一个新的网络结构取得很大的优先。这个叫做GoogLeNet的网络虽然在名字上是向LeNet致敬，但网络结构里很难看到LeNet的影子。它颠覆的大家对卷积神经网络串联一系列层的固定做法。下图是其[论文](https://arxiv.org/abs/1409.4842)对GoogLeNet的可视化\n",
    "\n",
    "![](../img/googlenet.png)\n",
    "\n",
    "## 定义Inception\n",
    "\n",
    "可以看到其中有多个四个并行卷积层的块。这个块一般叫做Inception，其基于[Network in network](./nin-gluon.md)的思想做了很大的改进。我们先看下如何定义一个下图所示的Inception块。\n",
    "\n",
    "![](../img/inception.svg)\n",
    "\n",
    "```{.python .input}\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "class Inception(nn.Block):\n",
    "    def __init__(self, n1_1, n2_1, n2_3, n3_1, n3_5, n4_1, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # path 1\n",
    "        self.p1_conv_1 = nn.Conv2D(n1_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "        # path 2\n",
    "        self.p2_conv_1 = nn.Conv2D(n2_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "        self.p2_conv_3 = nn.Conv2D(n2_3, kernel_size=3, padding=1,\n",
    "                                   activation='relu')\n",
    "        # path 3\n",
    "        self.p3_conv_1 = nn.Conv2D(n3_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "        self.p3_conv_5 = nn.Conv2D(n3_5, kernel_size=5, padding=2,\n",
    "                                   activation='relu')\n",
    "        # path 4\n",
    "        self.p4_pool_3 = nn.MaxPool2D(pool_size=3, padding=1,\n",
    "                                      strides=1)\n",
    "        self.p4_conv_1 = nn.Conv2D(n4_1, kernel_size=1,\n",
    "                                   activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_conv_1(x)\n",
    "        p2 = self.p2_conv_3(self.p2_conv_1(x))\n",
    "        p3 = self.p3_conv_5(self.p3_conv_1(x))\n",
    "        p4 = self.p4_conv_1(self.p4_pool_3(x))\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)\n",
    "```\n",
    "\n",
    "可以看到Inception里有四个并行的线路。\n",
    "\n",
    "1. 单个$1\\times 1$卷积。\n",
    "2. $1\\times 1$卷积接上$3\\times 3$卷积。通常前者的通道数少于输入通道，这样减少后者的计算量。后者加上了`padding=1`使得输出的长宽的输入一致\n",
    "3. 同2，但换成了$5 \\times 5$卷积\n",
    "4. 和1类似，但卷积前用了最大池化层\n",
    "\n",
    "最后将这四个并行线路的结果在通道这个维度上合并在一起。\n",
    "\n",
    "测试一下：\n",
    "\n",
    "```{.python .input}\n",
    "incp = Inception(64, 96, 128, 16, 32, 32)\n",
    "incp.initialize()\n",
    "\n",
    "x = nd.random.uniform(shape=(32,3,64,64))\n",
    "incp(x).shape\n",
    "```\n",
    "\n",
    "## 定义GoogLeNet\n",
    "\n",
    "GoogLeNet将数个Inception串联在一起。注意到原论文里使用了多个输出，为了简化我们这里就使用一个输出。为了可以更方便的查看数据在内部的形状变化，我们对每个块使用一个`nn.Sequential`，然后再把所有这些块连起来。\n",
    "\n",
    "```{.python .input}\n",
    "class GoogLeNet(nn.Block):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(GoogLeNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        # add name_scope on the outer most Sequential\n",
    "        with self.name_scope():\n",
    "            # block 1\n",
    "            b1 = nn.Sequential()\n",
    "            b1.add(\n",
    "                nn.Conv2D(64, kernel_size=7, strides=2,\n",
    "                          padding=3, activation='relu'),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "            # block 2\n",
    "            b2 = nn.Sequential()\n",
    "            b2.add(\n",
    "                nn.Conv2D(64, kernel_size=1),\n",
    "                nn.Conv2D(192, kernel_size=3, padding=1),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "\n",
    "            # block 3\n",
    "            b3 = nn.Sequential()\n",
    "            b3.add(\n",
    "                Inception(64, 96, 128, 16,32, 32),\n",
    "                Inception(128, 128, 192, 32, 96, 64),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "\n",
    "            # block 4\n",
    "            b4 = nn.Sequential()\n",
    "            b4.add(\n",
    "                Inception(192, 96, 208, 16, 48, 64),\n",
    "                Inception(160, 112, 224, 24, 64, 64),\n",
    "                Inception(128, 128, 256, 24, 64, 64),\n",
    "                Inception(112, 144, 288, 32, 64, 64),\n",
    "                Inception(256, 160, 320, 32, 128, 128),\n",
    "                nn.MaxPool2D(pool_size=3, strides=2)\n",
    "            )\n",
    "\n",
    "            # block 5\n",
    "            b5 = nn.Sequential()\n",
    "            b5.add(\n",
    "                Inception(256, 160, 320, 32, 128, 128),\n",
    "                Inception(384, 192, 384, 48, 128, 128),\n",
    "                nn.AvgPool2D(pool_size=2)\n",
    "            )\n",
    "            # block 6\n",
    "            b6 = nn.Sequential()\n",
    "            b6.add(\n",
    "                nn.Flatten(),\n",
    "                nn.Dense(num_classes)\n",
    "            )\n",
    "            # chain blocks together\n",
    "            self.net = nn.Sequential()\n",
    "            self.net.add(b1, b2, b3, b4, b5, b6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out\n",
    "```\n",
    "\n",
    "我们看一下每个块对输出的改变。\n",
    "\n",
    "```{.python .input}\n",
    "net = GoogLeNet(10, verbose=True)\n",
    "net.initialize()\n",
    "\n",
    "x = nd.random.uniform(shape=(4, 3, 96, 96))\n",
    "y = net(x)\n",
    "```\n",
    "\n",
    "## 获取数据并训练\n",
    "\n",
    "跟VGG一样我们使用了较小的输入$96\\times 96$来加速计算。\n",
    "\n",
    "```{.python .input}\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from mxnet import gluon\n",
    "from mxnet import init\n",
    "\n",
    "train_data, test_data = utils.load_data_fashion_mnist(\n",
    "    batch_size=64, resize=96)\n",
    "\n",
    "ctx = utils.try_gpu()\n",
    "net = GoogLeNet(10)\n",
    "net.initialize(ctx=ctx, init=init.Xavier())\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        'sgd', {'learning_rate': 0.01})\n",
    "utils.train(train_data, test_data, net, loss,\n",
    "            trainer, ctx, num_epochs=1)\n",
    "```\n",
    "\n",
    "## 结论\n",
    "\n",
    "GoogLeNet加入了更加结构化的Inception块来使得我们可以使用更大的通道，更多的层，同时控制计算量和模型大小在合理范围内。\n",
    "\n",
    "## 练习\n",
    "\n",
    "GoogLeNet有数个后续版本，尝试实现他们并运行看看有什么不一样\n",
    "\n",
    "- v1: 本节介绍的是最早版本：[Going Deeper with Convolutions](http://arxiv.org/abs/1409.4842)\n",
    "- v2: 加入和Batch Normalization：[Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/abs/1502.03167)\n",
    "- v3: 对Inception做了调整：[Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567)\n",
    "- v4: 基于ResNet加入了Residual Connections：[Inception-ResNet and the Impact of Residual Connections on Learning](http://arxiv.org/abs/1602.07261)\n",
    "\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1662)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
