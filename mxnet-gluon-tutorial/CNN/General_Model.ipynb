{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T10:40:33.868884Z",
     "start_time": "2018-02-03T10:40:33.261269Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:82: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:86: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon, nd, autograd\n",
    "root= 'E:/Data/MXNet/fashion-mnist'\n",
    "\n",
    "def transform(data, label):\n",
    "        '''转换为 `float32` 数据类型'''\n",
    "        return nd.transpose(data.astype('float32'), (2, 0, 1)) / 255, label.astype('float32')\n",
    "    \n",
    "mnist_train = gluon.data.vision.FashionMNIST(root, train= True, transform= transform)\n",
    "mnist_test = gluon.data.vision.FashionMNIST(root, train= False, transform= transform)\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_data = gluon.data.DataLoader(mnist_train, batch_size, shuffle= True)\n",
    "test_data = gluon.data.DataLoader(mnist_test, batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T10:40:34.527000Z",
     "start_time": "2018-02-03T10:40:34.414672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (256, 1, 28, 28) \n",
      "label.shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_data:\n",
    "    # change data from batch x height x weight x channel to batch x channel x height x weight\n",
    "    print('data.shape: {} \\nlabel.shape: {}'.format(data.shape, label.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型\n",
    "\n",
    "因为卷积网络计算比全连接要复杂，这里我们默认使用 GPU 来计算。如果 GPU 不能用，默认使用CPU。（下面这段代码会保存在 `utils.py` 里可以下次重复使用）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T10:40:36.088249Z",
     "start_time": "2018-02-03T10:40:36.074242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "try:\n",
    "    ctx = mx.gpu()\n",
    "    _ = nd.zeros((1,), ctx= ctx)\n",
    "except:\n",
    "    ctx = mx.cpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T10:40:37.019766Z",
     "start_time": "2018-02-03T10:40:36.923470Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_params(weight_scale):\n",
    "    # output channels = 20, kernel = (5,5)\n",
    "    W1 = nd.random_normal(shape=(20, 1, 5, 5), scale= weight_scale, ctx= ctx)\n",
    "    b1 = nd.zeros(W1.shape[0], ctx= ctx)\n",
    "\n",
    "    # output channels = 50, kernel = (3,3)\n",
    "    W2 = nd.random_normal(shape=(50, 20, 3, 3), scale=weight_scale, ctx= ctx)\n",
    "    b2 = nd.zeros(W2.shape[0], ctx= ctx)\n",
    "\n",
    "    # output dim = 128\n",
    "    W3 = nd.random_normal(shape=(1250, 128), scale=weight_scale, ctx= ctx)\n",
    "    b3 = nd.zeros(W3.shape[1], ctx= ctx)\n",
    "\n",
    "    # output dim = 10\n",
    "    W4 = nd.random_normal(shape=(W3.shape[1], 10), scale=weight_scale, ctx= ctx)\n",
    "    b4 = nd.zeros(W4.shape[1], ctx= ctx)\n",
    "\n",
    "    params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params\n",
    "\n",
    "def net(X, params, verbose=False):\n",
    "    W1, b1, W2, b2, W3, b3, W4, b4 = params\n",
    "    # 第一层卷积\n",
    "    h1_conv = nd.Convolution(\n",
    "        data=X, weight=W1, bias=b1, kernel=W1.shape[2:], num_filter=W1.shape[0])\n",
    "    h1_activation = nd.relu(h1_conv)\n",
    "    h1 = nd.Pooling(\n",
    "        data=h1_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    # 第二层卷积\n",
    "    h2_conv = nd.Convolution(\n",
    "        data=h1, weight=W2, bias=b2, kernel=W2.shape[2:], num_filter=W2.shape[0])\n",
    "    h2_activation = nd.relu(h2_conv)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    # 第一层全连接\n",
    "    h3_linear = nd.dot(h2, W3) + b3\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    # 第二层全连接\n",
    "    h4_linear = nd.dot(h3, W4) + b4\n",
    "    if verbose:\n",
    "        print('1st conv block:', h1.shape)\n",
    "        print('2nd conv block:', h2.shape)\n",
    "        print('1st dense:', h3.shape)\n",
    "        print('2nd dense:', h4_linear.shape)\n",
    "        print('output:', h4_linear)\n",
    "    return h4_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T10:40:37.645389Z",
     "start_time": "2018-02-03T10:40:37.513073Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st conv block: (256, 20, 12, 12)\n",
      "2nd conv block: (256, 1250)\n",
      "1st dense: (256, 128)\n",
      "2nd dense: (256, 10)\n",
      "output: \n",
      "[[ 0.4162476   0.26670948  0.11270595 ... -0.04386711  0.06388557\n",
      "  -0.74774057]\n",
      " [ 0.54506636  0.33793062  0.840812   ...  0.5353344   0.19240683\n",
      "  -0.940362  ]\n",
      " [ 0.15876496  0.3288771   0.30370203 ... -0.10934281  0.11002457\n",
      "  -0.7485206 ]\n",
      " ...\n",
      " [ 0.56907856 -0.20700485  0.41588512 ...  0.53575045 -0.18031669\n",
      "  -0.96532357]\n",
      " [ 0.24785687  0.20551816  0.12323917 ...  0.20857449 -0.067404\n",
      "  -0.55670625]\n",
      " [ 0.4602481  -0.09172845  0.42637235 ...  0.7300366  -0.43264282\n",
      "  -0.8195547 ]]\n",
      "<NDArray 256x10 @gpu(0)>\n",
      "0.16015625\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_data:\n",
    "    data = data.as_in_context(ctx)\n",
    "    label = label.as_in_context(ctx)\n",
    "    params = init_params(weight_scale)\n",
    "    output = net(data, params, verbose=True)\n",
    "    acc = nd.mean(output.argmax(axis= 1)==label).asscalar()\n",
    "    print(acc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T10:40:38.806477Z",
     "start_time": "2018-02-03T10:40:38.766369Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mxnet import nd\n",
    "\n",
    "def softmax(output):\n",
    "    exp = nd.exp(output)\n",
    "    return exp/exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(yhat, y):\n",
    "    '''效果与 `y` 做了 `one-hot` 相同'''\n",
    "    return - nd.pick(nd.log(yhat), y)\n",
    "\n",
    "def softmax_cross_entropy(yhat, y):\n",
    "    return cross_entropy(softmax(yhat), y)\n",
    "\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] -= lr * param.grad \n",
    "        \n",
    "def accuracy(output, label):\n",
    "    return nd.mean(output.argmax(axis= 1)==label).asscalar()\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx, params):\n",
    "    acc = nd.array([0.], ctx= ctx)\n",
    "    n = 0.\n",
    "    if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "        data_iterator.reset()\n",
    "    for data, label in data_iterator:\n",
    "        label = label.as_in_context(ctx)\n",
    "        data = data.as_in_context(ctx)\n",
    "        acc += nd.sum(net(data, params).argmax(axis=1)==label)\n",
    "        n += len(label)\n",
    "        acc.wait_to_read() # don't push too many operators into backend\n",
    "    return acc.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T12:05:37.438468Z",
     "start_time": "2018-02-03T11:58:25.945007Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.704173, Train acc 0.744891, Test acc 0.8393, Time 21.2681 sec\n",
      "Epoch 1. Loss: 0.432081, Train acc 0.842991, Test acc 0.8659, Time 21.2816 sec\n",
      "Epoch 2. Loss: 0.374029, Train acc 0.86301, Test acc 0.8789, Time 21.4811 sec\n",
      "Epoch 3. Loss: 0.338713, Train acc 0.876219, Test acc 0.8869, Time 21.3909 sec\n",
      "Epoch 4. Loss: 0.313365, Train acc 0.885167, Test acc 0.8884, Time 21.2565 sec\n",
      "Epoch 5. Loss: 0.297956, Train acc 0.890758, Test acc 0.8944, Time 21.5322 sec\n",
      "Epoch 6. Loss: 0.280058, Train acc 0.897706, Test acc 0.899, Time 22.2492 sec\n",
      "Epoch 7. Loss: 0.268042, Train acc 0.900848, Test acc 0.8967, Time 23.305 sec\n",
      "Epoch 8. Loss: 0.258463, Train acc 0.904277, Test acc 0.9009, Time 21.2718 sec\n",
      "Epoch 9. Loss: 0.248514, Train acc 0.908289, Test acc 0.9027, Time 21.1091 sec\n",
      "Epoch 10. Loss: 0.240115, Train acc 0.910444, Test acc 0.9065, Time 21.6842 sec\n",
      "Epoch 11. Loss: 0.229878, Train acc 0.915221, Test acc 0.9039, Time 21.4453 sec\n",
      "Epoch 12. Loss: 0.22257, Train acc 0.91701, Test acc 0.9055, Time 21.1833 sec\n",
      "Epoch 13. Loss: 0.214821, Train acc 0.920346, Test acc 0.9067, Time 21.4686 sec\n",
      "Epoch 14. Loss: 0.210888, Train acc 0.921171, Test acc 0.9072, Time 21.2014 sec\n",
      "Epoch 15. Loss: 0.20233, Train acc 0.925089, Test acc 0.9077, Time 22.0075 sec\n",
      "Epoch 16. Loss: 0.195933, Train acc 0.927438, Test acc 0.9073, Time 22.2432 sec\n",
      "Epoch 17. Loss: 0.189278, Train acc 0.929133, Test acc 0.9022, Time 21.422 sec\n",
      "Epoch 18. Loss: 0.183527, Train acc 0.931006, Test acc 0.9093, Time 21.6234 sec\n",
      "Epoch 19. Loss: 0.184321, Train acc 0.932175, Test acc 0.9069, Time 21.0269 sec\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "weight_scale = .1\n",
    "params = init_params(weight_scale)\n",
    "learning_rate = .2\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    m = len(train_data)\n",
    "    \n",
    "    start = time()\n",
    "    for data, label in train_data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        data = data.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data, params)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        # 将梯度做平均，这样学习率会对 batch size 不那么敏感\n",
    "        SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "        n += len(label)\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, net, ctx, params)\n",
    "    print((\"Epoch %d. Loss: %g, Train acc %g, Test acc %g, Time %g sec\" % (\n",
    "        epoch, train_loss/m, train_acc/m, test_acc, time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T11:51:51.180576Z",
     "start_time": "2018-02-03T11:51:51.175531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1517658711.1765313"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T11:38:39.277151Z",
     "start_time": "2018-02-03T11:38:39.270128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T11:41:17.659250Z",
     "start_time": "2018-02-03T11:41:16.123168Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 0.\n",
    "for x, y in test_data:\n",
    "    n += len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T11:41:19.848073Z",
     "start_time": "2018-02-03T11:41:19.844062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
