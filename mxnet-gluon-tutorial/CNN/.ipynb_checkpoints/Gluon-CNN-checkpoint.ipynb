{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T11:08:06.112343Z",
     "start_time": "2018-02-02T11:08:05.755398Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['time']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from mxnet import nd\n",
    "from mxnet import image\n",
    "from mxnet.gluon import nn\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "class DataLoader(object):\n",
    "    \"\"\"similiar to gluon.data.DataLoader, but might be faster.\n",
    "\n",
    "    The main difference this data loader tries to read more exmaples each\n",
    "    time. But the limits are 1) all examples in dataset have the same shape, 2)\n",
    "    data transfomer needs to process multiple examples at each time\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size, shuffle):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = self.dataset[:]\n",
    "        X = nd.array(data[0])\n",
    "        y = nd.array(data[1])\n",
    "        n = len(X)\n",
    "        idx = np.arange(n)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idx)\n",
    "\n",
    "        for i in range(0, n, self.batch_size):\n",
    "            j = nd.array(idx[i: min(i + batch_size, n)])\n",
    "            yield nd.take(X, j), nd.take(y, j)\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T11:11:17.651011Z",
     "start_time": "2018-02-02T11:11:17.631961Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(batch_size, resize=None, root= 'E:/Data/MXNet/fashion-mnist'):\n",
    "    \"\"\"download the fashion mnist dataest and then load into memory\"\"\"\n",
    "    def transform(data, label):\n",
    "        # transform a batch of examples\n",
    "        if resize:\n",
    "            n = data.shape[0]\n",
    "            new_data = nd.zeros((n, resize, resize, data.shape[3]))\n",
    "            for i in range(n):\n",
    "                new_data[i] = image.imresize(data[i], resize, resize)\n",
    "            data = new_data\n",
    "        # change data from batch x height x weight x channel to batch x channel x height x weight\n",
    "        return nd.transpose(data.astype('float32'), (0,3,1,2))/255, label.astype('float32')\n",
    "    mnist_train = gluon.data.vision.FashionMNIST(root=root, train=True, transform= transform)\n",
    "    mnist_test = gluon.data.vision.FashionMNIST(root=root, train=False, transform= transform)\n",
    "    train_data = DataLoader(mnist_train, batch_size, shuffle=True)\n",
    "    test_data = DataLoader(mnist_test, batch_size, shuffle=False)\n",
    "    return (train_data, test_data)\n",
    "\n",
    "def try_gpu():\n",
    "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu()\"\"\"\n",
    "    try:\n",
    "        ctx = mx.gpu()\n",
    "        _ = nd.array([0], ctx=ctx)\n",
    "    except:\n",
    "        ctx = mx.cpu()\n",
    "    return ctx\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"Return all available GPUs, or [mx.gpu()] if there is no GPU\"\"\"\n",
    "    ctx_list = []\n",
    "    try:\n",
    "        for i in range(16):\n",
    "            ctx = mx.gpu(i)\n",
    "            _ = nd.array([0], ctx=ctx)\n",
    "            ctx_list.append(ctx)\n",
    "    except:\n",
    "        pass\n",
    "    if not ctx_list:\n",
    "        ctx_list = [mx.cpu()]\n",
    "    return ctx_list\n",
    "\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad\n",
    "\n",
    "def accuracy(output, label):\n",
    "    return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "\n",
    "def _get_batch(batch, ctx):\n",
    "    \"\"\"return data and label on ctx\"\"\"\n",
    "    if isinstance(batch, mx.io.DataBatch):\n",
    "        data = batch.data[0]\n",
    "        label = batch.label[0]\n",
    "    else:\n",
    "        data, label = batch\n",
    "    return (gluon.utils.split_and_load(data, ctx),\n",
    "            gluon.utils.split_and_load(label, ctx),\n",
    "            data.shape[0])\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx=[mx.cpu()]):\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    acc = nd.array([0])\n",
    "    n = 0.\n",
    "    if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "        data_iterator.reset()\n",
    "    for batch in data_iterator:\n",
    "        data, label, batch_size = _get_batch(batch, ctx)\n",
    "        for X, y in zip(data, label):\n",
    "            acc += nd.sum(net(X).argmax(axis=1)==y).copyto(mx.cpu())\n",
    "            n += y.size\n",
    "        acc.wait_to_read() # don't push too many operators into backend\n",
    "    return acc.asscalar() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data, test_data, net, loss, trainer, ctx, num_epochs, print_batches=None):\n",
    "    \"\"\"Train a network\"\"\"\n",
    "    print(\"Start training on \", ctx)\n",
    "    if isinstance(ctx, mx.Context):\n",
    "        ctx = [ctx]\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc, n, m = 0.0, 0.0, 0.0, 0.0\n",
    "        if isinstance(train_data, mx.io.MXDataIter):\n",
    "            train_data.reset()\n",
    "        start = time()\n",
    "        for i, batch in enumerate(train_data):\n",
    "            data, label, batch_size = _get_batch(batch, ctx)\n",
    "            losses = []\n",
    "            with autograd.record():\n",
    "                outputs = [net(X) for X in data]\n",
    "                losses = [loss(yhat, y) for yhat, y in zip(outputs, label)]\n",
    "            for l in losses:\n",
    "                l.backward()\n",
    "            train_acc += sum([(yhat.argmax(axis=1)==y).sum().asscalar()\n",
    "                              for yhat, y in zip(outputs, label)])\n",
    "            train_loss += sum([l.sum().asscalar() for l in losses])\n",
    "            trainer.step(batch_size)\n",
    "            n += batch_size\n",
    "            m += sum([y.size for y in label])\n",
    "            if print_batches and (i+1) % print_batches == 0:\n",
    "                print(\"Batch %d. Loss: %f, Train acc %f\" % (\n",
    "                    n, train_loss/n, train_acc/m\n",
    "                ))\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_data, net, ctx)\n",
    "        print(\"Epoch %d. Loss: %.3f, Train acc %.2f, Test acc %.2f, Time %.1f sec\" % (\n",
    "            epoch, train_loss/n, train_acc/m, test_acc, time() - start\n",
    "        ))\n",
    "\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,\n",
    "                                  strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1,\n",
    "                                      strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(out + x)\n",
    "\n",
    "def resnet18(num_classes):\n",
    "    net = nn.HybridSequential()\n",
    "    with net.name_scope():\n",
    "        net.add(\n",
    "            nn.BatchNorm(),\n",
    "            nn.Conv2D(64, kernel_size=3, strides=1),\n",
    "            nn.MaxPool2D(pool_size=3, strides=2),\n",
    "            Residual(64),\n",
    "            Residual(64),\n",
    "            Residual(128, same_shape=False),\n",
    "            Residual(128),\n",
    "            Residual(256, same_shape=False),\n",
    "            Residual(256),\n",
    "            nn.GlobalAvgPool2D(),\n",
    "            nn.Dense(num_classes)\n",
    "        )\n",
    "    return net\n",
    "\n",
    "def show_images(imgs, nrows, ncols, figsize=None):\n",
    "    \"\"\"plot a list of images\"\"\"\n",
    "    if not figsize:\n",
    "        figsize = (ncols, nrows)\n",
    "    _, figs = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            figs[i][j].imshow(imgs[i*ncols+j].asnumpy())\n",
    "            figs[i][j].axes.get_xaxis().set_visible(False)\n",
    "            figs[i][j].axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"Sample mini-batches in a random order from sequential data.\"\"\"\n",
    "    # Subtract 1 because label indices are corresponding input indices + 1. \n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    # Randomize samples.\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # Read batch_size random samples each time.\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        data = nd.array(\n",
    "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
    "        label = nd.array(\n",
    "            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\n",
    "        yield data, label\n",
    "\n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    \"\"\"Sample mini-batches in a consecutive order from sequential data.\"\"\"\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "    \n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # Subtract 1 because label indices are corresponding input indices + 1. \n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        data = indices[:, i: i + num_steps]\n",
    "        label = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield data, label\n",
    "\n",
    "\n",
    "def grad_clipping(params, clipping_norm, ctx):\n",
    "    \"\"\"Gradient clipping.\"\"\"\n",
    "    if clipping_norm is not None:\n",
    "        norm = nd.array([0.0], ctx)\n",
    "        for p in params:\n",
    "            norm += nd.sum(p.grad ** 2)\n",
    "        norm = nd.sqrt(norm).asscalar()\n",
    "        if norm > clipping_norm:\n",
    "            for p in params:\n",
    "                p.grad[:] *= clipping_norm / norm\n",
    "\n",
    "\n",
    "def predict_rnn(rnn, prefix, num_chars, params, hidden_dim, ctx, idx_to_char,\n",
    "                char_to_idx, get_inputs, is_lstm=False):\n",
    "    \"\"\"Predict the next chars given the prefix.\"\"\"\n",
    "    prefix = prefix.lower()\n",
    "    state_h = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    if is_lstm:\n",
    "        state_c = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars + len(prefix)):\n",
    "        X = nd.array([output[-1]], ctx=ctx)\n",
    "        if is_lstm:\n",
    "            Y, state_h, state_c = rnn(get_inputs(X), state_h, state_c, *params)\n",
    "        else:\n",
    "            Y, state_h = rnn(get_inputs(X), state_h, *params)\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = int(Y[0].argmax(axis=1).asscalar())\n",
    "        output.append(next_input)\n",
    "    return ''.join([idx_to_char[i] for i in output])\n",
    "\n",
    "\n",
    "def train_and_predict_rnn(rnn, is_random_iter, epochs, num_steps, hidden_dim, \n",
    "                          learning_rate, clipping_norm, batch_size,\n",
    "                          pred_period, pred_len, seqs, get_params, get_inputs,\n",
    "                          ctx, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          is_lstm=False):\n",
    "    \"\"\"Train an RNN model and predict the next item in the sequence.\"\"\"\n",
    "    if is_random_iter:\n",
    "        data_iter = data_iter_random\n",
    "    else:\n",
    "        data_iter = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    \n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for e in range(1, epochs + 1): \n",
    "        # If consecutive sampling is used, in the same epoch, the hidden state\n",
    "        # is initialized only at the beginning of the epoch.\n",
    "        if not is_random_iter:\n",
    "            state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            if is_lstm:\n",
    "                state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "        train_loss, num_examples = 0, 0\n",
    "        for data, label in data_iter(corpus_indices, batch_size, num_steps, \n",
    "                                     ctx):\n",
    "            # If random sampling is used, the hidden state has to be\n",
    "            # initialized for each mini-batch.\n",
    "            if is_random_iter:\n",
    "                state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "                if is_lstm:\n",
    "                    state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            with autograd.record():\n",
    "                # outputs shape：(batch_size, vocab_size)\n",
    "                if is_lstm:\n",
    "                    outputs, state_h, state_c = rnn(get_inputs(data), state_h,\n",
    "                                                    state_c, *params) \n",
    "                else:\n",
    "                    outputs, state_h = rnn(get_inputs(data), state_h, *params)\n",
    "                # Let t_ib_j be the j-th element of the mini-batch at time i.\n",
    "                # label shape：（batch_size * num_steps）\n",
    "                # label = [t_0b_0, t_0b_1, ..., t_1b_0, t_1b_1, ..., ].\n",
    "                label = label.T.reshape((-1,))\n",
    "                # Concatenate outputs:\n",
    "                # shape: (batch_size * num_steps, vocab_size).\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # Now outputs and label are aligned.\n",
    "                loss = softmax_cross_entropy(outputs, label)\n",
    "            loss.backward()\n",
    "\n",
    "            grad_clipping(params, clipping_norm, ctx)\n",
    "            SGD(params, learning_rate)\n",
    "\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            num_examples += loss.size\n",
    "\n",
    "        if e % pred_period == 0:\n",
    "            print(\"Epoch %d. Training perplexity %f\" % (e, \n",
    "                                               exp(train_loss/num_examples)))\n",
    "            for seq in seqs:\n",
    "                print(' - ', predict_rnn(rnn, seq, pred_len, params,\n",
    "                      hidden_dim, ctx, idx_to_char, char_to_idx, get_inputs,\n",
    "                      is_lstm))\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
