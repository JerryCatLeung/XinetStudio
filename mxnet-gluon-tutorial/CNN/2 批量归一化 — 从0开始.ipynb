{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量归一化 --- 从0开始\n",
    "\n",
    "在实际应用中，我们通常将输入数据的每个样本或者每个特征进行归一化，就是将均值变为 $0$ 方差变为 $1$，来使得数值更稳定。\n",
    "\n",
    "这个对\n",
    "我们在之前的课程里学过了[线性回归](../chapter_supervised-learning/linear-regression-\n",
    "scratch.md)和[逻辑回归](../chapter_supervised-learning/softmax-regression-\n",
    "scratch.md)很有效。因为输入层的输入值的大小变化不剧烈，那么输入也不会。但是，对于一个可能有很多层的深度学习模型来说，情况可能会比较复杂。\n",
    "\n",
    "举个例子，随着第一层和第二层的参数在训练时不断变化，第三层所使用的激活函数的输入值可能由于乘法效应而变得极大或极小，例如和第一层所使用的激活函数的输入值不在一个数量级上。这种在训练时可能出现的情况会造成模型训练的不稳定性。例如，给定一个学习率，某次参数迭代后，目标函数值会剧烈变化或甚至升高。数学的解释是，如果把目标函数\n",
    "$f$ 根据参数 $\\mathbf{w}$ 迭代（如 $f(\\mathbf{w} - \\eta \\nabla f(\\mathbf{w}))$\n",
    "）进行泰勒展开，有关学习率 $\\eta$ 的高阶项的系数可能由于数量级的原因（通常由于层数多）而不容忽略。然而常用的低阶优化算法（如梯度下降）对于不断降低目标函\n",
    "数的有效性通常基于一个基本假设：在以上泰勒展开中把有关学习率的高阶项通通忽略不计。\n",
    "\n",
    "为了应对上述这种情况，**Sergey Ioffe** 和 **Christian Szegedy** 在 2015 年提出了批量归一化的方法。简而言之，在训练时给定一个批量输入，批量归一化试图对深度学习模型的某一层所使用的激活函数的输入进行归一化：使批量呈标准正态分布（均值为 $0$，标准差为 $1$）。\n",
    "\n",
    "批量归一化通常应用于输入层或任意中间层。\n",
    "\n",
    "## 简化的批量归一化层\n",
    "\n",
    "给定一个批量 $B = \\{x_1 \\cdots x_m \\}$, 我们需要学习拉升参数 $\\gamma$ 和偏移参数 $\\beta$。\n",
    "\n",
    "我们定义：\n",
    "\n",
    "$$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m}x_i$$\n",
    "$$\\sigma_B^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(x_i - \\mu_B)^2$$\n",
    "$$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y_i \\leftarrow \\gamma \\hat{x_i} + \\beta \\equiv \\mbox{BN}_{\\gamma,\\beta}(x_i)$$\n",
    "\n",
    "批量归一化层的输出是 $\\{y_i = BN_{\\gamma, \\beta}(x_i)\\}$。\n",
    "\n",
    "我们现在来动手实现一个简化的批量归一化层。实现时对全连接层和二维卷积层两种情况做了区分。对于全连接层，很明显我们要对每个批量进行归一化。然而这里需要注意的是，对\n",
    "于二维卷积，我们要对每个通道进行归一化，并需要保持四维形状使得可以正确地广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T06:21:12.181429Z",
     "start_time": "2018-02-04T06:21:10.741563Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "c:\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "\n",
    "def pure_batch_norm(X, gamma, beta, eps=1e-5):\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    \n",
    "    # 全连接: batch_size x feature\n",
    "    if len(X.shape) == 2:\n",
    "        # 每个输入维度在样本上的平均 和 方差\n",
    "        mean = X.mean(axis= 0)\n",
    "        variance = ((X - mean)**2).mean(axis=0)\n",
    "        \n",
    "    # 2D卷积: batch_size x channel x height x width\n",
    "    else:\n",
    "        # 对每个通道算均值和方差，需要保持 4D 形状使得可以正确地广播\n",
    "        mean = X.mean(axis=(0, 2, 3), keepdims=True)\n",
    "        variance = ((X - mean)**2).mean(axis=(0, 2, 3), keepdims=True)\n",
    "\n",
    "    # 均一化\n",
    "    X_hat = (X - mean) / nd.sqrt(variance + eps)\n",
    "    \n",
    "    # 伸缩和偏移\n",
    "    return gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们检查一下。我们先定义全连接层的输入是这样的。每一行是批量中的一个实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:04:49.395471Z",
     "start_time": "2018-02-03T15:04:49.389457Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0. 1.]\n",
       " [2. 3.]\n",
       " [4. 5.]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = nd.arange(6).reshape((3,2))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们希望批量中的每一列都被归一化。结果符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T12:58:41.087826Z",
     "start_time": "2018-02-03T12:58:41.059742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-1.2247427 -1.2247427]\n",
       " [ 0.         0.       ]\n",
       " [ 1.2247427  1.2247427]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(A, gamma=nd.array([1,1]), beta=nd.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们定义二维卷积网络层的输入是这样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:17:12.845966Z",
     "start_time": "2018-02-03T15:17:12.838947Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 0.  1.  2.]\n",
       "   [ 3.  4.  5.]\n",
       "   [ 6.  7.  8.]]\n",
       "\n",
       "  [[ 9. 10. 11.]\n",
       "   [12. 13. 14.]\n",
       "   [15. 16. 17.]]]\n",
       "\n",
       "\n",
       " [[[18. 19. 20.]\n",
       "   [21. 22. 23.]\n",
       "   [24. 25. 26.]]\n",
       "\n",
       "  [[27. 28. 29.]\n",
       "   [30. 31. 32.]\n",
       "   [33. 34. 35.]]]]\n",
       "<NDArray 2x2x3x3 @cpu(0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = nd.arange(36).reshape((2,2,3,3))\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果也如预期那样，我们对每个通道做了归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T15:17:24.238339Z",
     "start_time": "2018-02-03T15:17:24.230317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[-1.3884367  -1.2816339  -1.174831  ]\n",
       "   [-1.0680282  -0.9612254  -0.85442257]\n",
       "   [-0.74761975 -0.6408169  -0.5340141 ]]\n",
       "\n",
       "  [[-1.3884367  -1.2816339  -1.174831  ]\n",
       "   [-1.0680282  -0.9612254  -0.85442257]\n",
       "   [-0.74761975 -0.6408169  -0.5340141 ]]]\n",
       "\n",
       "\n",
       " [[[ 0.5340141   0.6408169   0.74761975]\n",
       "   [ 0.85442257  0.9612254   1.0680282 ]\n",
       "   [ 1.174831    1.2816339   1.3884367 ]]\n",
       "\n",
       "  [[ 0.5340141   0.6408169   0.74761975]\n",
       "   [ 0.85442257  0.9612254   1.0680282 ]\n",
       "   [ 1.174831    1.2816339   1.3884367 ]]]]\n",
       "<NDArray 2x2x3x3 @cpu(0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(B, gamma=nd.array([1,1]), beta=nd.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量归一化层\n",
    "\n",
    "你可能会想，既然训练时用了批量归一化，那么测试时也该用批量归一化吗？其实这个问题乍一想不是很好回答，因为：\n",
    "\n",
    "* 不用的话，训练出的模型参数很可能在测试时就不准确了；\n",
    "* 用的话，万一测试的数据就只有一个数据实例就不好办了。\n",
    "\n",
    "事实上，在测试时我们还是需要继续使用批量归一化的，只是需要做些改动。在测试时，我们需要把原先训练时用到的批量均值和方差替换成**整个**训练数据的均值和方差。但\n",
    "是当训练数据极大时，这个计算开销很大。因此，我们用移动平均的方法来近似计算（参见实现中的`moving_mean`和`moving_variance`）。\n",
    "\n",
    "为了方便讨论批量归一化层的实现，我们先看下面这段代码来理解``Python``变量可以如何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T14:40:50.998371Z",
     "start_time": "2018-02-04T14:40:50.954253Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def batch_norm2D(X, gamma, beta, is_training, moving_mean, moving_variance,\n",
    "               eps = 1e-5, moving_momentum = 0.9):\n",
    "    '''\n",
    "    事实上，在测试时我们还是需要继续使用批量归一化的，只是需要做些改动。\n",
    "    在测试时，我们需要把原先训练时用到的批量均值和方差替换成**整个**训练数据的均值和方差。\n",
    "    但是当训练数据极大时，这个计算开销很大。因此，我们用移动平均的方法来近似计算\n",
    "    '''\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    # 全连接: batch_size x feature\n",
    "    if len(X.shape) == 2:\n",
    "        # 每个输入维度在样本上的平均和方差\n",
    "        mean = X.mean(axis=0)\n",
    "        variance = ((X - mean)**2).mean(axis=0)\n",
    "    # 2D卷积: batch_size x channel x height x width\n",
    "    else:\n",
    "        # 对每个通道算均值和方差，需要保持 4D 形状使得可以正确的广播\n",
    "        mean = X.mean(axis=(0,2,3), keepdims=True)\n",
    "        variance = ((X - mean)**2).mean(axis=(0,2,3), keepdims=True)\n",
    "        # 变形使得可以正确的广播\n",
    "        moving_mean = moving_mean.reshape(mean.shape)\n",
    "        moving_variance = moving_variance.reshape(mean.shape)\n",
    "\n",
    "    # 均一化\n",
    "    if is_training:\n",
    "        X_hat = (X - mean) / nd.sqrt(variance + eps)\n",
    "        #!!! 更新全局的均值和方差\n",
    "        moving_mean[:] = moving_momentum * moving_mean + (\n",
    "            1.0 - moving_momentum) * mean\n",
    "        moving_variance[:] = moving_momentum * moving_variance + (\n",
    "            1.0 - moving_momentum) * variance\n",
    "    else:\n",
    "        #!!! 测试阶段使用全局的均值和方差\n",
    "        X_hat = (X - moving_mean) / nd.sqrt(moving_variance + eps)\n",
    "\n",
    "    # 伸缩和偏移\n",
    "    return gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "我们尝试使用GPU运行本教程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T14:40:55.810131Z",
     "start_time": "2018-02-04T14:40:52.611500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "c:\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import gluon, autograd, nd\n",
    "from mxnet.gluon import nn\n",
    "import Xinet as T\n",
    "import mxnet as mx\n",
    "ctx = T.try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T14:41:00.842504Z",
     "start_time": "2018-02-04T14:40:59.564957Z"
    }
   },
   "outputs": [],
   "source": [
    "weight_scale = 1\n",
    "\n",
    "# 输出通道 = 20, 卷积核 = (5,5)\n",
    "c1 = 20\n",
    "W1 = nd.random.normal(shape=(c1,1,5,5), scale=weight_scale, ctx=ctx)\n",
    "b1 = nd.zeros(c1, ctx=ctx)\n",
    "\n",
    "# 第1层批量归一化\n",
    "gamma1 = nd.random.normal(shape=c1, scale=weight_scale, ctx=ctx)\n",
    "beta1 = nd.random.normal(shape=c1, scale=weight_scale, ctx=ctx)\n",
    "moving_mean1 = nd.zeros(c1, ctx=ctx)\n",
    "moving_variance1 = nd.zeros(c1, ctx=ctx)\n",
    "\n",
    "# 输出通道 = 50, 卷积核 = (3,3)\n",
    "c2 = 50\n",
    "W2 = nd.random_normal(shape=(c2,c1,3,3), scale=weight_scale, ctx=ctx)\n",
    "b2 = nd.zeros(c2, ctx=ctx)\n",
    "\n",
    "# 第2层批量归一化\n",
    "gamma2 = nd.random.normal(shape=c2, scale=weight_scale, ctx=ctx)\n",
    "beta2 = nd.random.normal(shape=c2, scale=weight_scale, ctx=ctx)\n",
    "moving_mean2 = nd.zeros(c2, ctx=ctx)\n",
    "moving_variance2 = nd.zeros(c2, ctx=ctx)\n",
    "\n",
    "# 输出维度 = 128\n",
    "o3 = 128\n",
    "W3 = nd.random.normal(shape=(1250, o3), scale=weight_scale, ctx=ctx)\n",
    "b3 = nd.zeros(o3, ctx=ctx)\n",
    "\n",
    "# 输出维度 = 10\n",
    "W4 = nd.random_normal(shape=(W3.shape[1], 10), scale=weight_scale, ctx=ctx)\n",
    "b4 = nd.zeros(W4.shape[1], ctx=ctx)\n",
    "\n",
    "# 注意这里 moving_*是不需要更新的\n",
    "params = [W1, b1, gamma1, beta1, W2, b2, gamma2, beta2, W3, b3, W4, b4]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义模型。我们添加了批量归一化层。特别要注意我们添加的位置：在卷积层后，在激活函数前。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T14:41:24.258408Z",
     "start_time": "2018-02-04T14:41:23.570141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:82: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)\n",
      "c:\\anaconda3\\lib\\site-packages\\mxnet\\gluon\\data\\vision\\datasets.py:86: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(fin.read(), dtype=np.uint8)\n"
     ]
    }
   ],
   "source": [
    "def net(X, is_training=False, verbose=False):\n",
    "    X = X.as_in_context(W1.context)\n",
    "    # 第一层卷积\n",
    "    h1_conv = nd.Convolution(\n",
    "        data=X, weight=W1, bias=b1, kernel=W1.shape[2:], num_filter=c1)\n",
    "    ### 添加了批量归一化层\n",
    "    h1_bn = batch_norm2D(h1_conv, gamma1, beta1, is_training,\n",
    "                       moving_mean1, moving_variance1)\n",
    "    h1_activation = nd.relu(h1_bn)\n",
    "    h1 = nd.Pooling(\n",
    "        data=h1_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    # 第二层卷积\n",
    "    h2_conv = nd.Convolution(\n",
    "        data=h1, weight=W2, bias=b2, kernel=W2.shape[2:], num_filter=c2)\n",
    "    ### 添加了批量归一化层\n",
    "    h2_bn = batch_norm2D(h2_conv, gamma2, beta2, is_training,\n",
    "                       moving_mean2, moving_variance2)\n",
    "    h2_activation = nd.relu(h2_bn)\n",
    "    h2 = nd.Pooling(data=h2_activation, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
    "    h2 = nd.flatten(h2)\n",
    "    # 第一层全连接\n",
    "    h3_linear = nd.dot(h2, W3) + b3\n",
    "    h3 = nd.relu(h3_linear)\n",
    "    # 第二层全连接\n",
    "    h4_linear = nd.dot(h3, W4) + b4\n",
    "    if verbose:\n",
    "        print('1st conv block:', h1.shape)\n",
    "        print('2nd conv block:', h2.shape)\n",
    "        print('1st dense:', h3.shape)\n",
    "        print('2nd dense:', h4_linear.shape)\n",
    "        print('output:', h4_linear)\n",
    "    return h4_linear\n",
    "\n",
    "from time import time\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 0.9\n",
    "root = 'E:/Data/MXNet/fashion-mnist'\n",
    "_train = gluon.data.vision.FashionMNIST(root= root, train= True, transform= T.transform3D)\n",
    "_test = gluon.data.vision.FashionMNIST(root= root, train= False, transform= T.transform3D)\n",
    "\n",
    "train_data = T.DataLoader(_train, batch_size, shuffle= True)\n",
    "test_data = T.DataLoader(_test, batch_size, shuffle= False)\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer = T.SGD(params, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们训练并测试模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T14:42:42.696762Z",
     "start_time": "2018-02-04T14:41:28.046544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start training on ', gpu(0))\n",
      "Epoch 0. Loss: 158.306, Train acc 0.100355, Test acc 0.1, Time 15.3507 sec\n",
      "Epoch 1. Loss: 158.204, Train acc 0.100466, Test acc 0.1, Time 14.7813 sec\n",
      "Epoch 2. Loss: 158.231, Train acc 0.100438, Test acc 0.1, Time 14.6849 sec\n",
      "Epoch 3. Loss: 158.303, Train acc 0.100355, Test acc 0.1, Time 15.0039 sec\n",
      "Epoch 4. Loss: 158.25, Train acc 0.100355, Test acc 0.1, Time 14.717 sec\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(data_iterator, net, ctx):\n",
    "    acc = nd.array([0.], ctx= ctx)\n",
    "    n = 0.\n",
    "    if isinstance(data_iterator, mx.io.MXDataIter):\n",
    "        data_iterator.reset()\n",
    "    for data, label in data_iterator:\n",
    "        label = label.as_in_context(ctx)\n",
    "        data = data.as_in_context(ctx)\n",
    "        acc += nd.sum(net(data, is_training= False).argmax(axis=1)==label)\n",
    "        n += len(label)\n",
    "        acc.wait_to_read() # don't push too many operators into backend\n",
    "    return acc.asscalar() / n\n",
    "\n",
    "def train(train_data, test_data, net, loss, trainer, ctx, num_epochs, batch_size, print_batches=None):\n",
    "    \"\"\"\n",
    "    Train a network\n",
    "    `BN = True` 表示对训练数据进行了 Batch Normlization 处理\n",
    "    \"\"\"\n",
    "    print((\"Start training on \", ctx))\n",
    "    if isinstance(train_data, mx.io.MXDataIter):\n",
    "        train_data.reset()\n",
    "\n",
    "    n = len(train_data)   \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "\n",
    "        start = time()\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            data = data.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data, is_training= True)\n",
    "                L = loss(output, label)\n",
    "            L.backward()\n",
    "            \n",
    "            if isinstance(trainer, gluon.Trainer):\n",
    "                trainer.step(batch_size)\n",
    "            else:\n",
    "                # 将梯度做平均，这样学习率会对 batch size 不那么敏感\n",
    "                trainer\n",
    "                \n",
    "            train_loss += nd.mean(L).asscalar()\n",
    "            train_acc += T.accuracy(output, label)\n",
    "\n",
    "        test_acc = evaluate_accuracy(test_data, net, ctx)\n",
    "\n",
    "        print((\"Epoch %d. Loss: %g, Train acc %g, Test acc %g, Time %g sec\" % (\n",
    "                epoch, train_loss/n, train_acc/n, test_acc, time() - start)))\n",
    "        \n",
    "train(train_data, test_data, net, loss, trainer, ctx, num_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "相比[卷积神经网络 --- 从0开始](cnn-scratch.md)来说，通过加入批量归一化层，即使是同样的参数，测试精度也有明显提升，尤其是最开始几轮。\n",
    "\n",
    "## 练习\n",
    "\n",
    "尝试调大学习率，看看跟前面比，是不是可以使用更大的学习率。\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T05:59:23.384749Z",
     "start_time": "2018-02-04T05:59:23.378736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 0.  1.  2.  3.]\n",
       "   [ 4.  5.  6.  7.]]\n",
       "\n",
       "  [[ 8.  9. 10. 11.]\n",
       "   [12. 13. 14. 15.]]\n",
       "\n",
       "  [[16. 17. 18. 19.]\n",
       "   [20. 21. 22. 23.]]]\n",
       "\n",
       "\n",
       " [[[24. 25. 26. 27.]\n",
       "   [28. 29. 30. 31.]]\n",
       "\n",
       "  [[32. 33. 34. 35.]\n",
       "   [36. 37. 38. 39.]]\n",
       "\n",
       "  [[40. 41. 42. 43.]\n",
       "   [44. 45. 46. 47.]]]]\n",
       "<NDArray 2x3x2x4 @cpu(0)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nd.arange(48).reshape((2, 3, 2, 4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T05:59:28.518219Z",
     "start_time": "2018-02-04T05:59:28.514238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 0. 24.]\n",
       "   [ 8. 32.]\n",
       "   [16. 40.]]\n",
       "\n",
       "  [[ 4. 28.]\n",
       "   [12. 36.]\n",
       "   [20. 44.]]]\n",
       "\n",
       "\n",
       " [[[ 1. 25.]\n",
       "   [ 9. 33.]\n",
       "   [17. 41.]]\n",
       "\n",
       "  [[ 5. 29.]\n",
       "   [13. 37.]\n",
       "   [21. 45.]]]\n",
       "\n",
       "\n",
       " [[[ 2. 26.]\n",
       "   [10. 34.]\n",
       "   [18. 42.]]\n",
       "\n",
       "  [[ 6. 30.]\n",
       "   [14. 38.]\n",
       "   [22. 46.]]]\n",
       "\n",
       "\n",
       " [[[ 3. 27.]\n",
       "   [11. 35.]\n",
       "   [19. 43.]]\n",
       "\n",
       "  [[ 7. 31.]\n",
       "   [15. 39.]\n",
       "   [23. 47.]]]]\n",
       "<NDArray 4x2x3x2 @cpu(0)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.T\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T05:57:52.326759Z",
     "start_time": "2018-02-04T05:57:52.317736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[[[[[   5.   59.]\n",
       "     [  32.   86.]]\n",
       "\n",
       "    [[  14.   68.]\n",
       "     [  41.   95.]]\n",
       "\n",
       "    [[  23.   77.]\n",
       "     [  50.  104.]]]\n",
       "\n",
       "\n",
       "   [[[  14.  230.]\n",
       "     [ 122.  338.]]\n",
       "\n",
       "    [[  50.  266.]\n",
       "     [ 158.  374.]]\n",
       "\n",
       "    [[  86.  302.]\n",
       "     [ 194.  410.]]]\n",
       "\n",
       "\n",
       "   [[[  23.  401.]\n",
       "     [ 212.  590.]]\n",
       "\n",
       "    [[  86.  464.]\n",
       "     [ 275.  653.]]\n",
       "\n",
       "    [[ 149.  527.]\n",
       "     [ 338.  716.]]]]\n",
       "\n",
       "\n",
       "\n",
       "  [[[[  32.  572.]\n",
       "     [ 302.  842.]]\n",
       "\n",
       "    [[ 122.  662.]\n",
       "     [ 392.  932.]]\n",
       "\n",
       "    [[ 212.  752.]\n",
       "     [ 482. 1022.]]]\n",
       "\n",
       "\n",
       "   [[[  41.  743.]\n",
       "     [ 392. 1094.]]\n",
       "\n",
       "    [[ 158.  860.]\n",
       "     [ 509. 1211.]]\n",
       "\n",
       "    [[ 275.  977.]\n",
       "     [ 626. 1328.]]]\n",
       "\n",
       "\n",
       "   [[[  50.  914.]\n",
       "     [ 482. 1346.]]\n",
       "\n",
       "    [[ 194. 1058.]\n",
       "     [ 626. 1490.]]\n",
       "\n",
       "    [[ 338. 1202.]\n",
       "     [ 770. 1634.]]]]]\n",
       "\n",
       "\n",
       "\n",
       "\n",
       " [[[[[  59. 1085.]\n",
       "     [ 572. 1598.]]\n",
       "\n",
       "    [[ 230. 1256.]\n",
       "     [ 743. 1769.]]\n",
       "\n",
       "    [[ 401. 1427.]\n",
       "     [ 914. 1940.]]]\n",
       "\n",
       "\n",
       "   [[[  68. 1256.]\n",
       "     [ 662. 1850.]]\n",
       "\n",
       "    [[ 266. 1454.]\n",
       "     [ 860. 2048.]]\n",
       "\n",
       "    [[ 464. 1652.]\n",
       "     [1058. 2246.]]]\n",
       "\n",
       "\n",
       "   [[[  77. 1427.]\n",
       "     [ 752. 2102.]]\n",
       "\n",
       "    [[ 302. 1652.]\n",
       "     [ 977. 2327.]]\n",
       "\n",
       "    [[ 527. 1877.]\n",
       "     [1202. 2552.]]]]\n",
       "\n",
       "\n",
       "\n",
       "  [[[[  86. 1598.]\n",
       "     [ 842. 2354.]]\n",
       "\n",
       "    [[ 338. 1850.]\n",
       "     [1094. 2606.]]\n",
       "\n",
       "    [[ 590. 2102.]\n",
       "     [1346. 2858.]]]\n",
       "\n",
       "\n",
       "   [[[  95. 1769.]\n",
       "     [ 932. 2606.]]\n",
       "\n",
       "    [[ 374. 2048.]\n",
       "     [1211. 2885.]]\n",
       "\n",
       "    [[ 653. 2327.]\n",
       "     [1490. 3164.]]]\n",
       "\n",
       "\n",
       "   [[[ 104. 1940.]\n",
       "     [1022. 2858.]]\n",
       "\n",
       "    [[ 410. 2246.]\n",
       "     [1328. 3164.]]\n",
       "\n",
       "    [[ 716. 2552.]\n",
       "     [1634. 3470.]]]]]]\n",
       "<NDArray 2x2x3x3x2x2 @cpu(0)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = nd.dot(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
