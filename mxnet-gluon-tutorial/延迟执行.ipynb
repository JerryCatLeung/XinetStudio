{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 延迟执行\n",
    "\n",
    "MXNet 使用**延迟执行**来提升系统性能。绝大情况下我们不用知道它的存在，因为它不会对正常使用带来影响。但理解它的工作原理有助于开发更高效的程序。\n",
    "\n",
    "延迟执行是指命令可以等到之后它的结果真正的需要的时候再执行。我们先来看一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:05:40.995961Z",
     "start_time": "2018-04-20T10:05:40.991444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = 1 + 1\n",
    "# some other things\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一句对`a`赋值，再执行一些其指令后打印`a`的结果。因为这里我们可能很久以后才用`a`的值，所以我们可以把它的执行延迟到后面。这样的主要好处是在执行之前系统可以看到后面指令，从而有更多机会来对程序进行优化。例如如果`a`在被使用前被重新赋值了，那么我们可以不需要真正执行第一条语句。\n",
    "\n",
    "在MXNet里，我们把用户打交道的部分叫做**前端**。例如这个教程里我们一直在使用Python前端写代码。除了`Python`外，`MXNet`还支持其他例如`Scala`，`R`，`C++`的前端。不管使用什么前端，`MXNet`的程序执行主要都在 `C++` 后端。前端只是把程序传给后端。后端有自己的线程来不断的收集任务，构造计算图，优化，并执行。本章我们介绍后端优化之一：延迟执行。\n",
    "\n",
    "考虑下图的样例，我们在前端调用四条语句，它们被后端的线程分析依赖并构建成计算图。\n",
    "\n",
    "![](https://nbviewer.jupyter.org/github/q735613050/XinetStudio/tree/master/gluon-tutorials-zh/img/frontend-backend.svg)\n",
    "\n",
    "在延迟执行中，前端执行前三个语句的时候，它仅仅是把任务放进后端的队列里就返回了。当在需要打印结果时，前端会等待后端线程把`c`的结果计算完。\n",
    "\n",
    "这个设计的一个好处是前端，就是Python线程，不需要做实际计算工作，从而不管Python的性能如何，它对整个程序的影响会很小。只需要C++后端足够高效，那么不管前端语言性能如何，都可以提供一致的性能。\n",
    "\n",
    "下面的例子通过计时来展示了延后执行的效果。可以看到，当`y=...`返回的时候并没有等待它真的被计算完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:10:01.007430Z",
     "start_time": "2018-04-20T11:10:00.714878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workloads are queued:\t0.010012 sec\n",
      "\n",
      "[[ 501.15847778  508.29721069  495.6524353  ...,  492.84707642\n",
      "   492.69091797  490.04785156]\n",
      " [ 508.81048584  507.18218994  495.17425537 ...,  503.10540771  497.2930603\n",
      "   493.6791687 ]\n",
      " [ 489.56591797  499.47018433  490.17715454 ...,  490.99942017\n",
      "   488.05007935  483.28842163]\n",
      " ..., \n",
      " [ 484.00192261  495.71786499  479.9213562  ...,  493.69952393\n",
      "   478.89178467  487.20742798]\n",
      " [ 499.64923096  507.65090942  497.59384155 ...,  493.04745483\n",
      "   500.74511719  495.82705688]\n",
      " [ 516.01446533  519.17144775  506.35409546 ...,  510.08868408  496.3560791\n",
      "   495.42529297]]\n",
      "<NDArray 2000x2000 @cpu(0)>\n",
      "workloads are finished:\t0.283034 sec\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "x = nd.random_uniform(shape=(2000,2000))\n",
    "y = nd.dot(x, x)\n",
    "print('workloads are queued:\\t%f sec' % (time() - start))\n",
    "print(y)\n",
    "print('workloads are finished:\\t%f sec' % (time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "延迟执行大部分情况是对用户透明的。因为除非我们需要打印或者保存结果外，我们基本不需要关心目前是不是结果在内存里面已经计算好了。\n",
    "\n",
    "事实上，只要数据是保存在NDArray里，而且使用MXNet提供的运算子，后端将默认使用延迟执行来获取最大的性能。\n",
    "\n",
    "## 立即获取结果\n",
    "\n",
    "除了前面介绍的`print`外，我们还有别的方法可以让前端线程等待直到结果完成。我们可以使用`nd.NDArray.wait_to_read()`等待直到特定结果完成，或者`nd.waitall()`等待所有前面结果完成。后者是测试性能常用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:06:43.319596Z",
     "start_time": "2018-04-20T10:06:43.194063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11052918434143066"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.wait_to_read()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:06:44.721655Z",
     "start_time": "2018-04-20T10:06:44.465098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2495567798614502"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "z = nd.dot(x, x)\n",
    "nd.waitall()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何方法将内容从`NDArray`搬运到其他不支持延迟执行的数据结构里都会触发等待，例如`asnumpy()`, `asscalar()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:06:53.635250Z",
     "start_time": "2018-04-20T10:06:53.509196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1200411319732666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.asnumpy()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:06:56.766445Z",
     "start_time": "2018-04-20T10:06:56.486933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27251672744750977"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.norm().asscalar()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延迟执行带来的便利\n",
    "\n",
    "下面例子中，我们不断的对`y`进行赋值。如果每次我们需要等到`y`的值，那么我们必须要要计算它。而在延迟执行里，系统有可能省略掉一些执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T10:07:24.420318Z",
     "start_time": "2018-04-20T10:07:02.500784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lazy evaluation: 11.086034 sec\n",
      "With evaluation: 10.825502 sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "for i in range(1000):\n",
    "    y = x + 1\n",
    "    y.wait_to_read()\n",
    "\n",
    "print('No lazy evaluation: %f sec' % (time()-start))\n",
    "\n",
    "start = time()\n",
    "for i in range(1000):\n",
    "    y = x + 1\n",
    "nd.waitall()\n",
    "print('With evaluation: %f sec' % (time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延迟执行带来的影响\n",
    "\n",
    "在延迟执行里，只要最终结果是一致的，系统可能使用跟代码不一样的顺序来执行，例如假设我们写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T06:00:15.603401Z",
     "start_time": "2018-03-07T06:00:15.597381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一句和第二句之间没有依赖，所以把`b=2`提前到`a=1`前执行也是可以的。但这样可能会导致内存使用的变化。\n",
    "\n",
    "下面我们列举几个在训练和预测中常见的现象。一般每个批量我们都会评测一下，例如计算损失或者精度，其中会用到`asscalar`或者`asnumpy`。这样我们会每次仅仅将一个批量的任务放进后端系统执行。但如果我们去掉这些同步函数，会导致我们将大量的批量任务同时放进系统，从而可能导致系统占用过多资源。\n",
    "\n",
    "为了演示这种情况，我们定义一个数据获取函数，它会打印什么数据是什么时候被请求的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:09:10.416967Z",
     "start_time": "2018-04-20T11:09:10.410961Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    start = time()\n",
    "    batch_size = 1024\n",
    "    for i in range(60):\n",
    "        if i % 10 == 0:\n",
    "            print('batch %d, time %f sec' %(i, time()-start))\n",
    "        x = nd.ones((batch_size, 1024))\n",
    "        y = nd.ones((batch_size,))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用两层网络和和`L2`损失函数作为样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:09:35.345778Z",
     "start_time": "2018-04-20T11:09:13.172884Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Dense(1024, activation='relu'),\n",
    "        nn.Dense(1024, activation='relu'),\n",
    "        nn.Dense(1),\n",
    "    )\n",
    "net.initialize()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {})\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义辅助函数来监测内存的使用（只能在 `Linux` 运行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:09:42.534588Z",
     "start_time": "2018-04-20T11:09:42.527579Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_mem():\n",
    "    \"\"\"get memory usage in MB\"\"\"\n",
    "    res = subprocess.check_output(['ps', 'u', '-p', str(os.getpid())])\n",
    "    return int(str(res).split()[15])/1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以做测试了。我们先试运行一次让系统把`net`的参数初始化（回忆[延后初始化](http://zh.gluon.ai/chapter_gluon-basics/parameters.html)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:10:05.651127Z",
     "start_time": "2018-04-20T11:10:05.482063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000000 sec\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_data():\n",
    "    break\n",
    "loss(y, net(x)).wait_to_read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们用`net`来做预测，正常情况下对每个批量的结果我们把它复制出`NDArray`，例如打印或者保存在磁盘上。这里我们简单使用`wait_to_read`来模拟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-20T11:10:39.697424Z",
     "start_time": "2018-04-20T11:10:39.522381Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['ps', 'u', '-p', '20940']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-25423a102cac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_to_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7ee30d0dabd3>\u001b[0m in \u001b[0;36mget_mem\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_mem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"\"\"get memory usage in MB\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ps'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'u'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1e3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 336\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[1;32m--> 418\u001b[1;33m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['ps', 'u', '-p', '20940']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "for x, y in get_data():\n",
    "    loss(y, net(x)).wait_to_read()\n",
    "nd.waitall()\n",
    "\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们不使用`wait_to_read()`， 那么前端会将所有批量的计算一次性的添加进后端。可以看到每个批量的数据都会在很短的时间内生成，同时在接下来的数秒钟内，我们看到了内存的增长（包括了在内存中保存所有`x`和`y`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T06:03:44.838536Z",
     "start_time": "2018-03-07T06:03:44.726528Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['ps', 'u', '-p', '9168']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0a98eeb1d1c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-7ee30d0dabd3>\u001b[0m in \u001b[0;36mget_mem\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_mem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"\"\"get memory usage in MB\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ps'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'u'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1e3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 336\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[1;32m--> 418\u001b[1;33m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['ps', 'u', '-p', '9168']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "for x, y in get_data():\n",
    "    loss(y, net(x))\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样对于训练，如果我们每次计算损失，那么就加入了同步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "mem = get_mem()\n",
    "\n",
    "total_loss = 0\n",
    "for x, y in get_data():\n",
    "    with autograd.record():\n",
    "        L = loss(y, net(x))\n",
    "    total_loss += L.sum().asscalar()\n",
    "    L.backward()\n",
    "    trainer.step(x.shape[0])\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但如果不去掉同步，同样会首先把数据全部生成好，导致占用大量内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "mem = get_mem()\n",
    "\n",
    "total_loss = 0\n",
    "for x, y in get_data():\n",
    "    with autograd.record():\n",
    "        L = loss(y, net(x))\n",
    "    L.backward()\n",
    "    trainer.step(x.shape[0])\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "延后执行使得系统有更多空间来做性能优化。但我们推荐每个批量里至少有一个同步函数，例如对损失函数进行评估，来避免将过多任务同时丢进后端系统。\n",
    "\n",
    "## 练习\n",
    "\n",
    "为什么同步版本的训练中，我们看到了内存使用的大量下降？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1881)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
