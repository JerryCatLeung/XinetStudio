{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多GPU来训练 --- 使用Gluon\n",
    "\n",
    "\n",
    "在Gluon里可以很容易的使用数据并行。在[多GPU来训练 --- 从0开始](http://zh.gluon.ai/chapter_gluon-advances/multiple-gpus-scratch.html)里我们手动实现了几个数据同步函数来使用数据并行，Gluon里实现了同样的功能。\n",
    "\n",
    "\n",
    "## 多设备上的初始化\n",
    "\n",
    "之前我们介绍了如果使用`initialize()`里的`ctx`在CPU或者特定GPU上初始化模型。事实上，`ctx`可以接受一系列的设备，它会将初始好的参数复制所有的设备上。\n",
    "\n",
    "这里我们使用之前介绍Resnet18来作为演示。\n",
    "\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from mxnet import gpu\n",
    "from mxnet import cpu\n",
    "\n",
    "net = utils.resnet18(10)\n",
    "ctx = [gpu(0), gpu(1)]\n",
    "net.initialize(ctx=ctx)\n",
    "```\n",
    "\n",
    "记得前面提到的[延迟初始化](../chapter_gluon-basics/parameters.md)，这里参数还没有被初始化。我们需要先给定数据跑一次。\n",
    "\n",
    "Gluon提供了之前我们实现的`split_and_load`函数，它将数据分割并返回各个设备上的复制。然后根据输入的设备，计算也会在相应的数据上执行。\n",
    "\n",
    "```python\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "\n",
    "x = nd.random.uniform(shape=(4, 1, 28, 28))\n",
    "x_list = gluon.utils.split_and_load(x, ctx)\n",
    "print(net(x_list[0]))\n",
    "print(net(x_list[1]))\n",
    "```\n",
    "\n",
    "这时候我们可以来看初始的过程发生了什么了。记得我们可以通过`data`来访问参数值，它默认会返回CPU上值。但这里我们只在两个GPU上初始化了，在访问的对应设备的值的时候，我们需要指定设备。\n",
    "\n",
    "```python\n",
    "weight = net[1].params.get('weight')\n",
    "print(weight.data(ctx[0])[0])\n",
    "print(weight.data(ctx[1])[0])\n",
    "try:\n",
    "    weight.data(cpu())\n",
    "except:\n",
    "    print('Not initialized on', cpu())\n",
    "```\n",
    "\n",
    "上一章我们提到过如何在多GPU之间复制梯度求和并广播，这个在`gluon.Trainer`里面会被默认执行。这样我们可以实现完整的训练函数了。\n",
    "\n",
    "## 训练\n",
    "\n",
    "```python\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "from time import time\n",
    "from mxnet import init\n",
    "\n",
    "def train(num_gpus, batch_size, lr):\n",
    "    train_data, test_data = utils.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "    ctx = [gpu(i) for i in range(num_gpus)]\n",
    "    print('Running on', ctx)\n",
    "\n",
    "    net = utils.resnet18(10)\n",
    "    net.initialize(init=init.Xavier(), ctx=ctx)\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(),'sgd', {'learning_rate': lr})\n",
    "\n",
    "    for epoch in range(5):\n",
    "        start = time()\n",
    "        total_loss = 0\n",
    "        for data, label in train_data:\n",
    "            data_list = gluon.utils.split_and_load(data, ctx)\n",
    "            label_list = gluon.utils.split_and_load(label, ctx)\n",
    "            with autograd.record():\n",
    "                losses = [loss(net(X), y) for X, y in zip(\n",
    "                    data_list, label_list)]\n",
    "            for l in losses:\n",
    "                l.backward()\n",
    "            total_loss += sum([l.sum().asscalar() for l in losses])\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "        nd.waitall()\n",
    "        print('Epoch %d, training time = %.1f sec'%(\n",
    "            epoch, time()-start))\n",
    "\n",
    "        test_acc = utils.evaluate_accuracy(test_data, net, ctx[0])\n",
    "        print('         validation accuracy = %.4f'%(test_acc))\n",
    "```\n",
    "\n",
    "尝试在单GPU上执行。\n",
    "\n",
    "```python\n",
    "train(1, 256, .1)\n",
    "\n",
    "```\n",
    "\n",
    "同样的参数，但使用两个GPU。\n",
    "\n",
    "```python \n",
    "train(2, 256, .1)\n",
    "```\n",
    "\n",
    "增大批量值和学习率\n",
    "\n",
    "```input\n",
    "train(2, 512, .2)\n",
    "```\n",
    "\n",
    "## 结论\n",
    "\n",
    "Gluon的参数初始化和Trainer都支持多设备，从单设备到多设备非常容易。\n",
    "\n",
    "## 练习\n",
    "\n",
    "- 跟[多GPU来训练 --- 从0开始](http://zh.gluon.ai/chapter_gluon-advances/multiple-gpus-scratch.html)不一样，这里我们使用了更现代些的ResNet。看看不同的批量大小和学习率对不同GPU个数上的不一样。\n",
    "- 有时候各个设备计算能力不一样，例如同时使用CPU和GPU，或者GPU之间型号不一样，这时候应该怎么办？\n",
    "\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1885)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
