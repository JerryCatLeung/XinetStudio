{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning: 通过微调来迁移学习\n",
    "\n",
    "\n",
    "在前面的章节里我们展示了如何训练神经网络来识别小图片里的问题。我们也介绍了ImageNet这个学术界默认的数据集，它有超过一百万的图片和一千类的物体。这个数据集很大的改变计算机视觉这个领域，展示了很多事情虽然在小的数据集上做不到，但在数GB的大数据上是可能的。事实上，我们目前还不知道有什么技术可以在类似的但小图片数据集上，例如一万张图片，训练出一个同样强大的模型。\n",
    "\n",
    "所以这是一个问题。尽管深度卷积神经网络在ImageNet上有了很惊讶的结果，但大部分人不关心Imagenet这个数据集本身。他们关心他们自己的问题。例如通过图片里面的人脸识别身份，或者识别图片里面的10种不同的珊瑚。通常大部分在非BAT类似大机构里的人在解决计算机视觉问题的时候，能获得的只是相对来说中等规模的数据。几百张图片很正常，找到几千张图片也有可能，但很难同Imagenet一样获得上百万张图片。\n",
    "\n",
    "于是我们会有一个很自然的问题，如何使用在百万张图片上训练出来的强大的模型来帮助提升在小数据集上的精度呢？这种在源数据上训练，然后将学到的知识应用到目标数据集上的技术通常被叫做**迁移学习**。幸运的是，我们有一些有效的技术来解决这个问题。\n",
    "\n",
    "对于深度神经网络来首，最为流行的一个方法叫做微调（fine-tuning）。它的想法很简单但有效：\n",
    "\n",
    "\n",
    "* 在源数据 $S$ 上训练一个神经网络。\n",
    "* 砍掉它的头，将它的输出层改成适合目标数据 $S$ 的大小\n",
    "* 将输出层的权重初始化成随机值，但其它层保持跟原先训练好的权重一致\n",
    "* 然后开始在目标数据集开始训练\n",
    "\n",
    "## 热狗识别\n",
    "\n",
    "这一章我们将通过[ResNet](../chapter_convolutional-neural-networks/resnet-gluon.md)来演示如何进行微调。因为通常不会每次从0开始在ImageNet上训练模型，我们直接从Gluon的模型园下载已经训练好的。然后将其迁移到一个我们感兴趣的问题上：识别热狗。\n",
    "\n",
    "\n",
    "热狗识别是一个二分类问题。我们这里使用的热狗数据集是从网上抓取的，它有$1400$张正类和同样多的负类，负类主要是食品相关图片。我们将各类的$1000$张作为训练集合，其余的作为测试集合。\n",
    "\n",
    "### 获取数据\n",
    "\n",
    "我们首先从网上下载数据并解压到`../data/hotdog`。每个文件夹下会有对应的`png`文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T12:27:33.575574Z",
     "start_time": "2018-04-27T12:22:57.366718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading E:/Data\\hotdog.zip from https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/hotdog.zip...\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon\n",
    "import zipfile\n",
    "\n",
    "data_dir = 'E:/Data'\n",
    "fname = gluon.utils.download(\n",
    "    'https://apache-mxnet.s3-accelerate.amazonaws.com/gluon/dataset/hotdog.zip',\n",
    "    path=data_dir, sha1_hash='fba480ffa8aa7e0febbb511d181409f899b9baa5')\n",
    "\n",
    "with zipfile.ZipFile(fname, 'r') as f:\n",
    "    # 解压到 data_dir\n",
    "    f.extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-27T12:37:37.150627Z",
     "start_time": "2018-04-27T12:37:37.113083Z"
    }
   },
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile('E:/Data/hotdog.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用[图片增强](../image-augmentation.md)里类似的方法来处理图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet import image\n",
    "from mxnet import gluon\n",
    "\n",
    "train_augs = [\n",
    "    image.HorizontalFlipAug(.5),\n",
    "    image.RandomCropAug((224, 224))\n",
    "]\n",
    "\n",
    "test_augs = [\n",
    "    image.CenterCropAug((224, 224))\n",
    "]\n",
    "\n",
    "\n",
    "def transform(data, label, augs):\n",
    "    data = data.astype('float32')\n",
    "    for aug in augs:\n",
    "        data = aug(data)\n",
    "    data = nd.transpose(data, (2, 0, 1))\n",
    "    return data, nd.array([label]).asscalar().astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```{.python .input  n=18}\n",
    "\n",
    "```\n",
    "\n",
    "读取文件夹下的图片，并且画出一些图片\n",
    "\n",
    "```{.python .input  n=20}\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "\n",
    "train_imgs = gluon.data.vision.ImageFolderDataset(\n",
    "    data_dir+'/hotdog/train',\n",
    "    transform=lambda X, y: transform(X, y, train_augs))\n",
    "test_imgs = gluon.data.vision.ImageFolderDataset(\n",
    "    data_dir+'/hotdog/test',\n",
    "    transform=lambda X, y: transform(X, y, test_augs))\n",
    "\n",
    "data = gluon.data.DataLoader(train_imgs, 32, shuffle=True)\n",
    "for X, _ in data:\n",
    "    X = X.transpose((0,2,3,1)).clip(0,255)/255\n",
    "    utils.show_images(X, 4, 8)\n",
    "    break\n",
    "```\n",
    "\n",
    "### 模型和训练\n",
    "\n",
    "这里我们将使用Gluon提供的ResNet18来训练。我们先从模型园里获取改良过ResNet。使用`pretrained=True`将会自动下载并加载从ImageNet数据集上训练而来的权重。\n",
    "\n",
    "```{.python .input  n=21}\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "\n",
    "pretrained_net = models.resnet18_v2(pretrained=True)\n",
    "```\n",
    "\n",
    "通常预训练好的模型由两块构成，一是`features`，二是`output`。后者主要包括最后一层全连接层，前者包含从输入开始的大部分层。这样的划分的一个主要目的是为了更方便做微调。我们先看下`output`的内容：\n",
    "\n",
    "```{.python .input  n=22}\n",
    "pretrained_net.output\n",
    "```\n",
    "\n",
    "我们可以看一下第一个卷积层的部分权重。\n",
    "\n",
    "```{.python .input  n=23}\n",
    "pretrained_net.features[1].weight.data()[0][0]\n",
    "```\n",
    "\n",
    "在微调里，我们一般新建一个网络，它的定义跟之前训练好的网络一样，除了最后的输出数等于当前数据的类别数。新网络的`features`被初始化前面训练好网络的权重，而`output`则是从头开始训练。\n",
    "\n",
    "```{.python .input  n=24}\n",
    "from mxnet import init\n",
    "\n",
    "finetune_net = models.resnet18_v2(classes=2)\n",
    "finetune_net.features = pretrained_net.features\n",
    "finetune_net.output.initialize(init.Xavier())\n",
    "```\n",
    "\n",
    "我们先定义一个可以重复使用的训练函数。\n",
    "\n",
    "```{.python .input  n=25}\n",
    "\n",
    "def train(net, ctx, batch_size=64, epochs=10, learning_rate=0.01, wd=0.001):\n",
    "    train_data = gluon.data.DataLoader(train_imgs, batch_size, shuffle=True)\n",
    "    test_data = gluon.data.DataLoader(test_imgs, batch_size)\n",
    "\n",
    "    # 确保net的初始化在ctx上\n",
    "    net.collect_params().reset_ctx(ctx)\n",
    "    net.hybridize()\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    # 训练\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {\n",
    "        'learning_rate': learning_rate, 'wd': wd})\n",
    "    utils.train(train_data, test_data, net, loss, trainer, ctx, epochs)\n",
    "```\n",
    "\n",
    "现在我们可以训练了。\n",
    "\n",
    "```{.python .input  n=10}\n",
    "ctx = utils.try_all_gpus()\n",
    "train(finetune_net, ctx)\n",
    "```\n",
    "\n",
    "对比起见我们尝试从随机初始值开始训练一个网络。\n",
    "\n",
    "```{.python .input  n=11}\n",
    "scratch_net = models.resnet18_v2(classes=2)\n",
    "scratch_net.initialize(init=init.Xavier())\n",
    "train(scratch_net, ctx)\n",
    "```\n",
    "\n",
    "可以看到，微调版本收敛比从随机值开始的要快很多。\n",
    "\n",
    "### 图片预测\n",
    "\n",
    "```{.python .input  n=12}\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def classify_hotdog(net, fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        img = image.imdecode(f.read())\n",
    "    data, _ = transform(img, -1, test_augs)\n",
    "    plt.imshow(data.transpose((1,2,0)).asnumpy()/255)\n",
    "    data = data.expand_dims(axis=0)\n",
    "    out = net(data.as_in_context(ctx[0]))\n",
    "    out = nd.SoftmaxActivation(out)\n",
    "    pred = int(nd.argmax(out, axis=1).asscalar())\n",
    "    prob = out[0][pred].asscalar()\n",
    "    label = train_imgs.synsets\n",
    "    return 'With prob=%f, %s'%(prob, label[pred])\n",
    "```\n",
    "\n",
    "接下来我们用训练好的模型来预测几张图片：\n",
    "\n",
    "```{.python .input  n=13}\n",
    "classify_hotdog(finetune_net, '../img/real_hotdog.jpg')\n",
    "```\n",
    "\n",
    "```{.python .input  n=14}\n",
    "classify_hotdog(finetune_net, '../img/leg_hotdog.jpg')\n",
    "```\n",
    "\n",
    "```{.python .input  n=15}\n",
    "classify_hotdog(finetune_net, '../img/dog_hotdog.jpg')\n",
    "```\n",
    "\n",
    "## 结论\n",
    "\n",
    "我们看到，通过一个预先训练好的模型，我们可以在即使较小的数据集上训练得到很好的分类器。这是因为这两个任务里面的数据表示有很多共通性，例如都需要如何识别纹理、形状、边等等。而这些通常被在靠近数据的层有效的处理。因此，如果你有一个相对较小的数据在手，而且担心它可能不够训练出很好的模型，你可以寻找跟你数据类似的大数据集来先训练你的模型，然后再在你手上的数据集上微调。\n",
    "\n",
    "## 练习\n",
    "\n",
    "- 多跑几个`epochs`直到收敛（你可以也需要调调参数），看看`scratch_net`和`finetune_net`最后的精度是不是有区别\n",
    "- 这里`finetune_net`重用了`pretrained_net`除最后全连接外的所有权重，试试少重用些权重，有会有什么区别\n",
    "- 事实上`ImageNet`里也有`hotdog`这个类，它的index是713。例如它对应的weight可以这样拿到。试试如何重用这个权重\n",
    "\n",
    "```{.python .input  n=16}\n",
    "weight = pretrained_net.output.weight\n",
    "hotdog_w = nd.split(weight.data(), 1000, axis=0)[713]\n",
    "hotdog_w.shape\n",
    "```\n",
    "\n",
    "- 试试不让`finetune_net`里重用的权重参与训练，就是不更新权重\n",
    "- 如果图片预测这一章里我们训练的模型没有分对所有的图片，如何改进？\n",
    "\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/2272)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
