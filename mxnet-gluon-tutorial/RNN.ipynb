{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对输入输出数据有了解后，我们来正式介绍**循环神经网络**。\n",
    "\n",
    "首先回忆一下单隐含层的前馈神经网络的定义，例如[多层感知机](../chapter_supervised-learning/mlp-scratch.md)。假设隐含层的激活函数是$\\phi$，对于一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$（$\\mathbf{X}$是一个$n$行$x$列的实数矩阵）来说，那么这个隐含层的输出就是\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h)$$\n",
    "\n",
    "假定隐含层长度为$h$，其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$是权重参数。偏移参数 $\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$在与前一项$\\mathbf{X} \\mathbf{W}_{xh} \\in \\mathbb{R}^{n \\times h}$ 相加时使用了[广播](../chapter_crashcourse/ndarray.md)。这个隐含层的输出的尺寸为$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "把隐含层的输出$\\mathbf{H}$作为输出层的输入，最终的输出\n",
    "\n",
    "$$\\hat{\\mathbf{Y}} = \\text{softmax}(\\mathbf{H} \\mathbf{W}_{hy} + \\mathbf{b}_y)$$\n",
    "\n",
    "假定每个样本对应的输出向量维度为$y$，其中 $\\hat{\\mathbf{Y}} \\in \\mathbb{R}^{n \\times y}, \\mathbf{W}_{hy} \\in \\mathbb{R}^{h \\times y}, \\mathbf{b}_y \\in \\mathbb{R}^{1 \\times y}$且两项相加使用了[广播](../chapter_crashcourse/ndarray.md)。\n",
    "\n",
    "\n",
    "将上面网络改成循环神经网络，我们首先对输入输出加上时间戳$t$。假设$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$是序列中的第$t$个批量输入（样本数为$n$，每个样本的特征向量维度为$x$），对应的隐含层输出是隐含状态$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$（隐含层长度为$h$），而对应的最终输出是$\\hat{\\mathbf{Y}}_t \\in \\mathbb{R}^{n \\times y}$（每个样本对应的输出向量维度为$y$）。在计算隐含层的输出的时候，循环神经网络只需要在前馈神经网络基础上加上跟前一时间$t-1$输入隐含层$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$的加权和。为此，我们引入一个新的可学习的权重$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h)$$\n",
    "\n",
    "输出的计算跟前面一致：\n",
    "\n",
    "$$\\hat{\\mathbf{Y}}_t = \\text{softmax}(\\mathbf{H}_t \\mathbf{W}_{hy}  + \\mathbf{b}_y)$$\n",
    "\n",
    "一开始我们提到过，隐含状态可以认为是这个网络的记忆。该网络中，时刻$t$的隐含状态就是该时刻的隐含层变量$\\mathbf{H}_t$。它存储前面时间里面的信息。我们的输出是只基于这个状态。最开始的隐含状态里的元素通常会被初始化为0。\n",
    "\n",
    "# 周杰伦歌词数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:44:22.221966Z",
     "start_time": "2018-03-13T04:44:22.210995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机\n",
      "想要和你飞到宇宙去\n",
      "想要和你融化在一起\n",
      "融化在宇宙里\n",
      "我每天每天每天在想想想想著你\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('data/')\n",
    "\n",
    "with open('data/jaychou_lyrics.txt', encoding='utf-8') as f:\n",
    "    corpus_chars = f.read()\n",
    "print(corpus_chars[0:49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:45:14.673874Z",
     "start_time": "2018-03-13T04:45:14.667887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64925"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着我们稍微处理下数据集。为了打印方便，我们把换行符替换成空格，然后截去后面一段使得接下来的训练会快一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:45:23.778679Z",
     "start_time": "2018-03-13T04:45:23.773674Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字符的数值表示\n",
    "\n",
    "先把数据里面所有不同的字符拿出来做成一个字典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:46:04.288782Z",
     "start_time": "2018-03-13T04:46:04.282771Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:48:48.930608Z",
     "start_time": "2018-03-13T04:48:48.926573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 1465\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(char_to_idx)\n",
    "\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后可以把每个字符转成从$0$开始的索引(index)来方便之后的使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:50:42.522060Z",
     "start_time": "2018-03-13T04:50:42.511059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: \n",
      " 想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n",
      "\n",
      "indices: \n",
      " [199, 1334, 1078, 578, 1086, 152, 1449, 199, 1334, 108, 673, 46, 1429, 902, 56, 775, 1449, 199, 1334, 108, 673, 58, 549, 1430, 176, 883, 1449, 58, 549, 1430, 902, 56, 1444, 1449, 780, 101, 651, 101, 651, 101]\n"
     ]
    }
   ],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "sample = corpus_indices[:40]\n",
    "\n",
    "print('chars: \\n', ''.join([idx_to_char[idx] for idx in sample]))\n",
    "print('\\nindices: \\n', sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时序数据的批量采样\n",
    "\n",
    "同之前一样我们需要每次随机读取一些（`batch_size`个）样本和其对用的标号。这里的样本跟前面有点不一样，这里一个样本通常包含一系列连续的字符（前馈神经网络里可能每个字符作为一个样本）。\n",
    "\n",
    "如果我们把序列长度（`num_steps`）设成 $5$，那么一个可能的样本是`'想要有直升'`。其对应的标号仍然是长为 $5$ 的序列，每个字符是对应的样本里字符的后面那个。例如前面样本的标号就是`'想要有直升'`。\n",
    "\n",
    "## 随机批量采样\n",
    "下面代码每次从数据里随机采样一个批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:55:53.142068Z",
     "start_time": "2018-03-13T04:55:53.120072Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from mxnet import nd\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    # 减一是因为 label 的索引是相应 data 的索引加一\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    # 随机化样本\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回num_steps个数据\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        data = nd.array(\n",
    "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
    "        label = nd.array(\n",
    "            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了便于理解时序数据上的随机批量采样，让我们输入一个从$0$到$29$的人工序列，看下读出来长什么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:55:54.309783Z",
     "start_time": "2018-03-13T04:55:54.295800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      "[[ 0.  1.  2.]\n",
      " [12. 13. 14.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 1.  2.  3.]\n",
      " [13. 14. 15.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[21. 22. 23.]\n",
      " [ 6.  7.  8.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[22. 23. 24.]\n",
      " [ 7.  8.  9.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[15. 16. 17.]\n",
      " [ 9. 10. 11.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[16. 17. 18.]\n",
      " [10. 11. 12.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[24. 25. 26.]\n",
      " [ 3.  4.  5.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[25. 26. 27.]\n",
      " [ 4.  5.  6.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "\n",
    "for data, label in data_iter_random(my_seq, batch_size=2, num_steps=3):\n",
    "    print('data: ', data, '\\nlabel:', label, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于各个采样在原始序列上的位置是随机的时序长度为`num_steps`的连续数据点，相邻的两个随机批量在原始序列上的位置不一定相毗邻。因此，在训练模型时，读取每个随机时序批量前需要重新初始化隐含状态。\n",
    "\n",
    "## 相邻批量采样\n",
    "除了对原序列做随机批量采样之外，我们还可以使相邻的两个随机批量在原始序列上的位置相毗邻。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:56:13.339123Z",
     "start_time": "2018-03-13T04:56:13.325088Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        data = indices[:, i: i + num_steps]\n",
    "        label = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相同地，为了便于理解时序数据上的相邻批量采样，让我们输入一个从$0$到$29$的人工序列，看下读出来长什么样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:56:14.402702Z",
     "start_time": "2018-03-13T04:56:14.369706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  \n",
      "[[ 0.  1.  2.]\n",
      " [15. 16. 17.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 1.  2.  3.]\n",
      " [16. 17. 18.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[ 3.  4.  5.]\n",
      " [18. 19. 20.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 4.  5.  6.]\n",
      " [19. 20. 21.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[ 6.  7.  8.]\n",
      " [21. 22. 23.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[ 7.  8.  9.]\n",
      " [22. 23. 24.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n",
      "data:  \n",
      "[[ 9. 10. 11.]\n",
      " [24. 25. 26.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "label: \n",
      "[[10. 11. 12.]\n",
      " [25. 26. 27.]]\n",
      "<NDArray 2x3 @cpu(0)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_seq = list(range(30))\n",
    "\n",
    "for data, label in data_iter_consecutive(my_seq, batch_size=2, num_steps=3):\n",
    "    print('data: ', data, '\\nlabel:', label, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于各个采样在原始序列上的位置是毗邻的时序长度为`num_steps`的连续数据点，因此，使用相邻批量采样训练模型时，读取每个时序批量前，我们需要将该批量最开始的隐含状态设为上个批量最终输出的隐含状态。在同一个epoch中，隐含状态只需要在该epoch开始的时候初始化。\n",
    "\n",
    "\n",
    "## One-hot向量\n",
    "\n",
    "注意到每个字符现在是用一个整数来表示，而输入进网络我们需要一个定长的向量。一个常用的办法是使用one-hot来将其表示成向量。也就是说，如果一个字符的整数值是$i$, 那么我们创建一个全0的长为`vocab_size`的向量，并将其第$i$位设成`1`。该向量就是对原字符的 one-hot 向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:39:32.593481Z",
     "start_time": "2018-03-13T04:39:32.572485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1. 0. 0. ... 0. 0. 0.]\n",
       " [0. 0. 1. ... 0. 0. 0.]]\n",
       "<NDArray 2x1465 @cpu(0)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.one_hot(nd.array([0, 2]), vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记得前面我们每次得到的数据是一个`batch_size * num_steps`的批量。下面这个函数将其转换成`num_steps`个可以输入进网络的`batch_size * vocab_size`的矩阵。对于一个长度为`num_steps`的序列，每个批量输入$\\mathbf{X} \\in \\mathbb{R}^{n \\times x}$，其中$n=$ `batch_size`，而$x=$`vocab_size`（onehot编码向量维度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:57:00.385535Z",
     "start_time": "2018-03-13T04:57:00.375542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input length:  3\n",
      "input[0] shape:  (2, 1465)\n"
     ]
    }
   ],
   "source": [
    "def get_inputs(data):\n",
    "    return [nd.one_hot(X, vocab_size) for X in data.T]\n",
    "\n",
    "inputs = get_inputs(data)\n",
    "\n",
    "print('input length: ', len(inputs))\n",
    "print('input[0] shape: ', inputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "对于序列中任意一个时间戳，一个字符的输入是维度为`vocab_size`的 one-hot 向量，对应输出是预测下一个时间戳为词典中任意字符的概率，因而该输出是维度为`vocab_size`的向量。\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`（对应模型定义中的$n$）的批量，每个时间戳上的输入和输出皆为尺寸`batch_size * vocab_size`（对应模型定义中的$n \\times x$）的矩阵。假设每个样本对应的隐含状态的长度为`hidden_dim`（对应模型定义中隐含层长度$h$），根据矩阵乘法定义，我们可以推断出模型隐含层和输出层中各个参数的尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:58:28.099554Z",
     "start_time": "2018-03-13T04:58:24.537762Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use gpu(0)\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "# 尝试使用GPU\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "ctx = utils.try_gpu()\n",
    "print('Will use', ctx)\n",
    "\n",
    "input_dim = vocab_size\n",
    "# 隐含状态长度\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "std = .01\n",
    "\n",
    "def get_params():\n",
    "    # 隐含层\n",
    "    W_xh = nd.random_normal(scale=std, shape=(input_dim, hidden_dim), ctx=ctx)\n",
    "    W_hh = nd.random_normal(scale=std, shape=(hidden_dim, hidden_dim), ctx=ctx)\n",
    "    b_h = nd.zeros(hidden_dim, ctx=ctx)\n",
    "\n",
    "    # 输出层\n",
    "    W_hy = nd.random_normal(scale=std, shape=(hidden_dim, output_dim), ctx=ctx)\n",
    "    b_y = nd.zeros(output_dim, ctx=ctx)\n",
    "\n",
    "    params = [W_xh, W_hh, b_h, W_hy, b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "当序列中某一个时间戳的输入为一个样本数为`batch_size`的批量，而整个序列长度为`num_steps`时，以下`rnn`函数的`inputs`和`outputs`皆为`num_steps` 个尺寸为`batch_size * vocab_size`的矩阵，隐含变量$\\mathbf{H}$是一个尺寸为`batch_size * hidden_dim`的矩阵。该隐含变量$\\mathbf{H}$也是循环神经网络的隐含状态`state`。\n",
    "\n",
    "我们将前面的模型公式翻译成代码。这里的激活函数使用了按元素操作的双曲正切函数\n",
    "\n",
    "$$\\text{tanh}(x) = \\frac{1 - e^{-2x}}{1 + e^{-2x}}$$\n",
    "\n",
    "需要注意的是，双曲正切函数的值域是$[-1, 1]$。如果自变量均匀分布在整个实域，该激活函数输出的均值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:58:57.088666Z",
     "start_time": "2018-03-13T04:58:57.078678Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn(inputs, state, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵。\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。\n",
    "    H = state\n",
    "    W_xh, W_hh, b_h, W_hy, b_y = params\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n",
    "        Y = nd.dot(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做个简单的测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T04:59:28.932037Z",
     "start_time": "2018-03-13T04:59:25.515718Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output length:  3\n",
      "output[0] shape:  (2, 1465)\n",
      "state shape:  (2, 256)\n"
     ]
    }
   ],
   "source": [
    "state = nd.zeros(shape=(data.shape[0], hidden_dim), ctx=ctx)\n",
    "\n",
    "params = get_params()\n",
    "outputs, state_new = rnn(get_inputs(data.as_in_context(ctx)), state, *params)\n",
    "\n",
    "print('output length: ',len(outputs))\n",
    "print('output[0] shape: ', outputs[0].shape)\n",
    "print('state shape: ', state_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测序列\n",
    "\n",
    "在做预测时我们只需要给定时间0的输入和起始隐含变量。然后我们每次将上一个时间的输出作为下一个时间的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T05:00:01.656603Z",
     "start_time": "2018-03-13T05:00:01.626588Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_rnn(rnn, prefix, num_chars, params, hidden_dim, ctx, idx_to_char,\n",
    "                char_to_idx, get_inputs, is_lstm=False):\n",
    "    # 预测以 prefix 开始的接下来的 num_chars 个字符。\n",
    "    prefix = prefix.lower()\n",
    "    state_h = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    if is_lstm:\n",
    "        # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "        state_c = nd.zeros(shape=(1, hidden_dim), ctx=ctx)\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "    for i in range(num_chars + len(prefix)):\n",
    "        X = nd.array([output[-1]], ctx=ctx)\n",
    "        # 在序列中循环迭代隐含变量。\n",
    "        if is_lstm:\n",
    "            # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "            Y, state_h, state_c = rnn(get_inputs(X), state_h, state_c, *params)\n",
    "        else:\n",
    "            Y, state_h = rnn(get_inputs(X), state_h, *params)\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = int(Y[0].argmax(axis=1).asscalar())\n",
    "        output.append(next_input)\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度剪裁\n",
    "\n",
    "我们在[正向传播和反向传播](../chapter_supervised-learning/backprop.md)中提到，\n",
    "训练神经网络往往需要依赖梯度计算的优化算法，例如我们之前介绍的[随机梯度下降](../chapter_supervised-learning/linear-regression-scratch.md)。\n",
    "而在循环神经网络的训练中，当每个时序训练数据样本的时序长度`num_steps`较大或者时刻$t$较小，目标函数有关$t$时刻的隐含层变量梯度较容易出现衰减（valishing）或爆炸（explosion）。我们会在[下一节](bptt.md)详细介绍出现该现象的原因。\n",
    "\n",
    "为了应对梯度爆炸，一个常用的做法是如果梯度特别大，那么就投影到一个比较小的尺度上。假设我们把所有梯度接成一个向量 $\\boldsymbol{g}$，假设剪裁的阈值是$\\theta$，那么我们这样剪裁使得$\\|\\boldsymbol{g}\\|$不会超过$\\theta$：\n",
    "\n",
    "$$ \\boldsymbol{g} = \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T05:00:33.192057Z",
     "start_time": "2018-03-13T05:00:33.184082Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_clipping(params, theta, ctx):\n",
    "    if theta is not None:\n",
    "        norm = nd.array([0.0], ctx)\n",
    "        for p in params:\n",
    "            norm += nd.sum(p.grad ** 2)\n",
    "        norm = nd.sqrt(norm).asscalar()\n",
    "        if norm > theta:\n",
    "            for p in params:\n",
    "                p.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "下面我们可以还是训练模型。跟前面前置网络的教程比，这里有以下几个不同。\n",
    "\n",
    "1. 通常我们使用困惑度（Perplexity）这个指标。\n",
    "2. 在更新前我们对梯度做剪裁。\n",
    "3. 在训练模型时，对时序数据采用不同批量采样方法将导致隐含变量初始化的不同。\n",
    "\n",
    "### 困惑度（Perplexity）\n",
    "\n",
    "回忆以下我们之前介绍的[交叉熵损失函数](../chapter_supervised-learning/softmax-regression-scratch.md)。在语言模型中，该损失函数即被预测字符的对数似然平均值的相反数：\n",
    "\n",
    "$$\\text{loss} = -\\frac{1}{N} \\sum_{i=1}^N \\log p_{\\text{target}_i}$$\n",
    "\n",
    "其中$N$是预测的字符总数，$p_{\\text{target}_i}$是在第$i$个预测中真实的下个字符被预测的概率。\n",
    "\n",
    "而这里的困惑度可以简单的认为就是对交叉熵做exp运算使得数值更好读。\n",
    "\n",
    "为了解释困惑度的意义，我们先考虑一个完美结果：模型总是把真实的下个字符的概率预测为1。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1$。这种完美情况下，困惑度值为1。\n",
    "\n",
    "我们再考虑一个基线结果：给定不重复的字符集合$W$及其字符总数$|W|$，模型总是预测下个字符为集合$W$中任一字符的概率都相同。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 1/|W|$。这种基线情况下，困惑度值为$|W|$。\n",
    "\n",
    "最后，我们可以考虑一个最坏结果：模型总是把真实的下个字符的概率预测为0。也就是说，对任意的$i$来说，$p_{\\text{target}_i} = 0$。这种最坏情况下，困惑度值为正无穷。\n",
    "\n",
    "任何一个有效模型的困惑度值必须小于预测集中元素的数量。在本例中，困惑度必须小于字典中的字符数$|W|$。如果一个模型可以取得较低的困惑度的值（更靠近1），通常情况下，该模型预测更加准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T05:01:25.697153Z",
     "start_time": "2018-03-13T05:01:25.581155Z"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from math import exp\n",
    "           \n",
    "def train_and_predict_rnn(rnn, is_random_iter, epochs, num_steps, hidden_dim, \n",
    "                          learning_rate, clipping_theta, batch_size,\n",
    "                          pred_period, pred_len, seqs, get_params, get_inputs,\n",
    "                          ctx, corpus_indices, idx_to_char, char_to_idx,\n",
    "                          is_lstm=False):\n",
    "    if is_random_iter:\n",
    "        data_iter = data_iter_random\n",
    "    else:\n",
    "        data_iter = data_iter_consecutive\n",
    "    params = get_params()\n",
    "    \n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        # 如使用相邻批量采样，在同一个epoch中，隐含变量只需要在该epoch开始的时候初始化。\n",
    "        if not is_random_iter:\n",
    "            state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            if is_lstm:\n",
    "                # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "        train_loss, num_examples = 0, 0\n",
    "        for data, label in data_iter(corpus_indices, batch_size, num_steps, \n",
    "                                     ctx):\n",
    "            # 如使用随机批量采样，处理每个随机小批量前都需要初始化隐含变量。\n",
    "            if is_random_iter:\n",
    "                state_h = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "                if is_lstm:\n",
    "                    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                    state_c = nd.zeros(shape=(batch_size, hidden_dim), ctx=ctx)\n",
    "            with autograd.record():\n",
    "                # outputs 尺寸：(batch_size, vocab_size)\n",
    "                if is_lstm:\n",
    "                    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                    outputs, state_h, state_c = rnn(get_inputs(data), state_h,\n",
    "                                                    state_c, *params) \n",
    "                else:\n",
    "                    outputs, state_h = rnn(get_inputs(data), state_h, *params)\n",
    "                # 设t_ib_j为i时间批量中的j元素:\n",
    "                # label 尺寸：（batch_size * num_steps）\n",
    "                # label = [t_0b_0, t_0b_1, ..., t_1b_0, t_1b_1, ..., ]\n",
    "                label = label.T.reshape((-1,))\n",
    "                # 拼接outputs，尺寸：(batch_size * num_steps, vocab_size)。\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                # 经上述操作，outputs和label已对齐。\n",
    "                loss = softmax_cross_entropy(outputs, label)\n",
    "            loss.backward()\n",
    "\n",
    "            grad_clipping(params, clipping_theta, ctx)\n",
    "            utils.SGD(params, learning_rate)\n",
    "\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            num_examples += loss.size\n",
    "\n",
    "        if e % pred_period == 0:\n",
    "            print(\"Epoch %d. Perplexity %f\" % (e, \n",
    "                                               exp(train_loss/num_examples)))\n",
    "            for seq in seqs:\n",
    "                print(' - ', predict_rnn(rnn, seq, pred_len, params,\n",
    "                      hidden_dim, ctx, idx_to_char, char_to_idx, get_inputs,\n",
    "                      is_lstm))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下定义模型参数和预测序列前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T05:01:46.745330Z",
     "start_time": "2018-03-13T05:01:46.737346Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "num_steps = 35\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "seq1 = '分开'\n",
    "seq2 = '不分开'\n",
    "seq3 = '战争中部队'\n",
    "seqs = [seq1, seq2, seq3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先采用随机批量采样实验循环神经网络谱写歌词。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T05:02:44.069447Z",
     "start_time": "2018-03-13T05:02:12.541647Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in greater\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-1efe2b5aca9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                       \u001b[0mget_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                       \u001b[0mcorpus_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_to_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                       char_to_idx=char_to_idx)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-f30ada263c64>\u001b[0m in \u001b[0;36mtrain_and_predict_rnn\u001b[1;34m(rnn, is_random_iter, epochs, num_steps, hidden_dim, learning_rate, clipping_theta, batch_size, pred_period, pred_len, seqs, get_params, get_inputs, ctx, corpus_indices, idx_to_char, char_to_idx, is_lstm)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpred_period\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             print(\"Epoch %d. Perplexity %f\" % (e, \n\u001b[1;32m---> 60\u001b[1;33m                                                exp(train_loss/num_examples)))\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 print(' - ', predict_rnn(rnn, seq, pred_len, params,\n",
      "\u001b[1;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=True, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再采用相邻批量采样实验循环神经网络谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_predict_rnn(rnn=rnn, is_random_iter=False, epochs=200, num_steps=35,\n",
    "                      hidden_dim=hidden_dim, learning_rate=0.2,\n",
    "                      clipping_theta=5, batch_size=32, pred_period=20,\n",
    "                      pred_len=100, seqs=seqs, get_params=get_params,\n",
    "                      get_inputs=get_inputs, ctx=ctx,\n",
    "                      corpus_indices=corpus_indices, idx_to_char=idx_to_char,\n",
    "                      char_to_idx=char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到一开始学到简单的字符，然后简单的词，接着是复杂点的词，然后看上去似乎像个句子了。\n",
    "\n",
    "## 结论\n",
    "\n",
    "* 通过隐含状态，循环神经网络适合处理前后有依赖关系时序数据样本。\n",
    "* 对前后有依赖关系时序数据样本批量采样时，我们可以使用随机批量采样和相邻批量采样。\n",
    "* 循环神经网络较容易出现梯度衰减和爆炸。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 调调参数（例如数据集大小、序列长度、隐含状态长度和学习率），看看对运行时间、perplexity和预测的结果造成的影响。\n",
    "* 在随机批量采样中，如果在同一个epoch中只把隐含变量在该epoch开始的时候初始化会怎么样？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
