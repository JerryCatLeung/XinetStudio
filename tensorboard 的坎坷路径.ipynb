{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：[My TensorBoard isn't showing any data! What's wrong?](https://github.com/tensorflow/tensorboard/blob/master/README.md#my-tensorboard-isnt-showing-any-data-whats-wrong)\n",
    "\n",
    "# TensorBoard ![Travis build status](https://travis-ci.org/tensorflow/tensorboard.svg?branch=master)\n",
    "\n",
    "TensorBoard is a suite of web applications for inspecting and understanding your\n",
    "TensorFlow runs and graphs.\n",
    "\n",
    "This README gives an overview of key concepts in TensorBoard, as well as how to\n",
    "interpret the visualizations TensorBoard provides. For an in-depth example of\n",
    "using TensorBoard, see the tutorial: [TensorBoard: Visualizing\n",
    "Learning][].\n",
    "For in-depth information on the Graph Visualizer, see this tutorial: \n",
    "[TensorBoard: Graph Visualization][].\n",
    "\n",
    "[TensorBoard: Visualizing Learning]: https://www.tensorflow.org/get_started/summaries_and_tensorboard\n",
    "[TensorBoard: Graph Visualization]: https://www.tensorflow.org/get_started/graph_viz\n",
    "\n",
    "You may also want to watch\n",
    "[this video tutorial][] that walks\n",
    "through setting up and using TensorBoard. There's an associated \n",
    "[tutorial with an end-to-end example of training TensorFlow and using TensorBoard][].\n",
    "\n",
    "[this video tutorial]: https://www.youtube.com/watch?v=eBbEDRsCmv4\n",
    "\n",
    "[tutorial with an end-to-end example of training TensorFlow and using TensorBoard]: https://github.com/dandelionmane/tf-dev-summit-tensorboard-tutorial\n",
    "\n",
    "# Usage\n",
    "\n",
    "Before running TensorBoard, make sure you have generated summary data in a log\n",
    "directory by creating a summary writer:\n",
    "\n",
    "``` python\n",
    "# sess.graph contains the graph definition; that enables the Graph Visualizer.\n",
    "\n",
    "file_writer = tf.summary.FileWriter('/path/to/logs', sess.graph)\n",
    "```\n",
    "\n",
    "For more details, see \n",
    "[the TensorBoard tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
    "Once you have event files, run TensorBoard and provide the log directory. If\n",
    "you're using a precompiled TensorFlow package (e.g. you installed via pip), run:\n",
    "\n",
    "```\n",
    "tensorboard --logdir path/to/logs\n",
    "```\n",
    "\n",
    "Or, if you are building from source:\n",
    "\n",
    "```bash\n",
    "bazel build tensorboard:tensorboard\n",
    "./bazel-bin/tensorboard/tensorboard --logdir path/to/logs\n",
    "\n",
    "# or even more succinctly\n",
    "bazel run tensorboard -- --logdir path/to/logs\n",
    "```\n",
    "\n",
    "This should print that TensorBoard has started. Next, connect to\n",
    "http://localhost:6006.\n",
    "\n",
    "TensorBoard requires a `logdir` to read logs from. For info on configuring\n",
    "TensorBoard, run `tensorboard --help`.\n",
    "\n",
    "TensorBoard can be used in Google Chrome or Firefox. Other browsers might\n",
    "work, but there may be bugs or performance issues.\n",
    "\n",
    "# Key Concepts\n",
    "\n",
    "### Summary Ops: How TensorBoard gets data from TensorFlow\n",
    "\n",
    "The first step in using TensorBoard is acquiring data from your TensorFlow run.\n",
    "For this, you need \n",
    "[summary ops](https://www.tensorflow.org/api_docs/python/tf/summary).\n",
    "Summary ops are ops, like\n",
    "[`tf.matmul`](https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/matmul)\n",
    "or\n",
    "[`tf.nn.relu`](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/relu),\n",
    "which means they take in tensors, produce tensors, and are evaluated from within\n",
    "a TensorFlow graph. However, summary ops have a twist: the Tensors they produce\n",
    "contain serialized protobufs, which are written to disk and sent to TensorBoard.\n",
    "To visualize the summary data in TensorBoard, you should evaluate the summary\n",
    "op, retrieve the result, and then write that result to disk using a\n",
    "summary.FileWriter. A full explanation, with examples, is in [the\n",
    "tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
    "\n",
    "The supported summary ops include:\n",
    "* tf.summary.scalar\n",
    "* tf.summary.image\n",
    "* tf.summary.audio\n",
    "* tf.summary.text\n",
    "* tf.summary.histogram\n",
    "\n",
    "### Tags: Giving names to data\n",
    "\n",
    "When you make a summary op, you will also give it a `tag`. The tag is basically\n",
    "a name for the data recorded by that op, and will be used to organize the data\n",
    "in the frontend. The scalar and histogram dashboards organize data by tag, and\n",
    "group the tags into folders according to a directory/like/hierarchy. If you have\n",
    "a lot of tags, we recommend grouping them with slashes.\n",
    "\n",
    "### Event Files & LogDirs: How TensorBoard loads the data\n",
    "\n",
    "`summary.FileWriters` take summary data from TensorFlow, and then write them to a\n",
    "specified directory, known as the `logdir`. Specifically, the data is written to\n",
    "an append-only record dump that will have \"tfevents\" in the filename.\n",
    "TensorBoard reads data from a full directory, and organizes it into the history\n",
    "of a single TensorFlow execution.\n",
    "\n",
    "Why does it read the whole directory, rather than an individual file? You might\n",
    "have been using\n",
    "[supervisor.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py)\n",
    "to run your model, in which case if TensorFlow crashes, the supervisor will\n",
    "restart it from a checkpoint. When it restarts, it will start writing to a new\n",
    "events file, and TensorBoard will stitch the various event files together to\n",
    "produce a consistent history of what happened.\n",
    "\n",
    "### Runs: Comparing different executions of your model\n",
    "\n",
    "You may want to visually compare multiple executions of your model; for example,\n",
    "suppose you've changed the hyperparameters and want to see if it's converging\n",
    "faster. TensorBoard enables this through different \"runs\". When TensorBoard is\n",
    "passed a `logdir` at startup, it recursively walks the directory tree rooted at\n",
    "`logdir` looking for subdirectories that contain tfevents data. Every time it\n",
    "encounters such a subdirectory, it loads it as a new `run`, and the frontend\n",
    "will organize the data accordingly.\n",
    "\n",
    "For example, here is a well-organized TensorBoard log directory, with two runs,\n",
    "\"run1\" and \"run2\".\n",
    "\n",
    "```\n",
    "/some/path/mnist_experiments/\n",
    "/some/path/mnist_experiments/run1/\n",
    "/some/path/mnist_experiments/run1/events.out.tfevents.1456525581.name\n",
    "/some/path/mnist_experiments/run1/events.out.tfevents.1456525585.name\n",
    "/some/path/mnist_experiments/run2/\n",
    "/some/path/mnist_experiments/run2/events.out.tfevents.1456525385.name\n",
    "/tensorboard --logdir /some/path/mnist_experiments\n",
    "```\n",
    "\n",
    "You may also pass a comma separated list of log directories, and TensorBoard\n",
    "will watch each directory. You can also assign names to individual log\n",
    "directories by putting a colon between the name and the path, as in\n",
    "\n",
    "```\n",
    "tensorboard --logdir name1:/path/to/logs/1,name2:/path/to/logs/2\n",
    "```\n",
    "\n",
    "# The Visualizations\n",
    "\n",
    "### Scalar Dashboard\n",
    "\n",
    "TensorBoard's Scalar Dashboard visualizes scalar statistics that vary over time;\n",
    "for example, you might want to track the model's loss or learning rate. As\n",
    "described in *Key Concepts*, you can compare multiple runs, and the data is\n",
    "organized by tag. The line charts have the following interactions:\n",
    "\n",
    "* Clicking on the small blue icon in the lower-left corner of each chart will\n",
    "expand the chart\n",
    "\n",
    "* Dragging a rectangular region on the chart will zoom in\n",
    "\n",
    "* Double clicking on the chart will zoom out\n",
    "\n",
    "* Mousing over the chart will produce crosshairs, with data values recorded in\n",
    "the run-selector on the left.\n",
    "\n",
    "Additionally, you can create new folders to organize tags by writing regular\n",
    "expressions in the box in the top-left of the dashboard.\n",
    "\n",
    "### Histogram Dashboard\n",
    "\n",
    "The HistogramDashboard displays how the statistical distribution of a Tensor\n",
    "has varied over time. It visualizes data recorded via `tf.summary.histogram`.\n",
    "Each chart shows temporal \"slices\" of data, where each slice is a histogram of\n",
    "the tensor at a given step. It's organized with the oldest timestep in the back,\n",
    "and the most recent timestep in front. By changing the Histogram Mode from\n",
    "\"offset\" to \"overlay\", the perspective will rotate so that every histogram slice\n",
    "is rendered as a line and overlaid with one another.\n",
    "\n",
    "### Distribution Dashboard\n",
    "\n",
    "The Distribution Dashboard is another way of visualizing histogram data from\n",
    "`tf.summary.histogram`. It shows some high-level statistics on a distribution.\n",
    "Each line on the chart represents a percentile in the distribution over the\n",
    "data: for example, the bottom line shows how the minimum value has changed over\n",
    "time, and the line in the middle shows how the median has changed. Reading from\n",
    "top to bottom, the lines have the following meaning: `[maximum, 93%, 84%, 69%,\n",
    "50%, 31%, 16%, 7%, minimum]`\n",
    "\n",
    "These percentiles can also be viewed as standard deviation boundaries on a\n",
    "normal distribution: `[maximum, μ+1.5σ, μ+σ, μ+0.5σ, μ, μ-0.5σ, μ-σ, μ-1.5σ,\n",
    "minimum]` so that the colored regions, read from inside to outside, have widths\n",
    "`[σ, 2σ, 3σ]` respectively.\n",
    "\n",
    "\n",
    "### Image Dashboard\n",
    "\n",
    "The Image Dashboard can display pngs that were saved via a `tf.summary.image`.\n",
    "The dashboard is set up so that each row corresponds to a different tag, and\n",
    "each column corresponds to a run. Since the image dashboard supports arbitrary\n",
    "pngs, you can use this to embed custom visualizations (e.g. matplotlib\n",
    "scatterplots) into TensorBoard. This dashboard always shows you the latest image\n",
    "for each tag.\n",
    "\n",
    "### Audio Dashboard\n",
    "\n",
    "The Audio Dashboard can embed playable audio widgets for audio saved via a\n",
    "`tf.summary.audio`. The dashboard is set up so that each row corresponds to a\n",
    "different tag, and each column corresponds to a run. This dashboard always\n",
    "embeds the latest audio for each tag.\n",
    "\n",
    "### Graph Explorer\n",
    "\n",
    "The Graph Explorer can visualize a TensorBoard graph, enabling inspection of the\n",
    "TensorFlow model. To get best use of the graph visualizer, you should use name\n",
    "scopes to hierarchically group the ops in your graph - otherwise, the graph may\n",
    "be difficult to decipher. For more information, including examples, see [the\n",
    "graph visualizer tutorial](https://www.tensorflow.org/get_started/graph_viz).\n",
    "\n",
    "### Embedding Projector\n",
    "\n",
    "The Embedding Projector allows you to visualize high-dimensional data; for\n",
    "example, you may view your input data after it has been embedded in a high-\n",
    "dimensional space by your model. The embedding projector reads data from your\n",
    "model checkpoint file, and may be configured with additional metadata, like\n",
    "a vocabulary file or sprite images. For more details, see [the embedding\n",
    "projector tutorial](https://www.tensorflow.org/get_started/embedding_viz).\n",
    "\n",
    "### Text Dashboard\n",
    "\n",
    "The Text Dashboard displays text snippets saved via `tf.summary.text`. Markdown\n",
    "features including hyperlinks, lists, and tables are all supported.\n",
    "\n",
    "# Frequently Asked Questions\n",
    "\n",
    "### My TensorBoard isn't showing any data! What's wrong?\n",
    "\n",
    "The first thing to do is ensure that TensorBoard is properly loading data from\n",
    "the correct directory. Launch `tensorboard --logdir DIRECTORY_PATH --debug` and\n",
    "look for output of the form\n",
    "\n",
    "`INFO:tensorflow:TensorBoard path_to_run is: {'DIRECTORY_PATH': None}`\n",
    "\n",
    "Verify that the DIRECTORY_PATH TensorBoard is looking at is the path you expect.\n",
    "(Note: There's a known issue where TensorBoard [does not handle paths starting\n",
    "in ~ properly](https://github.com/tensorflow/tensorflow/issues/1587)).\n",
    "\n",
    "If you're loading from the proper path, make sure that event files are present.\n",
    "TensorBoard will recursively walk its logdir, it's fine if the data is nested\n",
    "under a subdirectory. Try running the command:\n",
    "\n",
    "`find DIRECTORY_PATH | grep tfevents`\n",
    "\n",
    "If you have at least one result, then TensorBoard should be able to load data.\n",
    "\n",
    "Finally, let's make sure that the event files actually have data. Run\n",
    "tensorboard in inspector mode to inspect the contents of your event files.\n",
    "\n",
    "`tensorboard --inspect --logdir DIRECTORY_PATH`\n",
    "\n",
    "If after running this procedure, it's still not working, please file an [issue\n",
    "on GitHub](https://github.com/tensorflow/tensorflow/issues). It will be much\n",
    "easier for us to debug it if you provide an event file that isn't working.\n",
    "\n",
    "### TensorBoard is showing only some of my data, or isn't properly updating!\n",
    "\n",
    "This issue usually comes about because of how TensorBoard iterates through the\n",
    "`tfevents` files: it progresses through the events file in timestamp order, and\n",
    "only reads one file at a time. Let's suppose we have files with timestamps `a`\n",
    "and `b`, where `a<b`. Once TensorBoard has read all the events in `a`, it will\n",
    "never return to it, because it assumes any new events are being written in the\n",
    "more recent file. This could cause an issue if, for example, you have two\n",
    "`FileWriters` simultaneously writing to the same directory. If you have\n",
    "multiple summary writers, each one should be writing to a separate directory.\n",
    "\n",
    "### Does TensorBoard support multiple or distributed summary writers?\n",
    "\n",
    "No. TensorBoard expects that only one events file will be written to at a time,\n",
    "and multiple summary writers means multiple events files. If you are running a\n",
    "distributed TensorFlow instance, we encourage you to designate a single worker\n",
    "as the \"chief\" that is responsible for all summary processing. See\n",
    "[supervisor.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/supervisor.py)\n",
    "for an example.\n",
    "\n",
    "### I'm seeing data overlapped on itself! What gives?\n",
    "\n",
    "If you are seeing data that seems to travel backwards through time and overlap\n",
    "with itself, there are a few possible explanations.\n",
    "\n",
    "* You may have multiple execution of TensorFlow that all wrote to the same log\n",
    "directory. Please have each TensorFlow run write to its own logdir.\n",
    "\n",
    "* You may have a have a bug in your code where the global_step variable (passed\n",
    "to `FileWriter.add_summary`) is being maintained incorrectly.\n",
    "\n",
    "* It may be that your TensorFlow job crashed, and was restarted from an earlier\n",
    "checkpoint. See *How to handle TensorFlow restarts*, below.\n",
    "\n",
    "As a workaround, try changing the x-axis display in TensorBoard from `steps` to\n",
    "`wall_time`. This will frequently clear up the issue.\n",
    "\n",
    "### How should I handle TensorFlow restarts?\n",
    "\n",
    "TensorFlow is designed with a mechanism for graceful recovery if a job crashes\n",
    "or is killed: TensorFlow can periodically write model checkpoint files, which\n",
    "enable you to restart TensorFlow without losing all your training progress.\n",
    "\n",
    "However, this can complicate things for TensorBoard; imagine that TensorFlow\n",
    "wrote a checkpoint at step `a`, and then continued running until step `b`, and\n",
    "then crashed and restarted at timestamp `a`. All of the events written between\n",
    "`a` and `b` were \"orphaned\" by the restart event and should be removed.\n",
    "\n",
    "To facilitate this, we have a `SessionLog` message in\n",
    "`tensorflow/core/util/event.proto` which can record `SessionStatus.START` as an\n",
    "event; like all events, it may have a `step` associated with it. If TensorBoard\n",
    "detects a `SessionStatus.START` event with step `a`, it will assume that every\n",
    "event with a step greater than `a` was orphaned, and it will discard those\n",
    "events. This behavior may be disabled with the flag\n",
    "`--purge_orphaned_data false` (in versions after 0.7).\n",
    "\n",
    "### How can I export data from TensorBoard?\n",
    "\n",
    "The Scalar Dashboard supports exporting data; you can click the \"enable\n",
    "download links\" option in the left-hand bar. Then, each plot will provide\n",
    "download links for the data it contains.\n",
    "\n",
    "If you need access to the full dataset, you can read the event files that\n",
    "TensorBoard consumes by using the [`summary_iterator`](https://github.com/tensorflow/tensorflow/blob/e7f333b5f8b3c53b21d149d8d14c0cebbde431aa/tensorflow/python/summary/summary_iterator.py#L313)\n",
    "method.\n",
    "\n",
    "\n",
    "### Can I overlap multiple plots?\n",
    "\n",
    "Right now, you can overlap plots only if they are from different runs, and both\n",
    "have the same tag name.\n",
    "\n",
    "### Can I create scatterplots (or other custom plots)?\n",
    "\n",
    "This isn't yet possible. As a workaround, you could create your custom plot in\n",
    "your own code (e.g. matplotlib) and then write it into an `SummaryProto`\n",
    "(`core/framework/summary.proto`) and add it to your `FileWriter`. Then, your\n",
    "custom plot will appear in the TensorBoard image tab.\n",
    "\n",
    "### Is my data being downsampled? Am I really seeing all the data?\n",
    "\n",
    "TensorBoard uses [reservoir\n",
    "sampling](https://en.wikipedia.org/wiki/Reservoir_sampling) to downsample your\n",
    "data so that it can be loaded into RAM. You can modify the number of elements it\n",
    "will keep per tag in\n",
    "[tensorboard/backend/application.py](tensorboard/backend/application.py).\n",
    "See this [StackOverflow question](http://stackoverflow.com/questions/43702546/tensorboard-doesnt-show-all-data-points/)\n",
    "for some more information.\n",
    "\n",
    "### I get a network security popup every time I run TensorBoard on a mac!\n",
    "\n",
    "This is because by default, TensorBoard serves on host `0.0.0.0` which is\n",
    "publicly accessible. You can stop the popups by specifying `--host localhost` at\n",
    "startup.\n",
    "\n",
    "### How can I contribute to TensorBoard development?\n",
    "\n",
    "See [DEVELOPMENT.md](DEVELOPMENT.md).\n",
    "\n",
    "### I have a different issue that wasn't addressed here!\n",
    "\n",
    "First, try searching our [GitHub\n",
    "issues](https://github.com/tensorflow/tensorboard/issues) and [Stack\n",
    "Overflow](https://stackoverflow.com/questions/tagged/tensorboard). It may be\n",
    "that someone else has already had the same issue or question.\n",
    "\n",
    "If you have a bug, please [file a GitHub\n",
    "issue](https://github.com/tensorflow/tensorboard/issues). If the bug is related\n",
    "to your specific data (e.g. the events aren't loading properly), please do both\n",
    "of the following things to make it easier for us to debug and fix:\n",
    "\n",
    "- Run tensorboard in --inspect mode and copy paste the debug output.\n",
    "- Upload some events files that will reproduce the issue.\n",
    "\n",
    "If you have a feature request, please [file a GitHub\n",
    "issue](https://github.com/tensorflow/tensorboard/issues).\n",
    "\n",
    "General usage questions should go to [Stack\n",
    "Overflow](http://stackoverflow.com/questions/tagged/tensorflow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Sum of outputs over time is illegal; using Sum_of_outputs_over_time instead.\n",
      "INFO:tensorflow:Summary name Average of outputs over time is illegal; using Average_of_outputs_over_time instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.Graph()  # 创建一个Graph对象\n",
    "\n",
    "with g.as_default():   # 将 Graph对象 设为默认 Graph对象\n",
    "    with tf.name_scope('variables'):\n",
    "        # 记录数据流图运行次数的 Variable 对象\n",
    "        # 这是一种常见的范式，在整个API中，这种范式会频繁出现\n",
    "        global_step = tf.Variable(0, dtype= tf.float32, trainable= False, name= 'global_step')\n",
    "        \n",
    "        # 追踪该模型的所有输出随时间的累加和的 Variable 对象\n",
    "        total_output = tf.Variable(0.0, dtype= tf.float32, trainable= False, name= 'total_output')\n",
    "        \n",
    "        \n",
    "    with tf.name_scope('transformation'):  # 该模型的核心变换部分\n",
    "        \n",
    "        # 独立的输入层\n",
    "        with tf.name_scope('input'):\n",
    "            # 创建输出占位符，用于接收任意长度的向量\n",
    "            a = tf.placeholder(tf.float32, shape= [None], name= 'input_placeholder_a')\n",
    "            \n",
    "        # 独立的中间层\n",
    "        with tf.name_scope('intermediate_layer'):\n",
    "            # 对整个向量实施乘法和加法\n",
    "            b = tf.reduce_prod(a, name= 'prod_b')\n",
    "            c = tf.reduce_sum(a, name= 'sum_c')\n",
    "            \n",
    "        # 独立的输出层\n",
    "        with tf.name_scope('output'):\n",
    "            output = tf.add(b, c, name= 'output')\n",
    "        \n",
    "        # 对变量进行更新\n",
    "        with tf.name_scope('update'):\n",
    "            # 用最新的输出更新Variable对象total_output\n",
    "            update_total = total_output.assign(output)\n",
    "            # 将Variable对象global_step增 1 ，只要数据流图运行，该操作便需要进行\n",
    "            increment_step = global_step.assign_add(1)\n",
    "            \n",
    "        with tf.name_scope('summaries'):\n",
    "            avg = tf.div(update_total, tf.cast(increment_step, tf.float32), name= 'average')\n",
    "            \n",
    "            # 为输出节点创建汇总数据\n",
    "            tf.summary.scalar('Output', output)\n",
    "            tf.summary.scalar('Sum of outputs over time', update_total)\n",
    "            tf.summary.scalar('Average of outputs over time', avg)\n",
    "            \n",
    "        with tf.name_scope('global_ops'):\n",
    "            # 初始化Op\n",
    "            init = tf.global_variables_initializer()\n",
    "            # 将所有汇总数据合并到一个Op中\n",
    "            merged_summaries = tf.summary.merge_all()\n",
    "            \n",
    "sess = tf.Session(graph= g)\n",
    "# 保存汇总数据\n",
    "writer = tf.summary.FileWriter('E:/Graphs/xin_graph', g)\n",
    "sess.run(init)\n",
    "def run_graph(input_tensor):\n",
    "    '''\n",
    "    运行计算图\n",
    "    '''\n",
    "    feed_dict = {a: input_tensor}\n",
    "    _, step, summary = sess.run([output, increment_step, merged_summaries],feed_dict= feed_dict)\n",
    "\n",
    "    writer.add_summary(summary, global_step= step)\n",
    "# 用不同的输入运行该数据流图\n",
    "run_graph([2, 8])\n",
    "run_graph([3, 1, 3, 3])\n",
    "run_graph([8])\n",
    "run_graph([1, 2, 3])\n",
    "run_graph([11, 4])\n",
    "run_graph([4, 1])\n",
    "run_graph([7, 3, 1])\n",
    "run_graph([6, 3])\n",
    "run_graph([0, 2])\n",
    "run_graph([4, 5 ,6])\n",
    "\n",
    "writer.flush()      # 将汇总数据写入磁盘\n",
    "writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir E:/Graphs/xin_graph --debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
